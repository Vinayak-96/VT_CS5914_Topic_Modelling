Title,Abstract,Date
Visualize This: Lessons from the Front-lines of High Performance Visualization,"This paper presents a comprehensive workflow to address two major factors in multivariate multidimensional (MVMD) scientific visualization: the scalability of rendering and the scalability of representation (for perception). Our workflow integrates the metrics of scientific computing and visualization across di fferent STEM domains to deliver perceivable visualizations that meet scientists’ expectations. Our approach attempts to balance the performance of MVMD visualizations using techniques such as sub-sampling, domain decomposition, and parallel rendering. When mapping data to visual form we considered: the nature of the data (dimensionality, type, and distribution), the computing power (serial or parallel), and the rendering power (rendering mechanism, format, and display spectrum). We used HPC clusters to perform remote parallel processing and visualization of large-scale data sets such as 3D point clouds, galaxy catalogs, and airflow simulations. Our workflow brings these considerations into a structured form to guide the decisions of visualization designers who deal with large heterogeneous data sets.",2020-04-02
Compiler-Directed Failure Atomicity for Nonvolatile Memory,"This paper presents iDO, a compiler-directed approach to failure atomicity with nonvolatile memory. Unlike most prior work, which instruments each store of persistent data for redo or undo logging, the iDO compiler identifies idempotent instruction sequences, whose re-execution is guaranteed to be side effect-free, thereby eliminating the need to log every persistent store. Using an extension of prior work on JUSTDO logging, the compiler then arranges, during recovery from failure, to back up each thread to the beginning of the current idempotent region and re-execute to the end of the current failure-atomic section. This extension transforms JUSTDO logging from a technique of value only on hypothetical future machines with nonvolatile caches into a technique that also significantly outperforms state-of-the art lock-based persistence mechanisms on current hardware during normal execution, while preserving very fast recovery times.",2019-07-15
ELASTIN: Achieving Stagnation-Free Intermittent Computation with Boundary-Free Adaptive Execution,"This paper presents ELASTIN, a stagnation-free intermittent computing system for energy-harvesting devices that ensures forward progress in the presence of frequent power outages without partitioning program into recoverable regions or tasks. ELASTIN leverages both timer-based checkpointing of volatile registers and copy-on-write mappings of nonvolatile memory pages to restore them in the wake of power failure. During each checkpoint interval, ELASTIN tracks memory writes on a per-page basis and backs up the original page using custom software-controlled memory protection without MMU or TLB. When a new interval starts at each timer expiration, ELASTIN clears the write permission of all the pages written during the previous interval and checkpoints all registers including a program counter as a recovery point. In particular, ELASTIN dynamically reconfigures both the checkpoint interval and the page size to achieve stagnation-free intermittent computation and maximize forward progress across power outages. The experiments on TI’s MSP430 board with energy harvesting traces show that ELASTIN outperforms the state-of-the-art scheme by 3.5X on average (up to orders of magnitude speedup) and guarantees forward progress.",2019-07-15
BenchPrime: Accurate Benchmark Subsetting with Optimized Clustering Algorithm Selection,"This paper presents BenchPrime, an automated benchmark analysis toolset that is systematic and extensible to analyze the similarity and diversity of benchmark suites. BenchPrime takes multiple benchmark suites and their evaluation metrics as inputs and generates a hybrid benchmark suite comprising only essential applications. Unlike prior work, BenchPrime uses linear discriminant analysis rather than principal component analysis, as well as selects the best clustering algorithm and the optimized number of clusters in an automated and metric-tailored way, thereby achieving high accuracy. In addition, BenchPrime ranks the benchmark suites in terms of their application set diversity and estimates how unique each benchmark suite is compared to other suites. As a case study, this work for the first time compares the DenBench with the MediaBench and MiBench using four different metrics to provide a multi-dimensional understanding of the benchmark suites. For each metric, BenchPrime measures to what degree DenBench applications are irreplaceable with those in MediaBench and MiBench. This provides means for identifying an essential subset from the three benchmark suites without compromising the application balance of the full set. The experimental results show that the necessity of including DenBench applications varies across the target metrics and that significant redundancy exists among the three benchmark suites.",2018-08-24
A Composable Workflow for Productive FPGA Computing via Whole-Program Analysis and Transformation (with Code Excerpts),"We present a composable workflow to enable highly-productive heterogeneous computing on FPGAs. The workflow consists of a trio of static analysis and transformation tools: (1) a whole-program, source-to-source translator to transform existing parallel code to OpenCL, (2) a set of OpenCL kernel linters, which target FPGAs to detect possible semantic errors and performance traps, and (3) a whole-program OpenCL linter to validate the host-to-device interface of OpenCL programs. The workflow promotes rapid realization of heterogeneous parallel code across a multitude of heterogeneous computing environments, particularly FPGAs, by providing complementary tools for automatic CUDA-to-OpenCL translation and compile-time OpenCL validation in advance of very expensive compilation, placement, and routing on FPGAs. The proposed tools perform whole-program analysis and transformation to tackle realworld, large-scale parallel applications. The efficacy of the workflow tools is demonstrated via a representative translation and analysis of a sizable CUDA finite automata processing engine as well as the analysis and validation of an additional 96 OpenCL benchmarks.",2018-07-24
MOANA: Modeling and Analyzing I/O Variability in Parallel System Experimental Design,"Exponential increases in complexity and scale make variability a growing threat to sustaining HPC performance at exascale. Performance variability in HPC I/O is common, acute, and formidable. We take the first step towards comprehensively studying linear and nonlinear approaches to modeling HPC I/O system variability. We create a modeling and analysis approach (MOANA) that predicts HPC I/O variability for thousands of software and hardware configurations on highly parallel shared-memory systems. Our findings indicate nonlinear approaches to I/O variability prediction are an order of magnitude more accurate than linear regression techniques. We demonstrate the use of MOANA to accurately predict the confidence intervals of unmeasured I/O system configurations for a given number of repeat runs – enabling users to quantitatively balance experiment duration with statistical confidence.",2018-04-19
Modeling Influence using Weak Supervision: A joint Link and Post-level Analysis,"Microblogging websites, like Twitter and Weibo, are used by billions of people to create and spread information. This activity depends on various factors such as the friendship links between users, their topic interests and social influence between them. Making sense of these behaviors is very important for fully understanding and utilizing these platforms.  Most prior work on modeling social-media either ignores the effect of social influence, or considers its effect only on link formation or post generation. In contrast, in this paper we propose POLIM, which jointly models the effect of influence on both link and post generation, leveraging weak supervision. We also give POLIM-FIT, an efficient parallel inference algorithm for POLIM which scales to large datasets. In our experiments on a large tweets corpus, we detect meaningful topical communities, celebrities, as well as the influence strengths patterns among them. Further, we find that there are significant portions of posts and links that are caused by influence, and this portion increases when the data focuses on a specific event. We also show that differentiating and identifying these influenced content benefits other quantitative downstream tasks as well, like predicting future tweets and link formation.",2018-04-09
Segmentations with Explanations for Outage Analysis,"Recent hurricane events have caused unprecedented amounts of damage and severely threatened our public safety and economy. The most observable (and severe) impact of these hurricanes is the loss of electric power in many regions, which causes the breakdown of many public services. Understanding the power outages and how they evolve during a hurricane provide insights on how to reduce outages in the future, and how to improve the robustness of the underlying critical infrastructure systems.  In this paper, we propose a novel segmentation with explanations framework to help experts understand such datasets. Our method, CUT-n-REVEAL, first finds a segmentation of the outage sequences to capture pattern changes in the sequences. We then propose a novel explanation optimization problem to find an intuitive explanation of the segmentation, that highlights the culprit of the change. Via extensive experiments, we show that our method performs consistently in multiple datasets with ground truth. We further study real county-level power outage data from several recent hurricanes (Matthew, Harvey, Irma) and show that CUT-n-REVEAL recovers important, nontrivial and actionable patterns for domain experts.",2018-04-09
GPU Power Prediction via Ensemble Machine Learning for DVFS Space Exploration,"A software-based approach to achieve high performance within a power budget often involves dynamic voltage and frequency scaling (DVFS). Consequently, accurately predicting the power consumption of an application at different DVFS levels (or more generally, different processor configurations) is paramount for the energy-efficient functioning of a high-performance computing (HPC) system. The increasing prevalence of graphics processing units (GPUs) in HPC systems presents new multi-dimensional challenges in power management, and machine learning presents an unique opportunity to improve the software-based power management of these HPC systems. As such, we explore the problem of predicting the power consumption of a GPU at different DVFS states via machine learning. Specifically, we perform statistically rigorous experiments to quantify eight machine-learning techniques (i.e., ZeroR, simple linear regression (SLR), KNN, bagging, random forest, sequential minimal optimization regression (SMOreg), decision tree, and neural networks) to predict GPU power consumption at different frequencies. Based on these results, we propose a hybrid ensemble technique that incorporates SMOreg, SLR, and decision tree, which, in turn, reduces the mean absolute error (MAE) to 3.5%.",2018-02-02
ETH: A Framework for the Design-Space Exploration of Extreme-Scale Visualization,"As high-performance computing (HPC) moves towards the exascale era, large-scale scientific simulations are generating enormous datasets. A variety of techniques (e.g., in-situ methods, data sampling, and compression) have been proposed to help visualize these large datasets under various constraints such as storage, power, and energy. However, evaluating these techniques and understanding the various trade-offs (e.g., performance, efficiency, quality) remains a challenging task.   To enable the investigation and optimization across such tradeoffs, we propose a toolkit for the early-stage exploration of visualization and rendering approaches, job layout, and visualization pipelines. Our framework covers a broader parameter space than existing visualization applications such as ParaView and VisIt. It also promotes the study of simulation-visualization coupling strategies through a data-centric approach, rather than requiring the code itself. Furthermore, with experimentation on an extensively instrumented supercomputer, we study more metrics of interest than was previously possible. Overall, our framework will help to answer important what-if scenarios and trade-off questions in early stages of pipeline development, helping scientists to make informed choices about how to best couple a simulation code with visualization at extreme scale.",2017-09-29
CommAnalyzer: Automated Estimation of Communication Cost on HPC Clusters Using Sequential Code,"MPI+X is the de facto standard for programming applications on HPC clusters.  The performance and scalability on such systems is limited by the communication cost on different number of processes and compute nodes. Therefore, the current communication analysis tools play a critical role in the design and development of HPC applications. However, these tools require the availability of the MPI implementation, which might not exist in the early stage of the development process due to the parallel programming effort and time. This paper presents CommAnalyzer, an automated tool for communication model generation from a sequential code. CommAnalyzer uses novel compiler analysis techniques and graph algorithms to capture the inherent communication characteristics of sequential applications, and to estimate their communication cost on HPC systems. The experiments with real-world, regular and irregular scientific applications demonstrate the utility of CommAnalyzer in estimating the communication cost on HPC clusters with more than 95% accuracy on average.",2017-08-14
Personal Reflections on 50 Years of Scientific Computing: 1967–2017,"Computer hardware, software, numerical algorithms, and science and engineering applications are traced for a half century from the author's perspective.",2017-08-10
Understanding Recurring Software Quality Problems of Novice Programmers,"It remains unclear when is the right time to introduce software quality into the computing curriculum. Introductory students often cannot afford to also worry about software quality, while advanced students may have been groomed into undisciplined development practices already. To be able to answer these questions, educators need strong quantitative evidence about the persistence of software quality problems in programs written by novice programmers.  This technical report presents a comprehensive study of software quality in programs written by novice programmers. By leveraging the patterns of recurring quality problems, known as code smells, we analyze a longitudinal dataset of more than 100 novice Scratch programmers and close to 3,000 of their programs. Even after gaining proficiency, students continue to introduce certain quality problems into their programs, suggesting the need for educational interventions. Given the importance of software quality for modern society, computing educators should teach quality-promoting practices alongside the core computing concepts.",2017-07-12
DroidCat: Unified Dynamic Detection of Android Malware,"Various dynamic approaches have been developed to detect or categorize Android malware. These approaches execute software, collect call traces, and then detect abnormal system calls or sensitive API usage. Consequently, attackers can evade these approaches by intentionally obfuscating those calls under focus. Additionally, existing approaches treat detection and categorization of malware as separate tasks, although intuitively both tasks are relevant and could be performed simultaneously. This paper presents DroidCat, the first unified dynamic malware detection approach, which not only detects malware, but also pinpoints the malware family. DroidCat leverages supervised machine learning to train a multi-class classifier using diverse behavioral profiles of benign apps and different kinds of malware. Compared with prior heuristics-based machine learning-based approaches, the feature set used in DroidCat is decided purely based on a systematic dynamic characterization study of benign and malicious apps. All differentiating features that show behavioral differences between benign and malicious apps are included. In this way, DroidCat is robust to existing evasion attacks. We evaluated DroidCat using leave-one-out cross validation with 136 benign apps and 135 malicious apps. The evaluation shows that DroidCat provided an effective and scalable unified malware detection solution with 81% precision, 82% recall, and 92% accuracy.",2016
A Numerical Investigation of Matrix-Free Implicit Time-Stepping Methods for Large CFD Simulations,"This paper is concerned with the development and testing of advanced time-stepping methods suited for the integration of time-accurate, real-world applications of computational fluid dynamics (CFD). The performance of several time discretization methods is studied numerically with regards to computational efficiency, order of accuracy, and stability, as well as the ability to treat effectively stiff problems. We consider matrix-free implementations, a popular approach for time-stepping methods applied to large CFD applications due to its adherence to scalable matrix-vector operations and a small memory footprint. We compare explicit methods with matrix-free implementations of implicit, linearly-implicit, as well as Rosenbrock-Krylov methods. We show that Rosenbrock-Krylov methods are competitive with existing techniques excelling for a number of prob-
lem types and settings.",2016
LIRK-W: Linearly-implicit Runge-Kutta methods with approximate matrix factorization,"This paper develops a new class of linearly implicit time integration schemes called Linearly-Implicit Runge-Kutta-W (LIRK-W) methods. These schemes are based on an implicit-explicit approach which does not require a splitting of the right hand side and allow for arbitrary, time dependent, and stage varying approximations of the linear systems appearing in the method. Several formulations of LIRK-W schemes, each designed for specific approximation types, and their associated order condition theories are presented.",2016-11-22
Multivariate predictions of local reduced-order-model errors and dimensions,"This paper introduces multivariate input-output models to predict the errors and bases dimensions of local parametric Proper Orthogonal Decomposition reduced-order models. We refer to these multivariate mappings as the MP-LROM models. We employ Gaussian Processes and Artificial Neural Networks to construct approximations of these multivariate mappings. Numerical results with a viscous Burgers model illustrate the performance and potential of the machine learning based regression MP-LROM models to approximate the characteristics of parametric local reduced-order models. The predicted reduced-order models errors are compared against the multi-fidelity correction and reduced order model error surrogates methods predictions, whereas the predicted reduced-order dimensions are tested against the standard method based on the spectrum of snapshots matrix. Since the MP-LROM models incorporate more features and elements to construct the probabilistic mappings they achieve more accurate results. However, for high-dimensional parametric spaces, the MP-LROM models might suffer from the curse of dimensionality. Scalability challenges of MP-LROM models and the feasible ways of addressing them are also discussed in this study.",2017-01-16
AutoMatch: Automated Matching of Compute Kernels to Heterogeneous HPC Architectures,"HPC systems contain a wide variety of heterogeneous computing resources, ranging from general-purpose CPUs to specialized accelerators. Porting sequential applications to such systems for achieving high performance requires significant software and hardware expertise as well as extensive manual analysis of both the target architectures and applications to decide the best performing architecture and implementation technique for each application. To streamline this tedious process, this paper presents AutoMatch, a tool for automated matching of compute kernels to heterogeneous HPC architectures. AutoMatch analyzes the sequential application code and automatically predicts the performance of the best parallel implementation of its compute kernels on different hardware architectures. AutoMatch leverages such prediction results to identify the best device for each kernel from a set of devices including multi-core CPUs and many-core GPUs. In addition, it estimates the relative execution cost between the different architectures to drive a workload distribution scheme, which enables end users to efficiently exploit the available compute resources across multiple heterogeneous architectures. We demonstrate the efficacy of AutoMatch, using a set of open-source HPC applications and benchmarks with different parallelism profiles and memory-access patterns. The empirical evaluation shows that AutoMatch is highly accurate across five different heterogeneous architectures, identifying the best architecture for each workload in 96% of the test cases, and its workload distribution scheme has a comparable performance to a profiling-driven oracle.",2016-12-13
Understanding Application Behaviours for Android Security: A Systematic Characterization,"In contrast to most existing research on Android focusing on specific security issues, there is little broad understanding of Android application run-time characteristics and their security implications. To mitigate this gap, we present the first dynamic characterization study of Android applications that targets such a broad understanding for Android security. Through lightweight method-level profiling, we have collected 33GB traces of method calls and inter-component communication (ICC) from 114 popular Android applications on Google Play and 61 communicating pairs among them that enabled an extensive empirical investigation of the run-time behaviours of Android applications. Our study revealed that (1) the Android framework was the target of 88.3% of all calls during application executions, (2) callbacks accounted for merely 3% of the total method calls, (3) 75% of ICCs did not carry any data payloads with those doing so preferring bundles over URIs, (4) 85% of sensitive data sources and sinks targeted one or two top categories of information or operations which were also most likely to constitute data leaks. We discuss the security implications of our findings to secure development and effective security defense of modern Android applications.",2016
Accelerating Workloads on FPGAs via OpenCL: A Case Study with OpenDwarfs,"For decades, the streaming architecture of FPGAs has delivered accelerated performance across many application domains, such as option pricing solvers in finance, computational fluid dynamics in oil and gas, and packet processing in network routers and firewalls. However, this performance comes at the expense of programmability. FPGA developers use hardware design languages (HDLs) to implement the application data and control path and to design hardware modules for computational pipelines, memory management, synchronization, and communication. This process requires extensive knowledge of logic design, design automation tools, and low-level details of FPGA architecture, this consumes significant development time and effort.  To address this lack of programmability of FPGAs, OpenCL provides an easy-to-use and portable programming model for CPUs, GPUs, APUs, and now, FPGAs. Although this significantly improved programmability yet an optimized GPU implementation of kernel may lack performance portability for FPGA. To improve the performance of OpenCL kernels on FPGAs we identify general techniques to optimize OpenCL kernels for FPGAs under device-specific hardware constraints. We then apply these optimizations techniques to the OpenDwarfs benchmark suite, which has diverse parallelism profiles and memory access patterns, in order to evaluate the effectiveness of the optimizations in terms of performance and resource utilization. Finally, we present the performance of structured grids and N-body dwarf-based benchmarks in the context of various optimization along with their potential re-factoring. We find that careful design of kernels for FPGA can result in a highly efficient pipeline achieving 91% of theoretical throughput for the structured grids dwarf.  Index Terms—OpenDwarfs; FPGA; OpenCL; GPU; MIC; Accelerators; Performance Portability",2016-05-13
Bridging the Performance-Programmability Gap for FPGAs via OpenCL: A Case Study with OpenDwarfs,"For decades, the streaming architecture of FPGAs has delivered accelerated performance across many application domains, such as option pricing solvers in finance, computational fluid dynamics in oil and gas, and packet processing in network routers and firewalls. However, this performance has come at the significant expense of programmability, i.e., the performance-programmability gap. In particular, FPGA developers use hardware design languages (HDLs) to implement the application data path and to design hardware modules for computation pipelines, memory management, synchronization, and communication. This process requires extensive low-level knowledge of the target FPGA architecture and consumes significant development time and effort.  To address this lack of programmability of FPGAs, OpenCL provides an easy-to-use and portable programming model for CPUs, GPUs, APUs, and now, FPGAs. However, this significantly improved programmability can come at the expense of performance; that is, there still remains a performance-programmability gap. To improve the performance of OpenCL kernels on FPGAs, and thus, bridge the performance-programmability gap, we identify general techniques to optimize OpenCL kernels for FPGAs under device-specific hardware constraints. We then apply these optimization techniques to the OpenDwarfs benchmark suite, with its diverse parallelism profiles and memory access patterns, in order to evaluate the effectiveness of the optimizations in terms of performance and resource utilization. Finally, we present the performance of the optimized OpenDwarfs, along with their potential re-factoring, to bridge the performance gap from programming in OpenCL versus programming in a HDL.  Index Terms—OpenDwarfs; FPGA; OpenCL; GPU; GPGPU; MIC; Accelerators; Performance Portability",2016-05-13
Telescoping Architectures: A Methodology for Evaluating Next-Generation Heterogeneous Computing,"Architectural innovation has telescoped the HPC community from the commodity (Beowulf) cluster in a machine room, i.e., a multi-node system with Ethernet interconnect, to a commodity cluster on a chip, i.e., multicore CPU with an on-die interconnect. We project that this “telescoping architecture” will apply more broadly to heterogeneous computing, namely from heterogeneous clusters like Tianhe-2 in a machine room to on a chip. To that end, we present an experimental study that extends the notion of telescoping architectures to identify the ideal mixture of compute engines (CEs) and the number of such CEs on a chip to create a heterogeneous “cluster on a chip” (CoC). Specifically, we experiment with heterogeneous architectures that contain single or multiple instances of CPUs, GPUs, Intel MICs, and FPGAs to demonstrate their performance efficacy given continuing advances in hardware technology, software, tools, and run-time support.  Index Terms—architecture; microprocessor design; heterogeneous computing; dwarfs; motifs; system on a chip;",2016-05-13
Identifying Product Defects from User Complaints: A Probabilistic Defect Model,"The recent surge in using social media has created a massive amount of unstructured textual complaints about products and services. However, discovering and quantifying potential product defects from large amounts of unstructured text is a nontrivial task. In this paper, we develop a probabilistic defect model (PDM) that identifies the most critical product issues and corresponding product attributes, simultaneously. We facilitate domain-oriented key attributes (e.g., product model, year of production, defective components, symptoms, etc.) of a product to identify and acquire integral information of defect. We conduct comprehensive evaluations including quantitative evaluations and qualitative evaluations to ensure the quality of discovered information. Experimental results demonstrate that our proposed model outperforms existing unsupervised method (K-Means Clustering), and could find more valuable information. Our research has significant managerial implications for mangers, manufacturers, and policy makers.",2016-03-02
An Automated Framework for Characterizing and Subsetting GPGPU Workloads,"Graphics processing units (GPUs) are becoming increasingly common in today’s computing systems due to their superior performance and energy efficiency relative to their cost. To further improve these desired characteristics, researchers have proposed several software and hardware techniques. Evaluation of these proposed techniques could be tricky due to the ad-hoc nature in which applications are selected for evaluation. Sometimes researchers spend unnecessary time evaluating redundant workloads, which is particularly problematic for time-consuming studies involving simulation. Other times, they fail to expose the shortcomings of their proposed techniques when too few workloads are chosen for evaluation.  To overcome these problems, we propose an automated framework that characterizes and subsets GPGPU workloads, depending on a user-chosen set of performance metrics/counters. This framework internally uses principal component analysis (PCA) to reduce the dimensionality of the chosen metrics and then uses hierarchical clustering to identify similarity among the workloads. In this study, we use our framework to identify redundancy in the recently released SPEC ACCEL OpenCL benchmark suite using a few architecture-dependent metrics. Our analysis shows that a subset of eight applications provides most of the diversity in the 19-application benchmark suite. We also subset the Parboil, Rodinia, and SHOC benchmark suites and then compare them against each another to identify “gaps” in these suites. As an example, we show that SHOC has many applications that are similar to each other and could benefit from adding four applications from Parboil to improve its diversity.",2015-12-18
CAN-zip – Centroid Based Delta Compression of Next Generation Sequencing Data,"We present CANzip, a novel algorithm for compressing short read DNA sequencing data in FastQ format. CANzip is based on delta compression, a process in which only the differences of a specific data stream relative to a given reference stream are stored. However CANzip uniquely assumes no given reference stream. Instead it creates artificial references for different clusters of reads, by constructing an artificial representative sequence for each given cluster. Each cluster sequence is then recoded to indicate only how it differs relative to this artificially created reference sequence. Remodeling the data in this way greatly improves the compression ratio achieved when used in conjunction with commodity tools such as bzip2. Our results indicate that CANzip outperforms gzip on average and that it can outperform bzip2.",2015-11-09
DISCRN: A Distributed Storytelling Framework for Intelligence Analysis,"Storytelling connects entities (people, locations, organizations) using their observed relationships to establish meaningful stories among them. Extending that, spatio-temporal storytelling incorporates spatial and graph computations to enhance coherence and meaning. These computations become a bottleneck when performed sequentially as massive number of entities make space and time complexity untenable. This paper presents DISCRN, a distributed frame work for performing spatio-temporal storytelling. The framework extracts entities from microblogs and event data, and links those entities to derive stories in a distributed fashion. Performing these operations at scale allows deeper and broader analysis of storylines. This work extends an existing technique based on ConceptGraph and ConceptRank applying them in a distributed key-value pair paradigm. The novel parallelization techniques speed up the generation and filtering of storylines on massive datasets. Experiments with Twitter data and GDELT events show the effectiveness of techniques in DISCRN.",2015
SLIM: A Session-Layer Intermediary for Enabling Multi-Party and Reconfigurable Communication,"Increasingly, communication requires more from the network stack. Due to missing functionality, we see a proliferation of networking libraries that attempt to fill the void (e.g., iOS to OSX Handoff and Google Cast SDK). This leads to considerable duplication of effort. Further, the provisions for extending legacy protocol stacks is largely exhausted (e.g., TCP options space is mostly allocated) making the addition of future extensions much more challenging. We present SLIM, an extensible session-layer intermediary that extracts the duplicate functionality from modern networking libraries and provides the means for future extensibility to the network stack. SLIM enables mobility, multi-party communication, and dynamic reconfiguration of the network stack in a straightforward and elegant way. SLIM includes an out-of-band signaling channel, which not only enables reconfiguration, but also allows for incremental evolution of the stack. To start, we tease out elements of session management which are currently conflated with transport semantics in TCP. Doing so highlights the need for sessions in contemporary use cases. Next, we propose session, flow and end-point abstractions that allow application developers to describe communication between any number of participants.The abstractions apply to individual or a group communication allowing them to be managed as one. We describe the abstractions and evaluate them in terms of typical communication patterns. We demonstrate the abstractions via a prototype implementation of SLIM.",2015-06-11
Partially assembled nucleosome structures at atomic detail,"Evidence is now overwhelming that partially assembled states of the nucleosome are as important as the canonical structure for the understanding of how DNA accessibility is regulated in cells. We use a combination of atomistic simulation and Atomic Force Microscopy (AFM) to propose models of key partially assembled structures of the nucleosome at atomic detail: the heaxasome ((H3·H4)2·(H2A·H2B)), the tetrasome ((H3·H4)2), and the disome (H3·H4). Despite large-scale fluctuations of the “free” DNA not in direct contact with the histones in these structures, the histone-DNA contacts are stable, establishing the basis for interpretation of the role of these structures in providing variable degree of DNA accessibility within the chromatin. We show how the atomistically detailed partially assembled nucleosome structures can be used to interpret experimental observations, both in-vitro and in-vivo. Specifically, interpretation of FRET, AFM, (and SAXS) experiments under conditions where partially assembled nucleosome states are expected is greatly facilitated by the availability of atomic- resolution structural ensembles of these states. We also suggest an alternative interpretation of a recent genome-wide study that investigated types of DNA protection in classical “active” chromatin, lending support for the transcription speed-up scenario that involves partially assembled sub-nucleosome structures.",2015-05-21
Extracting Named Entities Using Named Entity Recognizer and Generating Topics Using Latent Dirichlet Allocation Algorithm for Arabic News Articles,"This paper explains for the Arabic language, how to extract named entities and topics from news articles. Due to the lack of high quality tools for Named Entity Recognition (NER) and topic identification for Arabic, we have built an Arabic NER (RenA) and an Arabic topic extraction tool using the popular LDA algorithm (ALDA). NER involves extracting information and identifying types, such as name, organization, and location. LDA works by applying statistical methods to vector representations of collections of documents. Though there are effective tools for NER and LDA for English, these are not directly applicable to Arabic. Accordingly, we developed new methods and tools (i.e., RenA and ALDA). To allow assessment of these, and comparison with other methods and tools, we built a baseline corpus to be used in NER evaluation, with help from volunteer graduate students who understand Arabic. RenA produces good results, with accurate Name, Organization, and Location extraction from news articles collected from online resources. We compared the RenA results with a popular Arabic NER, and achieved an enhancement. We also carried out an experiment to evaluate ALDA, again involving volunteer graduate students who understand Arabic. ALDA showed very good results in terms of topics extraction form Arabic news articles, achieving high accuracy, based on an experimental evaluation with participants using a Likert scale.",2015
"Automated Arabic Text Classification with P-Stemmer, Machine Learning, and a Tailored News Article Taxonomy","Arabic news articles in electronic collections are difficult to work with. Browsing by category is rarely supported. While helpful machine learning methods have been applied successfully to similar situations for English news articles, limited research has been completed to yield suitable solutions for Arabic news. In connection with a QNRF funded project to build digital library community and infrastructure in Qatar, we developed software for browsing a collection of about 237K Arabic news articles, which should be applicable to other Arabic news collections as well. We designed a simple taxonomy for Arabic news stories that is suitable for the needs in Qatar and other nations, is compatible with the subject codes of the International Press Telecommunications Council, and was enhanced with the aid of a librarian expert as well as five Arabic-speaking volunteers. We developed tailored stemming (i.e., a new Arabic light stemmer) and automatic classification methods (the best being binary SVM classifiers) to work with the taxonomy. Using evaluation techniques commonly used in the information retrieval community, including 10-fold cross-validation and the Wilcoxon signed-rank test, we showed that our approach to stemming and classification is superior to state-of-the-art techniques.",2015-01-22
Design and Evaluation of Scalable Concurrent Queues for Many-Core Architectures,"As core counts increase and as heterogeneity becomes more common in parallel computing, we face the prospect of pro gramming hundreds or even thousands of concurrent threads in a single shared-memory system. At these scales, even highly-efficient concurrent algorithms and data structures can become bottlenecks, unless they are designed from the ground up with throughput as their primary goal.   In this paper, we present three contributions: (1) a characterization of queue designs in terms of modern multi- and many-core architectures, (2) the design of a high-throughput concurrent FIFO queue for many-core architectures that avoids the bottlenecks common in modern queue designs, and (3) a thorough evaluation of concurrent queue throughput across CPU, GPU, and co-processor devices. Our evaluation shows that focusing on throughput, rather than progress guarantees, allows our queue to scale to as much as three orders of magnitude (1000X) faster than lock-free and combining queues on GPU platforms and two times (2X) faster on CPU devices. These results deliver critical insight into the design of data structures for highly concurrent systems: (1) progress guarantees do not guarantee scalability, and (2) allowing an algorithm to block can actually increase throughput.",2014-08-06
Algorithm XXX: QNSTOP—Quasi-Newton Algorithm for Stochastic Optimization,"QNSTOP consists of serial and parallel (OpenMP) Fortran 2003 codes for the quasi-Newton stochastic optimization method of Castle and Trosset. For stochastic problems, convergence theory exists for the particular algorithmic choices and parameter values used in QNSTOP. Both the parallel driver subroutine, which offers several parallel decomposition strategies, and the serial driver subroutine can be used for stochastic optimization or deterministic global optimization, based on an input switch. QNSTOP is particularly effective for “noisy” deterministic problems, using only objective function values. Some performance data for computational systems biology problems is given.",2014-07-01
The Performance Effects of Power Scaling on Kernel- based Atomic Batch Transactions,"The need to balance performance and power is essential to computer system efficiency. Today’s server-class systems commonly support autonomous power scaling of processors, memory, and disks. While processor power scaling self- governance (e.g. Intel’s Turbo Boost) can improve both performance and efficiency, there is growing evidence that at times boosting processor power (and speed) actually harms performance. In this paper, we identify clear cases where processor power scaling can reduce performance by up to 68% on two I/O intensive benchmarks. We describe a methodology for isolating the performance effects of power scaling in server-class systems. We propose a new model to explain the root causes of performance loss in the Linux kernel due to power scaling for two I/O intensive benchmarks. Using the model, we are able to identify global system locks that cause slowdowns at higher processor power (and speed) in the Linux kernel and eliminate the potential performance loss (up to 68%) from power scaling for the benchmarks studied. We provide a detailed case study of the effects of power scaling on one type of Linux kernel-based lock (i.e. atomic batch transactions) and we discuss future performance challenges for power scalable systems.",2014-01-28
Probability-one Homotopy Maps for Constrained Clustering Problems,"Many algorithms for constrained clustering have been developed in the literature that aim to balance vector quantization requirements of cluster prototypes against the discrete satisfaction requirements of constraint (must-link or cannot-link) sets. A significant amount of research has been devoted to designing new algorithms for constrained clustering and understanding when constraints help clustering. However, no method exists to systematically characterize solution sets as constraints are gently introduced and how to assist practitioners in choosing a sweet spot between vector quantization and constraint satisfaction. A homotopy method is presented that can smoothly track solutions from unconstrained to constrained formulations of clustering. Beginning the homotopy zero curve tracking where the solution is (fairly) well-understood, the curve can then be tracked into regions where there is only a qualitative understanding of the solution set, finding multiple local solutions along the way. Experiments demonstrate how the new homotopy method helps identify better tradeoffs and reveals insight into the structure of solution sets not obtainable using pointwise exploration of parameters.",2013-12-31
Multiobjective Optimization Using an Adaptive Weighting Scheme,"A new Pareto front approximation method is proposed for multiobjective optimization problems with bound constraints. The method employs a hybrid optimization approach using two derivative free direct search techniques, and intends to solve blackbox simulation based multiobjective optimization problems where the analytical form of the objectives is not known and/or the evaluation of the objective function(s) is very expensive. A new adaptive weighting scheme is proposed to convert a multiobjective optimization problem to a single objective optimization problem. Another contribution of this paper is the generalization of the star discrepancy based performance measure for problems with more than two objectives. The method is evaluated using five test problems from the literature. Results show that the method achieves an arbitrarily close approximation to the Pareto front with a good collection of well-distributed nondominated points for all five test problems.",2013-12-31
ADML: Aircraft Design Markup Language for Multidisciplinary Aircraft Design and Analysis,"The process of conceptual aircraft design has advanced tremendously in the past few decades due to rapidly developing computer technology. Today’s modern aerospace systems exhibit strong, interdisciplinary coupling and require a multidisciplinary, collaborative approach. Efficient transfer, sharing, and manipulation of aircraft design and analysis data in such a collaborative environment demands a formal structured representation of data.  XML, a W3C recommendation,is one such standard concomitant with a number of powerful capabilities that alleviate interoperability issues in a collaborative environment. A compact, generic, and comprehensive XML schema for an aircraft design markup language (ADML) is proposed here to represent aircraft conceptual design and analysis data. The purpose of this unified data format is to provide a common language for data communication, and to improve efficiency and productivity within a multidisciplinary, collaborative aricraft design environment. An important feature of the proposed schema is the very expressive and efficient low level schemata (raw data, mathematical objects, and basic geometry). As a proof of concept the schema is used to encode an entire Convair B58. As the complexity of models and number of disciplines increases, the reduction in effort to exchange data models and analysis results in ADML also increases.",2013-12-31
Spatio-Temporal Storytelling on Twitter,"Social media, e.g.,Twitter, have provided us an unprecedented opportunity to observe events un-folding in real-time. The rapid pace at which situations play out on social media necessitates new tools for capturing and summarizing the spatio-temporal progression of events. This technical report describes methods for generating dynamic real-world storylines from Twitter Sources and shares the results of related experiments.",2013-12-16
Identity-sensitive Points-to Analysis for the Dynamic Behavior of JavaScript Objects,"JavaScript object behavior is dynamic and adheres to prototype-based inheritance. The behavior of a JavaScript object can be changed by adding and removing properties at runtime. Points-to analysis calculates the set of values a reference property or variable may have during execution. We present a novel, partially flow-sensitive, context-sensitive points-to algorithm that accurately models dynamic changes in object behavior. The algorithm represents objects by their creation sites and local property names; it tracks property updates via a new control-flow graph representation. The calling context is comprised of the receiver object, its local properties and prototype chain. We compare the new points-to algorithm with an existing JavaScript points-to algorithm in terms of their respective performance and accuracy on a client application. The experimental results on real JavaScript websites show that the new points-to analysis significantly improves precision, uniquely resolving on average 11% more property lookup statements.",2013-12-13
Collaborative Design for Young Children with Autism: Design Tools and a User Study,"This paper provides an overview of a collaborative design effort that involves computer scientists, psychologists, and designers working together to investigate design methods to help in the creation of technology to people with cognitive disabilities. The focus of this effort was in developing techniques to help novice designers create technology interfaces to support anger management in young people with autism spectrum disorder (ASD). The primary output for designers is a card set for which each card has a claim about an anger management technique that can help young people. Design activities leveraging scenarios and personas are suggested that leverage the card set in the creation of technology interfaces. This paper introduces the card set and supporting techniques, describes a design session in an undergraduate classroom setting, and speculates about future directions for this work.",2013-09-30
A Static Assurance Analysis of Android Applications,"We describe an efficient approach to identify malicious Android applications through specialized static program analysis. Our solution – referred to as user intention program dependence analysis – performs offline analysis to find the dependence relations between user triggers and entry points to methods providing critical system functions. Analyzing these types of dependences in programs can identify the privileged operations (e.g., file, network operations and sensitive data access) that are not intended by users. We apply our technique on 708 free popular apps and 482 malware apps for Android OS, and the experimental results show that our technique can differentiate between legitimate and malware applications with high accuracy. We also explain the limitations of the user-intention-based approach and point out the need for practitioners to adopt multiple analysis tools for evaluating the assurance of Android applications.",2013-07-11
Multipersona Hypovisors: Securing Mobile Devices through High-Performance Light-Weight Subsystem Isolation,"We propose and detail a system called multipersona Hypovisors for providing light-weight isolation for enhancing security on Multipersona mobile devices, particularly with respect to the current memory constraints of these devices. Multipersona Hypovisors leverage Linux kernel cGroups and namespaces to establish independent process container, al-lowing isolation of the Multipersona process tree from other simultaneous instances of Multipersona and the hypovisor which is an underlying Angstrom-based embedded Linux distributions designed to add additional security to the system. The system incorporates a wide range of data integrity tools in the embedded hypovisor, and an SE Linux-enabled kernel for mandatory access control and integrity tools for transparent auditing of running Multipersona instances. A prototype is presented which uses integrity tools external to the Multipersona container to audit it for malicious activity, and also has the ability to support a multipersona environment with multiple encrypted personas existing individually or simultaneously on the device. Two versions are demonstrated, one which allows cold-swapping of personas for high-assurance scenarios and also one that supports hot-swapping. Analysis shows that the hypovisor has a 40-50 MB impact on the overall memory footprint for the system.",2013-06-28
Modeling Correlated Proxy Web Traffic Using Fourier Analysis,"We analyze the arrival rate of accesses to Web proxy caching servers.  The results show that the data display strong periodic autocorrelation.  The examined data sets show a consistent behavior in terms of having periods corresponding to daily and weekly cycles that can be explained in terms of daily and weekly cyclic behavior of Web users.  While these results confirm the correlation in the network traffic noticed by other researchers, we emphasize that this correlation is periodic.  A new approach is introduced to model data that exhibit such characteristics by a combination of Fourier and statistical analysis techniques.  The source of high correlation in the data is shown to come from the periodic and hence the deterministic part. Synthesized data that results from this modeling approach is shown to have a long-range dependent and self-similar behavior.",1997-11-01
Improving Polymorphism in Common Object Models,"Most common object models of distributed object systems lack support for 'polymorphism' (an abstraction mechanism that represents a quality or state of being able to assume different forms).  This lack of support restricts the development of new components and limits reuse of existing components that use these advanced features.  In this paper, the Interoperable Common Object Model (ICOM), which focuses on a subset of object-oriented languages, specifically statically typed languages, is presented.  The ICOM model is an attempt to elevate common object models closer to the object models of statically typed object-oriented languages by including support for polymorphism.  Specific features of the ICOM object model include: remote inheritance, method overloading, and parameterized types.",1998-03-01
Computer Analysis of User Interfaces,"Interface evaluation is a necessary phase in the production of quality user interfaces.  The usual evaluation techniques involve formal experiments or observation, and can be invasive.  One non-invasive method that can be used at user sites is to record all user input and system output to a file.  This transcript is then algorithmatically analyzed to determine interface problems.  A new technique analyzes these transcripts by searching for maximal repeating patterns (MRPs), on the hypothesis that repeating sequences of user actions indicate interesting user behavior, and therefore may show problems in the interface.  The technique was tested by using it to evaluate the human-computer interface of a large and complex image processing system in active use.  Results show MRPs useful in detecting specific problems within the interface.",1989
A Comparison of Selected Conceptual Frameworks for SimulationModeling,"The purpose of this paper is compare 13 Conceptual Frameworks (CFs) selected from among several catagories of applicability to discrete-event simulation modeling.  Each CF is briefly reviewed to provide the background information required for the comparison.  Based on the insights gained in applying the CFs to the modeling complex traffic intersection system, the CFs are compared to their distinct characteristics and capabilities.  Comparative comments are grouped according to the design guidance and implementation guidance features of the CFs.  Conclusions highlight the inadequacies of the CFs and the importance of research in CF development.",1989
UIMS: Toward the Next Generation,"First generation User Interface Management Systems (UIMS) have established themselves in both research and commercial areas.  This paper discusses improved usability and extension of UIMS to include a broader whole system development life cycle as the basis for evolution of a second generation of UIMS.  Problems of first generation UIMS, some informal empirical work that is leading toward an interface development life cycle and UIMS to directions for the anticipated evolution are presented.",1986
Tracking Nonlinear Equilibrium Paths by a Homotopy Method,No abstract available.,1981
Validation of Multivariate Response Trace-driven Simulation Models,"A procedure is developed by using Rotelling's one-sample T^2 test to test the validity of a multivariate response trace-driven simulation model that represents an observable system. The vali- dity of the simulation model is tested with respect to the mean behavior under a given experimental frame. A procedure for cost-risk analysis for the one-sample T^2 test is developed. By using this procedure, a trade-off analysis can be performed and judgement decisions can be made as to what data collection budget to allocate, what data collection method to use, how many paired observations to collect on the model and system re- sponse variables, and what model builder's risk to choose for test- ing the validity under a satisfactory model user's risk. The procedure for validation and the cost-risk analysis are illustrated for a trace-driven simulation model that represents a time-sharing computer system with two performance measures of interest.",1982
The Algebraic Representation of Partial Functions,"The paper presents a generalization of the theorem which states that any (everywhere defined) function from a finite field GF(p^n) into itself may be represented at a polynomial over GF(p^n). The generalization is to partial functions over GF(p^n) and exhibits  representations of a partial function f by the sum of a polynomial and a sum of terms of the form a/(x-b)i, where b is one of the  points at which f is undefined. Three such representation theorems are given. The second is the analog of the Mittag-Leffler Theorem of the theory of functions of a single complex variable. The main result of the paper is that the sum of the degree of  the polynomial part of the representation and the degrees of the  principal parts of the representation need be no more than  max(|A|, |B|) where A is the set upon which the function is defined and B is the set upon which the function is undefined.",1978
A Study of Multiprocessor Architectures for Supporting Secure Database Management,"This is a work-in-progress report on multiprocesser architectures for supporting secure database management. The basic configuration contains three logical modules -- the user and applications module, the data storage and retrieval module, and the protection and security module. Each module resides on one or more processors that form a multiprocessor system called MULTISAFE. The background leading to the configuration and the functional characteristics of MULTISAFE are discussed. Some problems and questions for further research are also identified.",1977
Language Extensions for Specifying Access Control Policies in Programming Languages,"The scope rules in programming languages control the sharing of data among program units-e.g., blocks and procedures. Typically, scope rules provide an all-or-nothing kind of access control. A wide range of programming problems exist which require finer access control as well as considerable sophistication for the implementation of access control policies on high-level data objects such as files. This paper presents a number of language extensions that permit the programmer to specify the degree of access control for each abstract object that a program unit can manipulate. An attempt has been made to keep the number of extensions as small as possible and yet allow the user conveniently to specify the access control policies that he desires.  Some of the extensions permit access policies to be specified such that access correctness can be completely determined at compile time; other extensions permit policies to be specified that require some access checking to be done at runtime in order to ensure access correctness. The extensions have been developed such that subsets can be selected and implemented in programming languages to provide various access control policies.",1980
Nonlinear Thermal Waves: Part II - Analytical Solutions for Pulses,"The weak shock theory developed by Cramer (1994) is used to derive explicit solutions for the evolution of one-dimensional weakly, nonlinear, weakly relaxing heat pulses in rigid conductors.  Formulas and differential equations governing the evolution of general heat inputs are derived and applied to the special cases of a square-wave heat input and a finite width sine-wave heat input.  Numerical solutions to the exact Maxwell-Cattaneo theory are also presented and show excellent agreement with these analytical solutions.",1994-12-01
Digital Library: Gross Structure and Requirements (Report from aWorkshop),"The overall workshop goals were: 1) to define those digital library parameters which especially influences issues of access to, retrieval of, and interaction with information; 2) to identify key problems which must be solved to make digital library service an effective reality; 3) to identify a general structure or framework for integrating research and solutions; and 4) to propose and encourage specific, high-priority research directions within such a framework.",1994-06-01
Edge-Packing in Planar Graphs,"Maximum G Edge-Packing (EPack-sub G) is the problem of finding the maximum number of edge-disjoint isomorphic copies of a fixed guest graph G in a host graph H.  This paper investigates the computational complexity of edge-packing for planar guests and planar hosts. Edge-packing is solvable in polynomial time when both G and H are either a 3-cycle or a k-star (graphs isomorphic to K(sub 1,k).  Edge-packing is NP-complete when H is planar and G is either a cycle or a tree with greater than or equal to 3 edges.  A strategy for developing polynomial-time approximation algorithms for planar hosts is exemplified by a linear-time approximation algorithm that finds a k-star edge-packing of size at least 1/2 optimal.",1995-10-01
Transforming Command-Line Driven Systems to Web Applications,"A method fo add a Web-based interface to a command-line driven system is presented.  Without programming, Javamatic can generate a graphical user interface, then invokes commands in the legacy system transparently to the user.  The user interface (UI) is automatically generated as a Java applet or stand-alone interface from a high-level description of the application, which is UI independent, using a set of UI mapping rules.  The application is wrapped with an interface serverl thus multiple clients can use the legacy application through the Web.",1996-12-01
Professionalism in Computing: A Web-Based Learning System,"Starting from a project to develop a digital library for a Computer Science course studying social impact and computer ethics, a highly interactive Web-based learning system is in the process of development. Capable of supporting a variety of teaching/learning environments, the system is intended to support teachers whose disciplinary primary interest is other than social impact and ethics.",1997-03-01
Structural Descriptions And Inexact Matching,In this paper we formally define the structural description of an object and the concepts of exact and inexact matching of  two structural descriptions.  We discuss the problems associated  with a brute-force backtracking tree search for inexact matching and develop several different algorithms to make the tree search  more efficient.  We develop the formula for the expected number  of nodes in the tree for backtracking alone and vith a forward  checking algorithm.  Finally we present experimental results  verifying the theory and showing that forward checking is the most efficient of the algorithms tested.,1979
"To Be, or Not to Be -- Is That the Question?","My purpose is to identify three fundamental questions regarding language  standardization, to share with you my answers to these questions, and to  explore the rationale emanating from these answers.",1979
Preservation of Etds on Ndltd Version 1.0,"Theses and dissertations published at a university are important research resources. ETDs (Electronic Theses and Dissertations) are simply the theses and dissertations published in electronic form (e.g., in PDF). Many universities are implementing a requirement that theses and dissertations be submitted in electronic form, thus making it easier for other people to access these works. These ETDs typically are archived on a server at each local university. We have developed a mirroring system which will store additional copies of remote ETDs, and thus will preserve and enhance access to them. The local archive of ETDs will be updated regularly. If someday the university (Publisher) fails to provide access to one of its ETDs or an ETD copy is corrupted, the user will still have access to another copy of ETD. The above system will be used for NDLTD (Networked Digital Library of Theses and Dissertations).  NDLTD is an initiative to encourage the creation of ETDs by student authors, and to make ETDs easily accessible to students via World Wide Web, thus improving graduate education. There are currently over 150 members in NDLTD. Users can browse or search ETDs through the NDLTD website. The NDLTD website also provides a union catalog to search for ETDs.  The Open Archives Initiative (OAI) is dedicated to solving problems of digital library interoperability. OAI has developed a metadata harvesting protocol to support streaming of metadata from one repository to another, ultimately to a provider of user services such as browsing, searching, or annotation. An OAI harvester implements the OAI protocol for metadata harvesting.  We use an OAI harvester to harvest metadata about ETDs and then a simple web crawler is used to get the actual data and store it on a local machine. This ensures that we have a local copy of data even if the publisher of data is somehow unable to provide us with data. Our OAI harvester harvests metadata, which was not harvested since the last time it was run. Hence, updating the mirror site is easily accomplished. This is a very effective scheme, which can be used to mirror any collection of data, provided the collection has an associated OAI server.",2003
Global Optimization for Polynomial Programming Problems Usingm-homogeneous Polynomial Homotopies,"A polynomial programming problem is a nonlinear programming problem where the objective function, inequality constraints, and equality constraints all consist of polynomial functions.  The necessary optimality conditions for such a problem can be formulated as a polynomial system of equations, among whose zeros the global optimum must lie.  This note applies the theory of m-homogeneous polynomials in Cartesian product projective spaces and recent homotopy algorithms to significantly reduce the work of a naive homotopy approach to the polynomial system formation of the mecessary optimality conditions.  The m-homogeneous approach, providing the global optimum, is practical for small problems.  For example, the geometric modeling problem of finding the distance between two polynomial surfaces is a polynomial programming problem.  Also discussed is a prototype structural design problem.",1989
"Rapid Modeling, Prototyping, and Generation of Digital Libraries- a Theory-Based Approach","Despite some development in the area of DL architectures and systems, there is still little support for the complete life cycle of DL development, including requirements gathering, conceptual modeling, rapid prototyping, and code generation and reuse. Even when partially supported, those activities are uncorrelated within the current systems, which can lead to inconsistencies and incompleteness. Moreover, the current few existing approaches are not supported by comprehensive and formal foundations and theories, which brings problems of interoperability and makes it extremely difficult to adapt and tailor systems to specific societal preferences and needs of the target community. In this paper, having the 5S formal theoretical framework as support, we present an architecture and a family of tools that allow rapid modeling, prototyping, and generation of digital libraries. 5S stands for Streams, Structures, Spaces, Scenarios, and Societies and is our formal theory for DLs. 5SL is a domain-specific, declarative language for DL conceptual modeling. 5SGraph is a visual modeling tool that helps designers to model a digital library without knowing the theoretical foundations and the syntactical details of 5SL. Furthermore, 5SGraph maintains semantic constraints specified by a 5S metamodel and enforces these constraints over the instance model to ensure semantic consistency and correctness. 5SGraph also enables component reuse to reduce the time and efforts of designers. 5SLGen is a DL generation tool that takes specifications in 5SL and a set of component pools and generates portions of a running DL system. The outputs of 5SLGen include user interface prototypes, in a generic UI markup language, for validation of services behavior and workflow representations of the running system, generated to support the desired scenarios.",2003
Design and Evaluation of Menu Systems for Immersive Virtual Environments,"Interfaces for system control tasks in virtual environments (VEs) have not been extensively studied. This paper focuses on various types of menu systems to be used in such environments. We describe the design of the TULIP menu, a menu system using Pinch Gloves™, and compare it to two common alternatives: floating menus and pen and tablet menus. These three menus were compared in an empirical evaluation. The pen and tablet menu was found to be significantly faster, while users had a preference for TULIP. Subjective discomfort levels were also higher with the floating menus and pen and tablet.",2001
One-Dimensional Edge Detection and Representation,"One reason no single edge detector has been found that satisfies the requirements of a wide range of image analysis applications is that existing edge detectors often produce  unjustifiable interpretations  of edge  regions without  sufficient contextual  or external  information.  One-  dimensional edge semantics are examined in detail,  and a  one-dimensional edge-detector is proposed whose output is not just a local edge point assertion but a data structure. This data structure contains alternative interpretations of edge crossections as global gray level transition regions of  variable widths. The detector is parameter-free, and a set  of descriptive edge features is proposed.",1979
The Promise (and Reality) of Multidisciplinary Design Optimization,"Modern aerospace vehicle design requires the interaction of multiple disciplines, traditionally processed in a sequential order. Multidisciplinary optimization (MDO), a formal methodology for the integration of these disciplines, is evolving towards methods capable of replacing the traditional sequential methodology of aerospace vehicle design by concurrent algorithms, with both an overall gain in product performance and a decrease in design time.  This paper discusses the obstacles to MDO, and presents a parallel MDO paradigm using variable-complexity modeling and multipoint response surface approximations for the particular instance of the design of a high speed civil transport (HSCT).  This paradigm interleaves the disciplines at one level of complexity, and processes them hierarchically at another level of complexity, achieving parallelism within disciplines, rather than across disciplines.  A master-slave paradigm manages a coarse grained parallelism of the analysis and optimization codes required by the disciplines showing reasonable speedups and efficiencies on an Intel Paragon.",1995-08-01
"Principles of Simulation Model Validation, Verification, and Testing","Sufficient experience has been gained over the last decade in simulation model validation, verification, and testing (VV&T) to establish basic principles and its characteristics.  This paper presents 15 principles of simulation model VV&T.  These principles help the researchers, practitioners, and manaagers better understand what model VV&T is all about.  They serve to provide the underpinnings for the VV&T techniques that can be used throughout the life cycle of a simulation study.  Understanding and applying these principles is crucially important for the success of a simulation study.",1994-06-01
Detecting Delaminations in Composite Structures Using Anti-Optimization,"The present study proposes a detection technique for delaminations in a laminated composite structure.  The proposed technique optimizes the spatial distribution of harmonic excitation so as to magnify the difference between the delaminated and intact structure.  The technique is evaluated by numerical simulation of two-layered aluminum beams. Effects of measurement and geometric noises are included in the analysis.  A finite element model for a delaminated composite, based on the layer-wise laminated plate theory in conjunction with a step function to simulate delaminations, is used.",1994-07-01
Sorting by Bounded Block-Moves,"Given a permutation pi, a block-move is an operation that switches two adjacent blocks of elements in pi.  The problem of finding the minimum number of block-moves required to sort pi has applications in computational biology, particularly in the study of genome rearrangements.  This paper investigates variants of the problem where bounds are imposed on the lengths of the blocks moved.  Algorithms and reduction results are presented for these variants.",1997-05-01
A Homotopy Method Applied to Elastica Problems,"The inverse problem in nonlinear (incompressible) elastica theory, where the end positions and inclinations rather than the forces and moment are specified, is considered. Based on the globally convergent Chow-Yorke algorithm, a new homotopy method which is simple, accurate, stable,  and efficient is developed. For comparison, numerical results of some  other simple approaches (e.g., Newton's method based on shooting or  finite differences, standard embedding) are presented. The new homotopy method does not require a good initial estimate, and is guaranteed  to have no singular points. The homotopy method is applied to the problem of a circular elastica ring subjected to N symnetrical point loads, and  numerical results are given for N = 2,3,4.",1980
Modeling of MULTI SAFE Protection Enforcement Processes with Extended Petri Nets,"The various kinds of access decision dependency within a predicate-based model of database protection are classified according to cost of enforcement. Petri nets and some useful extensions are described. Extended Petri nets are used to model the flow of messages and data during protection enforcement within MULTISAFE, a multimodule system architecture for secure database management. The model demonstrates that stated criteria for security are met within MULTISAFE. Of particular interest is the modeling of data dependent access conditions with predicates at Petri net transitions.",1981
Computational Experience with the Chow-yorke,"The Chow-Yorke algorithm is a nonsimplicial homotopy-type method for computing Brouwer fixed points that is globally convergent. It is efficient and accurate for fixed point problems. L. T. Watson, T. Y. Li, and C. Y. Wang have adapted the method for zero finding problems, the nonlinear complementarity problem, and nonlinear two-point boundary value problems. Here theoretical justification is given for applying the method to some mathematical programming problems, and computational results are presented.",1978
ETANA-DL: Managing Complex Information Applications - an Archaeology Digital Library,"Archaeological research results in the generation of large quantities of heterogeneous information managed by different projects using custom information systems. We will demonstrate a prototype Digital Library (DL) for integrating and managing archaeological data and providing services useful to various user communities. ETANA-DL is a model-based, componentized, extensible, archaeological DL that manages complex information sources using the client-server paradigm of the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH).",2004
Distributed Process Creation Within a Shared Data Space Framework,"This paper describes the design and implementation of a remote process instantiation mechanism which is consistent with the Linda paradigm and semantics of the EVAL operation, and which uses shared data space as the medium for passing process and environment parameters.  The motivation fo such an implementation stems from our effort to rehost a uniprocessor version of the Linda computational system to a network of workstations.  The baseline version relies on the semantics of the UNIX fork() system call to create processes and to pass the proper execution parameters to them.  In creating a distributed version of the Linda environment, two major issues are addressed: (1) how to instantiate a remote process that knows where, among several possibilities, to begin its execution, and (2) how to communicate the proper runtime values of relevant variables to each new remote Linda process. Guiding our implementation was a desire to employ existing interprocess communication facilities, e.g., shared data space, to pass process creation and execution parameters.",1994
Algebraic Specification of Complex Data Types,"This paper discusses algebraic data type specifications that employ product sets as carriers. This leads to simplifications for certain data types. In particular, the method permits the specification of a complex type by the straight forward translation of an intuitive model of the type into a formal definition, provided that the model is conceived in terms of other well defined types. The concept is illustrated by two examples.",1978
Testbed Evaluation of Navigation and Text Display Techniques in an Information-Rich Virtual Environment,"Information-Rich Virtual Environment (IRVE) is a virtual environment (VE) that is enhanced with the addition of related abstract information. Such an environment conveys rich information set that can make the VE more useful if provided with a useful interface and efficient interaction techniques. This motivates our current research goals to investigate the interface and interaction issues involved in IRVE. The fundamental question is how to access and display information in an effective way. This paper presents the first controlled experiment using a travel testbed based on our text layout taxonomy. We investigated two manipulation-based navigation techniques: Hand-Centered Object Manipulation Extending Ray0casting (HOMER) and Go-Go based navigation, and two text layout techniques; within-the-world display (WWD) and heads-up display (HUD). Four search tasks with repeated measures were performed to measure subjects performance in a densely packed environment. We found that using Go-Go based navigation combined with HUD techniques is significantly better than the other three combinations for difficulty search tasks. HUD enabled better performance than WWD and Go-Go technique enabled better performance than HOMER technique ro most of the tasks. Also users preferred the combination of Go-Go and HUD for all tasks. Such results on their own, or combined with specific application characteristics, can provide valuable design guidelines for IRVEs.",2003
Who Reuses What? C++ Libraries and Programmer's Traits,"Software Reuse is one key to increasing productivity within the software development process.  By reusing existing software, time and effort are saved in the coding, testing, and maintenance phases of a software product.  The goals of this research involve examining the process of software reuse and attempt to characterize it.  A set of experiments have been run to determine the effects of software reuse in the object oriented paradigm.  This paper focuses on the traits of reusers, what type of reuse they perform, and the common characteristics of the classes being reused.  In particular, the traits of programmers who reuse often are identified as well as the traits of those who reuse infrequently.  Also, do programmers perform black box reuse (reusing library classes without modification) or do they perform white box reuse (using inheritance to modify a library class to the desired functionality) or do they reuse at all?  Lastly, a first attempt at characterizing the reusability of a C++ class is presented.  Several traits are identified as making a class more reusable to programmers.",1994-05-01
A Variant of the Fisher-Morris Garbage Compaction Algorithm,"The garbage compactification algorithm described works in linear time and, for the most part, does not require any work space. It combines marking and compactification into a two-step algorithm. The first step marks all non-garbage cells and, at the same time, rearranges the pointers such that the cells can be moved, the second step performs the actual compatification.",1980
Security of Database Systems: Authorization Features and Mechanisms,"Database security has become an essential issue in assuring the integrity, protection, and reliability of the data stored in a database management system (DBMS). The authorization mechanism is the component of the database security system which has the primary responsibility of safeguarding the previously defined data and access rules needed for database access control. The data and rules for authorization control assist in the enforcement of access controls regarding the list of authorized users, the data objects which the authorized users are allowed to manipulate, and the operations that these users can perform on these objects. As part of its tasks the authorization mechanism can grant or deny access to any user or group of users as appropriate.",1986
Application of the Analytic Hierarchy Process to Complex System Design Evaluation,"This paper examines the use of the Analytic Hierarchy Process (AHP) to weight indicators used in the evaluation of complex systems designs which involve software, hardware, and humanware.  Since such a comprehensive easily include hundreds of system quality indicators, evaluators need a technique to ensure the identification and emphasis of salient indicators in the determination of the quality of the design. The AHP is a popular technique for determining relative worth among a set of elements.  In the present work, we introduce AHP with a simple example, then illustrate the application of the AHP to design evaluation using a subset of indicators from the human component of a system.  We note in some detail issues which require added attention when applying AHP to this domain.  The issues include indicator selection, dealing with large numbers of indicators, incorporating group judgements, and conflict resolution.  We found AHP to be an effective tool for use in assigning weights criticality in indicator-based design evaluation, and propose elements of an environment in which the use of AHP is easily incorporated.",1994-06-01
"Massively Time-Parallel, Approximate Simulation of Loss QueueingSystems","A time-parallel simulation obtains parallelism by partitioning the time domain of the simulation.  An approximate time-parallel simulation algorithm named GG1K is developed for acyclic networks of loss FCFS G/G/1/K queues.  In the first phase, a similar system (i.e., a G/G/1/infinity queue) is simulated using the GLM algorithm.  Then the resultant trajectory is transformed into an approximate G/G/1/K trajectory in the second phase.  The closeness of the approximation is investigated theoretically and experimentally.  Our results show that the approximation is highly accurate except when K is very small (e.g., 5) in certain models.  The algorithm exploits unbounded parallelism and can achieve near-linear speedup when the number of arrivals simulated is sufficiently large.",1994-02-01
Object Oriented Metrics: Generation and Application,"Object oriented software has been cited by many [Cox86, Meye87, Coad90, Booc91] as a cure to some of the common problems occurring in software development.  In particular, the object oriented paradigm, through inheritance, polymorphism, and encapsulation, allows for more reusable software components, easier software maintenance, and reduced software complexity.  However, in order to make these claims, researchers will need to quantitatively measure various attributes of software (e.g., complexity, reusability, and maintainability).  In order to efficiently and accurately use software metrics, it is necessary to automate the metric collection process.  The metrics group at Virginia Tech has developed a tool (the Metrics Generator) that, through an interactive interface, allows software developers to generate various software metrics from Ada, C, Classic Ada, or C++ source.  The most recent use of the tool was a study by Li and Henry [LiWe93] into the maintainability of object oriented source code.  The next use of the tool involves the object oriented paradigm and software reusability.",1994-05-01
The Cellular Automata Paradigm for the Parallel Solution of HeatTransfer Problems,"This paper describes the numerical solution of heat transfer problems using cellular automata.  While traditional methods offer high performance on uniprocessor machines, their performance is limited on distributed memory multiprocessors by communication bottlenecks caused by the interdependence of the equations.  Using a cellular automata formulation, these bottlenecks can be avoided, and performance greater than that obtained by parallelizing traditional algorithms can be achieved.  This paper gives an overview of the cellular automata paradigm and specific examples of solutions to a hyperbolic and a parabolic problem.  The accuracy of the method is verified by comparisons of the results with analytical solutions and with results produced by other techniques.",1994-04-01
A Queueing Network Analysis of Dynamic Recon-figurability in a Hierarchical Information Network,"Using a queueing network model of a hierarchical information network, we compare the effect of limited dynamic reconfiguration on expected transmission delays. Two distinctive features characterize the queueing network model. First, the assignment of a set of weights to the nodes dependent on the hierarchical level reflects the increasing importance of information as it is transferred to higher levels. Second, the dynamic hierarchy requires a communications protocol that partitions the analysis of network delay into three periods: regular operation, reconfiguration, and adjustment. Characterization of the performance of the dynamic hierarchy entails the description of message transmission delay as a composite of the three periods.",1992
Segmentation of Images with Incompletely Specified Regions,Sometimes image regions are incompletely specified in  the sense that only a few representative points in each  region are known. In order to determine a segmentation of  such an image it is necessary to construct the region  boundaries. A fast algorithm called a diffusion algorithm  is given that determines region boundaries by diffusing  region labels outward from the known points.,1979
A Model Formulation of User Interface for Statistical Databases with Perturbed Responses,"A statistical database is defined as a collection of N records used to produce summary information only.  Each record contains confidential category and data fields of the same fixed length.  Category fields are used to identify and select records, while data fields hold other information, mostly numerical.",1986
Continuous Homotopies for the Linear Complementarity Problem,"There are various formulations of the linear complementarity problem as a Kakutani fixed point problem, a constrained optimization, or a nonlinear system of equations. These formulations have remained a curiosity since not many people seriously thought that a linear combinatorial problem should be converted to a nonlinear problem. Recent advances in homotopy theory and new mathematical software capabilities such as HOMPACK indicate that continuous nonlinear formulations of linear and combinatorial problems may not be far-fetched. Several different types of continuous homotopies for the linear complementarity problem are presented and analyzed here, with some numerical results. The homotopies with the best theoretical properties (global convergence and no singularities along the zero curve) turn out to also be the best in practice.",1987-05-01
Usort: An Efficient Hybrid of Distributive Partitioning Sorting,A new hybrid of Distributive Partitioning Sorting is described and tested against Quicksort on uniformly distributed items. Pointer sort versions of both algorithms are also tested.,1981
Acquisition of an Interactive Computing System for Academic Use: A Case Study,"The acquisition of a large-scale computer system is a complex and  important task that universities face periodically.  The large capital  expenditures and the always expanding need for computing services ensure an  increasing importance.  Virginia Tech recently made such an acquisition.  This paper describes the evaluation procedures leading to the acquisition, beginning with needs assessment and concluding with system selection.  The acquisition of a computing system,  in this instance a system  primarily for interactive instructional support of undergraduates, is a  decision that is subject to a variety of influences  technical,  managerial, political, and personal.  This paper describes the authors'  attempts (as then Associate Director of the Computing Center and then Head  of the Computer Science Department,  respectively) to deal with these  influences through the use of quantitative techniques, behavioral analysis, and common sense.",1981
ETANA-DL: A Digital Library for Integrated Handling of Heterogeneous Archaeological Data,"Archaeologists have to deal with vast quantities of information, generated both in the field and laboratory. That information is heterogeneous in nature, and different projects have their own systems to store and use it. This adds to the challenges regarding collaborative research between such projects as well as information retrieval for other more general purposes. This paper describes our approach towards creating ETANA-DL, a digital library (DL) to help manage these vast quantities of information and to provide various kinds of services. The 5S framework for modeling a DL gives us an edge in understanding this vast and complex information space, as well as in designing and prototyping a DL to satisfy information needs of archaeologists and other user communities.",2004
An Assessment of SEES Based on Operational Experiences,"During the Fall of 1995 a modified form of SEES (Software Engineering Evaluation System), denoted SEES, was defined and used in a study to examine the value added of Independent Verification and Validation.  A follow-on study, and topic of this report, focuses on an assessment of SEES based on operational experiences gained from the 1995 study.  The report partitions its findings relative to phases of the software development process.",1997-03-01
Queue Layouts and Staircase Covers of Matrices,"A connection between a queue layout of an undirected graph and a staircase cover of its adjacency matrix is established.  The connection is exploited to establish a number of combinatorial results relating the number of vertices, the number of edges, and the queue number of a queue layout.  The staircase notion is generalized to that of an (h,w)- staircase, and an efficient algorithm to optimally cover a matrix with (h,w)- staircases is presented.  The algorithm is applied to problems of monotonic subsequences and to the maxdominance problem of Atallah and Kosaraju.",1994-06-01
Sluice: A Java-Based Framework for Collaborative Interactive Modular Visualization Environments,"We present Sluice, a framework for constructing collaborative, interactive, modular visualization environments (CIMVEs) for use on the World Wide Web.  Sluice supports using Java to build modules that create, manipulate, and generate visualizations of tables of data. Sluice provides necessary infrastructure for a CIMV, including interface specifications for methods that modules of the MVE must implement, event propagation to support interaction and dynamic updating, and mechanisms for efficiently supporting collaboration.  Our fundamental description of data comes in the form of table which is filtered, extended, or otherwise manipulated to produce a new ""view"" of the data at each module. We use the observer/observable design pattern to support dynamic updating of visualizations downstream in the dataflow network.  Sluice also supports on-the-fly manipulation of the dataflow network.  Sluice uses functionality provided by the JavaBeans package to support introspection of modules.  This allows CIMVE implementations to discover, query, and set the values of properties of modules at runtime. After describing the design rationale, capabilities, and implementation of Sluice, we then describe our prototype CIMVE named SIEVE.",1997-10-01
Visualizing Case Studies,"Case studies contain valuable information about development records. They are, however, usually presented as textual documents. Here we begin an initial design for viewing case study data. A prototype for viewing case study data is created using information visualization techniques for the purpose of testing against existing techniques. The evaluations show that the new design is just as usable as existing techniques, making it suitable for future development. Future work may show our design can accomplish tasks both faster and more efficiently.",2004
Diagnostic Assistance Using Digraph Representation of Discrete Event Simulation Model Specifications,"Automated diagnosis of digraph representations of discrete event simulation models is illustrated as an effective model verification technique, applicable prior to coding the model in an executable language. The Condition Specification is shown to provide an effective representation, from which automated analysis can initiate with a digraph extraction. Subsequent diagnostic simplification techniques are applied to the digraph, either automatically or in concert with the modeler.",1986-03-01
Expert Retrieval for Computer Message Systems,"This paper describes how information storage and retrieval (IS&R) and artificial intelligence (AI) methods can be integrated with modem computers and networks to provide access for broad classes of users to archives of electronic: mail, digests, and bulletin boards.  A status report is given on the COmposite Document Expert/extended/effective Retrieval project, designed to employ communities of experts, operating on multiple communicating computers, for free text analysis, indexing, and retrieval.  Details of the document-type expert are included to illustrate the approach.",1986-06-01
Credibility Assessment of Simulation Results: The State of the Art,The purpose of this paper is to provide a state-of-the-art survey of credibility assessment of simulation results and suggest some future research directions. A hierarchy of the credibility assessment is introduced and the state-of-the-art survey is presented with respect to this hierarchy. A glossary is provided to alleviate the lack of standard terminology. The future research calls upon looking at the global picture when conducting a simulation study and being concerned with all of the eleven credibility assessment stages not just model validation and programmed model verification.,1986
Human Factors and Software Reuse: The Manager's Impact,"This paper describes the results of a controlled experiment designed to evaluate the impact of human factors on software reuse. The experiment concludes that (1) software reuse promotes higher productivity than no reuse, (2) reuse resulting from both moderate and strong encouragement promote higher productivity than no reuse, and (3) while strong managerial encouragement did not create a significant difference in productivity, it does tend to promote improper reuse activities.",1992
Procedure for Evaluating Human-Computer Interface Development Tools,"Human-computer interface development tools are interactive systems that support production and execution of the human-computer interface. With their recent proliferation, evaluations and comparisons are constantly done, but without a formal, structured approach.  Addressing these problems is difficult, largely because of the relative newness of such tools, because of the many different kinds of systems that are called UIMS, and because of their inherent complexity.  These tools are complex because human-computer interfaces, which produce tools, are complex.",1990
Using Virtual Environments in the Teaching of Computer Graphics,"Education has long been touted as an appropriate application area for immersive virtual environments (VEs), but few immersive applications have actually been used in the classroom, and even fewer have been compared empirically with other teaching methods. This paper presents VENTS, a novel immersive VE application intended to teach the concept of the three-dimensional (3D) normalizing transformation in an undergraduate computer graphics class. VENTS was developed based on key principles for the use of VEs in education, systematically evaluated for usability, and refined based on the results of this evaluation. Students in a university course used VENTS, and their learning was compared to that of other students who either attended a lecture on the topic or used a 3D desktop application covering the same material. The results of pre- and post-tests showed a larger percent increase in test score for the VE group than the desktop or lecture groups, although these differences are not statistically significant.",2003
Quantitative Relative Comparison of CFD Simulation Uncertainties for a Transonic Diffuser Problem,"Different sources of uncertainty in CFD simulations are illustrated by a detailed study of two-dimensional, turbulent, transonic flow in a converging-diverging channel. Runs were performed with the commercial CFD code GASP using different turbulence models, grid levels, and flux-limiters to see the effect of each on the CFD simulation uncertainties. Two flow conditions were studied by changing the exit pressure ratio: the first is a complex case with a strong shock and a separated flow region, the second is the weak shock case with no separation. The uncertainty in CFD simulations has been studied in terms of four contributions: (1) discretization error, (2) error in geometry representation, (3) turbulence model, and (4) the downstream boundary condition. In this paper, we have quantified the relative contribution and the importance of each source of uncertainty and shown the level of scatter in results that a well informed CFD user may obtain in a typical design activity. The nozzle efficiency results obtained in this study showed that the range of variation for the strong shock case was much larger than that observed in the weak shock case. The discretization errors were up to 6% and the relative uncertainty originating from the selection of different turbulence models was as large as 9% for the strong shock case. Furthermore, the results demonstrated that grid convergence is not achieved with grid levels that have moderate mesh sizes and showed that highly refined grids are required to obtain solutions with an acceptable level of accuracy in design problems that involve simulations of complex flow fields. The results illustrated the interaction of different sources of uncertainty and showed that the magnitudes of numerical errors are influenced by the physical models used.",2004
Pipeline Implementation of Cellular Automata for Structural Design on Message-Passing Multiprocessors,"The inherent structure of cellular automata is trivially parallelizable and can directly benefit from massively parallel machines in computationally intensive problems. This paper presents both synchronous and pipeline parallel implementations of cellular automata on distributed memory (message-passing) architectures. A structural design problem is considered to study the performance of the various cellular automata implementations. The synchronous parallel implementation is a mixture of Jacobi and Gauss-Seidel style iteration, where it is more Jacobi like as the number of processors increase. Therefore, it exhibits divergence because of the mathematical characteristics of Jacobi matrix iteration for the structural problem as the number of processors increases. The proposed pipeline implementation preserves convergence by simulating a pure Gauss-Seidel iteration. Numerical results for analysis and design of a cantilever plate made of composite material show that the pipeline update scheme is convergent and successfully generates optimal designs.",2003
Schema Mapper: A Visualization Tool for DL Integration,"Schema mapping is a challenging problem. It has come to the fore in recent years; there are important applications like database schema integration and, more recently, digital library merging of heterogeneous data. Previous studies have approached the schema mapping process either from algorithmic or visualization perspectives, with few integrating both. With Schema Mapper we demonstrate a semi-automatic tool for schema integration that combines a novel visual interface with an algorithm-based recommendation engine. Schemas are visualized as hyperbolic trees (see Fig. 1), thus allowing more schema nodes to be displayed at one time. Matches to selections are recommended to the user, which makes the mapping operation easier and faster.",2005
A Classification of Interactive Map Software,"Map software programs can be different in a number of ways. This paper reports on the classification of interactivity produced as a result of surveying forty map applications. Specific examples from the survey are referenced as we describe different navigation styles, different levels of collaboration support, and various data options. Unique combinations within the classification are also discussed as we explore ideas for new interaction technqiues.",2001
An Architecture for Multischeming in Digital Libraries,"In this paper we discuss the problem of handling many classification schemes within the context of a single digital library concurrently, which we term multischeming. We discuss how to represent which category describes an object in the digital library in this system, as well as the workings of the browsing process which is performed by the user. We motivate this problem as related to digital library interoperability, and propose an architecture for representation of classification schemes in the digital library which solves the problem. We also discuss its implementation in the CITIDEL project.",2003
Requirements Management Tools: A Quantitative Assessment,"This report is primarily aimed at people with some background in Requirements Engineering or practitioners wishing to assess tools available for managing requirements. We provide a starting point for this assessment, by presenting a brief survey of existing Requirements Management tools. As a part of the survey, we characterize a set of requirements management tools by outlining their features, capabilities and goals. The characterization offers a foundation to select and possibly customize a requirements engineering tool for a software project. This report consists of three parts. In Part I we define the terms requirements and requirements engineering and briefly point out the main components of the requirements engineering process. In Part II, we survey the characteristics and capabilities of 6 popular requirements management tools, available in the market. We enumerate the salient features of each of theses tools. In Part III, we briefly describe a Synergistic Environment for Requirement Generation. This environment captures additional tools augmenting the requirements generation process. A description of these tools is provided. In the concluding section, we present a discussion defining the ideal set of characteristics that should be embodied in a requirements management tool. This report is adapted from a compendium of assignments that were prepared by the students in a Requirements Engineering class offered in the Department of Computer Science at Virginia Tech.",2003
An O(n log n) Algorithm for Finding Minimal Perfect Hash Functions,"We describe the first practical algorithm for finding minimal perfect hash functions that can be used to access very large databases. This method extends earlier work wherein an O(n3) algorithm was devised, building upon prior work by Sager that described an O(n4) algorithm. Our new O(n log n) expected time algorithm makes use of three key  insights: applying randomness whereever possible, ordering our search for hash functions based on the degree of the vertices in a graph that represents word dependencies, and viewing hash value assignment in terms of adding circular patterns of related words to a partially filled disk.  While ultimately applicable to a wide variety of data and file access needs, this algorithm has already proven useful in aiding our work in improving performance of CD-ROM systems and our construction of a Large External Network Database (LEND) for semantic networks and hypertext/hypermedia collections.  Virginia Disc One includes a demonstration of a minimal perfect hash function running on a PC to access a 130,198 word list on that CD-ROM, and several other microcomputer, minicomputer, and parallel processor versions and applications of our algorithm have also been developed.  Tests with a French word list of 420,878 entries and a library catalog key set with over 1.2 million keys have shown that our methods work with very large databases.",1989
An Alternative to Full Configuration Interaction Based on a Tensor Product Decomposition,A new direct full variational approach exploits a tensor (Kronecker) product decompositions of the Hamiltonian. Explicit assembly and storage of the Hamiltonian matrix is avoided by using the Kronecker product structure to form matrix-vector products directly from the molecular integrals. Computation-intensive integral transformations and formula tapes are unnecessary. The wave function is expanded in terms of spin-free primitive sets rather than Slater determinants or configuration state functions and is equivalent to a full configuration interaction expansion. The approach suggests compact storage schemes and algorithms which are naturally suited to parallel and pipelined machines.,1989
Comparing Classroom Note Taking across Multiplatform Devices,"Many educators have suggested that note taking can be beneficial for the students' educational growth. Note taking is the core activity for students in a classroom and there been a large amount of research conducted, both from industry and from academia, into facilitating the note taking process. As such, there are many available systems for taking notes. However, what has not been given as much attention is how different devices, such as Tablet PCs and PDAs, effect this task. In this paper, we study students' current note taking behavior and the changes caused by the use of different platforms for this activity. Our goal is to provide ideas and general design guidelines for future note taking systems.",2004
Spatial and Temporal Path Planning,"For robots to move out of the lab and into the real-world, they must be able to plan routes not only through space but through time as well. The introduction of a time factor to the planning process implies that robots must reason about other processes and agents that move through space independently of the robot's actions. This thesis presents an integrated route planner and spatial representation system for planning real-time paths through dynamic domains called Robonav. Robonav will find the safest, most efficient route through time and space as described by an evaluation function. Due to the design of the spatial representation and the mechanics of the algorithm, Robonav has an isomorphic mapping onto a machine with a highly parallel SIMD architecture. When Robonav is operated in a predictable domain; paths are found in O(p) time (where p is the length of a path). In unpredictable domains, where Robonav is operated in incremental mode, paths are found and executed in $O(p^2)$ time.",1987
Multirate explicit Adams methods for time integration of conservation laws,This paper constructs multirate linear multistep time discretizations based on Adams-Bashforth methods. These methods are aimed at solving conservation laws and allow different timesteps to be used in different parts of the spatial domain. The proposed family of discretizations is second order accurate in time and has conservation and linear and nonlinear stability properties under local CFL conditions. Multirate timestepping avoids the necessity to take small global timesteps - restricted by the largest value of the Courant number on the grid - and therefore results in more efficient computations. Numerical results obtained for the advection and Burgers' equations confirm the theoretical findings.,2007-08-01
Effect of flexible joints on the stability and large deflections of a triangular frame,An isosceles triangular frame with rotationally resistive joints under a tip load is studied. The large in-plane deformation elastica equations are formulated. Stability analysis shows the frame can buckle symmetrically or asymmetrically. Post-buckling behavior showing limit load and hysteresis are obtained by shooting and homotopy numerical algorithms. The behavior of a frame with rigid joints is studied in detail. The effects of joint spring constant and base length are found.,2007
Developing an Automated Procedure for Evaluating Software Development Methodologies and Associated Products,"Over the past decade the demand for increasingly complex software systems has risen dramatically [PARD 85]. Recognizing the fact that such systems cannot be developed effectively through ad hoc means, the software engineering community has continued to investigate the fundamental concepts, development methodologies, and automated tools to assist in the software development process. For example, SREM [ALFM 85] and SADT [SOFT 76, ROSD 77] are methodology based environments that focus on supporting particular phases of the software life cycle. SCR [CLEP 84, HENK 78] and DARTS [GOMH 84], on the other hand, are methodologies that emphasize specific goals, e .g ., reducing software development costs and designing real-time systems, respectively.  This steady proliferation of design methodologies, however, is not without its price. In particular, users find increasing difficulty in choosing an appropriate methodological approach and recognizing reasonable expectations of a design or development methodology. These concerns motivated an initial research effort that led to a procedural approach for evaluating software development methodologies [ARTJ 86].",1987
The Pagenumber of Genus g Graph is 0(g),"In 1979, Berhart and Kainen conjectured that graphs of fixed genus g greater than or equal to 1 have unbounded pagenumber.  This proves that genus g graphs can be embedded in 0(g) pages, thus disproving the conjecture.  An Omega(square root of g) lower bound is also derived. The first algorithm in the literature for embedding an arbitrary graph in a book with a non-trivial upper bound on the number of pages is presented.  First, the algorithm computes the genus g of a graph using the algorithm of Filotti, Miller, Reif (1979), which is polynomial-time for fixed genus.  Second, it applies an optimal-time algorithm for obtaining an 0(g)-page book embedding.  We give separate book embedding algorithms for the cases of graphs embedded in orientable and nonorientable surfaces.  An important aspect of the construction is a new decomposition algorithm, of independent interest, for a graph embedded on a surface.  Book embedding has application in several areas, two of which are directly related to the results obtained: fault-tolerant VLSI and complexity theory.",1990
HOMPACK: A Suite of Codes for Globally Convergent Homotopy Algorithms,"There are algorithms for finding zeros or fixed points of nonlinear systems of equations that are globally convergent for almost all starting points, i.e., with probability one.  The essence of all such algorithms is the construction of an appropriate homotoppy map and then tracking some smooth curve in the zero set of this homotopy map. HOMPACK provides three qualitatively different algorithms for tracking the homotopy zero curve: ordinary differential equation based, normal flow, and augmented Jacobian matrix.  Separate routines are also provided for dense and sparse Jacobian matrices.  A high level driver is included for the special case of polynomial systems.",1990
Development of a Resource Center for Introductory Computer Science,"This paper reviews the experiences of developing a Computer Sciences Resource Center through the joint efforts of faculty and students, at minimal Cost to the institution, yet with maximal opportunity for learning experiences by students in even the initial offerings of the courses. Although especially directed at the so-called ""Computer Appreciation"" courses, the resources developed through this program are applicable to other computer science courses and to other presentations across the college campus.",1976
A Comparison of Discrete Event Simulation Courses Based on a Small Sample Survey,No abstract available.,1976
Formal Descriptors for Hardware Simulation,"This paper reviews the current status of an ongoing effort  to develop a hardware description language which would be  suitable for use as both a design tool and a documentor.  Included in the requirements for this language would be the  necessity for the language to function not only in many  areas, such as automated design and verification, or testing  and simulation, but also at many levels. That is, to range  over such applications as circuit design at one end of a  spectrum to the validation of systems configurations at the  other end.  This paper views the language requirements from three  points of view,  i) the subjective (human) elements usually associated  with the syntactic features of the language,  ii) the minimal semantic elements to be provided and  the structures (both program and data) which are  necessary, and  iii)  the features  to be included in  order to  facilitate the formal verification of the conformance  of the descriptor to preselected attributes.  The work described in this paper is based on continuing  research regardinq the nature of formal descriptor  techniques, on their applicability to automated theorem  provinq and techniques for  improving the teaching of  computer related languages currently under way at Virginia  Polytechnic Institute and State University.",1976
A Call for Integrating Advanced Information Retrieval Models with CD-ROM/Microcomputer Systems,"Recent advances in computer hardware and storage devices will allow inexpensive personal systems to be used by individuals to rapidly access vast collections of text. Research into database management, artificial intelligence, and information retrieval can all be applied to develop advanced retrieval systems. Retrieval models based on browsing, extended Boolean, vector, probabilistic, and artificial intelligence approaches have all been advanced as more effective for searchers than conventional methods. The CODER project aims to integrate these techniques. Ultimately it is hoped that CD-ROM based information retrieval systems will be released with many of the capabilities mentioned.",1986-06-01
Model Generation Issues in a Simulation Support Environment,No longer available as a technical report.  Contact authors re reprints of published article.,1990
On Computing a Buy/copy Policy Using the Pitt-Kraft Model,"The Pitt-Kraft model of buying versus photocopying results in a small, but complex, nonlinear program. This paper identifies a Kuhn-Tucker point and demonstrates that for certain parameter values it is not optimal. A policy generation procedure is presented; the purpose is to prevent convergence of a primal algorithm to this inferior policy, which satisfies the Kuhn-Tucker optimality conditions.",1975
Parallel scalability study of three dimensional additive Schwarz preconditioners in non-overlapping domain decomposition,"In this paper we study the parallel scalability of variants of additive Schwarz preconditioners for three dimensional non-overlapping domain decomposition methods. To alleviate the computational cost, both in terms of memory and floating-point complexity, we investigate variants based on a sparse approximation or on mixed 32- and 64-bit calculation. The robustness of the preconditioners is illustrated on a set of linear systems arising from the finite element discretization of elliptic PDEs through extensive parallel experiments on up to 1000 processors. Their efficiency from a numerical and parallel performance view point are studied.",2007
Fault Location in a Semiconductor Random-access Memory Unit,"A semiconductor random-accessor memory unit (RAM unit) is a connection  of RAM chips, Data Cable, Chip Select Cable, and Address Cable so that each  storage element can be selected for writing or reading independent of previous  write or read. The faulty RAM unit is represented by a model consisting of  four types of faults: RAM chip faults, D-fault, CS-fault, and A-fault. The  testing of a RAM unit and locating faults to the RAM chips or wires in the  various cables is considered. A set of six tests has been designed to diagnose  the faults in the model. The tests are used in defining a relation, ""test tj is  invalid"". on the fault model. The diagnostic graph of a RAM unit is drawn by  using this relatioon and a sequence in which tests have to be performed  obtained from this graph. By using this sequence faulty components in a RAM  unit are located when at most one type of fault in the model is present.  The symmetric array organization of storage elements in RAM chips is  used in developing the tests with minimal length. Test generation is using the  operations increment, decrement, compare and rotate, and quite easy to program.",1976
Musical Expression in Automated Composition of Phrases,"Music composed by computers has always been lacking in ""musical"" qualities: mood, emotional expression, and a sense of purpose or goal. A musical expert system called EMOTER is the first attempt to address these important musical aspects.  EMOTER receives as input a list of moods (e.g., happy, lively) and generates melodic passages intended to evoke those moods in an organized, coherent fashion.  EMOTER composes the basic units of music called phrases.  Results with EMOTER are excellent.  Some of the musical phrases generated from a mood-specification are comparable to music that a reasonably talented composer might produce.  More theory is needed, however, before the full complexities of human-composed music are sufficiently captured in code for EMOTER to pass a Turing test in music composition.",1987
An Extension of the Direct Method for Verifying Programs,"A direct method based on a finite set of path formulas which describe the input-output relations of a given program can be used to verify programs containing no overlapping loops. One major difficulty in verifying programs with overlapping loops using the above method is that too many path formulas (possibly infinite) needed to be considered. In this paper, we circumvent the above difficulty by applying the concept of modularity. The idea is to divide a program with overlapping loops into several small modules so that each module contains no overlapping loop. This can always be achieved if the program is in structured form. Then the path formulas will be derived for each module. By combining the path formulas for the modules, one can further obtain the path formulas for the given program and then use them to verify the program.",1975
Enumeration and Analysis of Variable Geometry Truss Manipulators,"The variable-geometry truss or VGT, is a generalization of Stewart's most basic platform manipulator concept.  In this paper, all possible basic forms of the truss-type manipulator are enumerated.  These basic forms, or unit cells, are shown to be building blocks from which all truss-type manipulators are formed.  Several manipulator concepts based on the unit cell construction are then presented as examples, including a new potentially superior form of Stewart's Platform.  Finally, a general method for the forward kinematic analysis of VGT unit cells is described.",1990
Collapsing the Resultant Dipole System,"In this paper we show how to reduce the resultant dipole system developed by Gabor, Hodgkin, and Nelson to a single quadratic equation. This system was developed to determine the resultant dipole of the heart from measurements on the body surface. Reducing the system to a quadratic equation eliminates previous difficulties in numerically solving the system.",1987
VPI Prolog User Manual,This paper documents how a user may work with the VPI Prolog compiler.  The user interface functions called debugging environment and input/output are all described in detail.  Anyone using this manual should be able to program effectively using VPI Prolog.,1990
A Case Study of Using Domain Analysis for the Conflation Algorithms Domain,"This paper documents the domain engineering process for much of the conflation algorithms domain. Empirical data on the process and products of domain engineering were collected. Six conflation algorithms of four different types: three affix removal, one successor variety, one table lookup, and one n-gram were analyzed. Products of the analysis include a generic architecture, reusable components, a little language and an application generator that extends the scope of the domain analysis beyond previous generators. The application generator produces source code for not only affix removal type but also successor variety, table lookup, and n-gram stemmers. The performance of the stemmers generated automatically was compared with the stemmers developed manually in terms of stem similarity, source and executable sizes, and development and execution times. All five stemmers generated by the application generator produced more than 99.9% identical stems with the manually developed stemmers. Some of the generated stemmers were as efficient as their manual equivalents and some were not.",2007
"Probability-one Homotopy Algorithms for Solving the Coupled Lyapunov Equations Arising in Reduced-Order H^2/H^(infinity) Modeling, Estimation, and Control","Optimal reduced order modeling, estimation, and control with respect to combined H^2/H^(infinity) criteria give rise to coupled Lyapunov and Riccati equations. To develop reliable numerical algorithms for these problems this paper focuses on the coupled Lyapunov equations which appear as a subset of the synthesis equations. In particular, this paper systematically examines the requirements of probability-one homotopy algorithms to guarantee global convergence. Homotopy algorithms for nonlinear systems of equations construct a continuous family of systems and solve the given system by tracking the continuous curve of solutions to the family. The main emphasis is on guaranteeing transversality for several homotopy maps based upon the pseudogramian formulation of the coupled Lyapunov equations and variations based upon canonical forms. These results are essential to the probability-one homotopy approach by guaranteeing good numerical properties in the computational implementation of the homotopy algorithms.",2000-04-01
A Practical Method to Estimate Information Content in the Context of 4D-Var Data Assimilation. I: Methodology,"Data assimilation obtains improved estimates of the state of a physical system by combining imperfect model results with sparse and noisy observations of reality. Not all observations used in data assimilation are equally valuable. The ability to characterize the usefulness of different data points is important for analyzing the effectiveness of the assimilation system, for data pruning, and for the design of future sensor systems. This paper focuses on the four dimensional variational (4D-Var) data assimilation framework. Metrics from information theory are used to quantify the contribution of observations to decreasing the uncertainty with which the system state is known. We establish an interesting relationship between different information-theoretic metrics and the variational cost function/gradient under Gaussian linear assumptions. Based on this insight we derive an ensemble-based computational procedure to estimate the information content of various observations in the context of 4D-Var. The approach is illustrated on linear and nonlinear test problems. In the companion paper [Singh et al.(2011)] the methodology is applied to a global chemical data assimilation problem.",2011-11-01
A Modiﬁed Uniformization Method for the Solution of the Chemical Master Equation,"The chemical master equation is considered an accurate description of general chemical systems, and especially so for modeling cell cycle and gene regulatory networks. This paper proposes an eﬃcient way of solving the chemical master equation for some prototypical problems in systems biology. A comparison between this new approach and some traditional approaches is also given.",2007
S4W: Globally Optimized Design of Wireless Communication Systems,"In this paper, a global optimization technique is applied to solve the optimal transmitter placement problem for indoor wireless systems. An efficient pattern search algorithm -- DIRECT (DIviding RECTangles) of Jones,Perttunen, and Stuckman(1993)--has been connected to a parallel 3D radio propagation ray tracing modeler running on a 200 node. Beowulf cluster of Linux workstations.  Surrogate functions for a parallel WCDMA (wideband code division multiple access) simulator were used to estimate the system performance for the global optimization algorithm. Power coverage and BER (bit error rate) are considered as two different criteria for optimizing locations of a specified number of transmitters across the feasible region of the design space. This paper briefly describes the underlying radio propagation and WCDMA simulations and focuses on the design issues of the optimization loop.",2002-07-01
The Need for Ethics Education in Computer Curriculum,"The availability of the computer to a broad section of the community has brought under its influence a number of individuals who may not have been so well disciplined in appropriate ethical behavior.  Lacking precedents and truly parallel  paradigms as in driver and sex education, this paper recommends that earnest consideration must be given to introducing ethical concepts and case studies into secondary school classes as well as in professional school curriculum related to computing.  Surveys have shown that the person most likely to have misused a computer/communication system is the employee of the company under attack.  It is the responsibility of the computer community to reach as many of these employees during their formative years to divert them from inappropriate practices.  The objective of this paper is to consider the state of affairs in computing which lead to deep concerns about ethical behavior and to present proposals for the inclusion of ethical concepts in early computer related courses.",1991-05-01
A Model of TCP/IP Suitable for Parallel Simulation of Large Internets,"The current Internet connects on the order of 105 hosts. What unanticipated network dynamics will arise as the Internet grows to a million or more nodes? This report describes a model of TCP/IP based on extended finite state machines intended to permit simulation on this scale. The model yields the throughput and delay averaged over all connections of a workload consisting of client and server hosts interconnected through a subnet of gateways. A variety of parameters characterize the workload, protocol software speed, subnet, and internet topology. The model presented embodies a number of conjectures on what aspects of a protocol should be omitted to permit simulation of large numbers of nodes, and conjectures on what protocol aspects should be included to give qualitative, rather than quantitative, insight into scaling the Internet.",1992
Design and Architectural Implications of a Spatial Information System,"Image analysis, at the higher levels, works with extracted regions  and line segments and their properties,  not with the original raster  data. Thus a spatial information system must be able to store points,  lines, and areas as well as their properties and interrelationships.  In a previous paper (Shapiro and Haralick [22]), we proposed for this  purpose an entity-oriented relational database system.  In this paper  we describe our first experimental spatial information system, which  employs these concepts to store and retrieve watershed data for a portion of the state of Virginia.  We describe the logical and physical  design of the system and discuss the architectural implications.",1982
Subpixel Edge Location in Binary Images Using Dithering,"This paper concerns the problem of obtaining subpixel estimates of the locations of straight lines in digital images for purposes of machine vision.  In particular, it presents a dithering method for improving the estimation accuracy on a rectangular sampling lattice.  By adding uniformly distributed independent random noise it is shown that estimation bias may be removed and that the estimation variance is inversely proportional to the length of the line segment.  The sensitivity to incorrect dither amplitude is calculated, and a novel approach is given for adding the dither by using grey-level image sensor and utilizing the imaging model.",1993-06-01
Overhang of a Heavy Elastic Sheet,A flexible elastic sheet overhangs from a corner. The deflection due to its own weight depends on a parameter K which represents the relative importance of overhang length to the bending length (EI/p)^1/3.,1981
A Prototype Assistance Manager for the Simulation Model Development Environment,"The Assistance Manager, one of the tools of the Simulation Model Development Environment (SMDE), is required to provide effective and efficient transfer of assistance information to an SMDE user. This paper describes a prototype of the SMDE Assistance Manager. Objectives are set forth and a design is established and implemented on a SUN 3/160C workstation. The prototype is evaluated with respect to the design objectives and is shown to provide a highly flexible interface between the user and the database of assistance information. Results indicate that the prototype Assistance Manager incorporates the characteristics considered desirable in on-line assistance systems and serves as a basis for future enhancement and development.",1987
Internet Based Real-Time Multiuser Simulation: Ppong!,"There is a growing demand for real-time collaborative applications through the World Wide Web.  New techniques are required for real-time applications to perform well in the face of changing network conditions that often include long delays.  We present some of the key issues for implementors of real-time web-based applications, including choices on centralized versus distributed control, two versus multiuser considerations, synchronous versus asynchronous message protocols, and simulation divergence.  We present an implementation for a web-based version of the classic Pong game, called Ppong!.  Ppong! was selected for implementation since it strips to its essentials many components of real-time collaborative simulation.  A key feature of Ppong! is implementation of a heuristic for ""retarding"" a user's view of the simulation to accommodate network delays.",1997-02-01
A Genetic Algorithm for Mixed Integer Nonlinear Programming Problems Using Separate Constraint Approximations,"This paper describes a new approach for reducing the number of the fitness and constraint function evaluations required by a genetic algorithm (GA) for optimization problems with mixed continuous and discrete design variables. The proposed additions to the GA make the search more effective and rapidly improve the fitness value from generation to generation.The additions involve memory as a function of both discrete and continuous design variables, and multivariate approximation of the individual functions' responses in terms of several continuous design variables. The approximation is demonstrated for the minimum weight design of a composite cylindrical shell with grid stiffeners.",2003
"Using Reflection for Implementing ICOM, an Interoperable Common Object Model","Reuse of already developed object-oriented software components is reduced when software is written in different object-oriented languages, or when it is not easy to relocate the software components from one machine to another machine.  Software development in a distributed and heterogeous environment is a solution to this problem.  Several common object models have been defined for software development in a distributed and heterogeous environment.  Most of the existing common object models do not agree upon a common set of object-oriented features.  These models are weak (i.e., they have only a modest subset of object-oriented features) because they support both object-oriented and non object-oriented languages and mapping of the model into non object-oriented languages is a complex problem.  In this paper an interoperable common object model (ICOM) that supports statically typed object-oriented languages is presented.  The ICOM model has an extensive set of object-oriented features that uses reflection techniques to support the model.  The architecture of ICOM framework is described and a detailed account on how reflection is used is given.  A prototype involving dynamic method binding feature is explained in detail.",1995-12-01
HOMPACK90: A Suite of FORTRAN 90 Codes for Globally Convergent Homotopy Algorithms,"HOMPACK90 is a FORTRAN 90 version of the FORTRAN 77 package HOMPACK (Algorithm 652), a collection of codes for finding zeros or fixed points of nonlinear systems using globally convergent probability-one homotopy algorithms.  Three qualitatively different algorithms - ordinary differential equation based, normal flow, quasi-Newton augmented Jacobian matrix - are provided for tracking homotopy zero curves, as well as separate routines for dense and sparse Jacobian matrices.  A high level driver for the special case of polynomial systems is also provided.  Changes to HOMPACK include numerous minor improvements, simpler and more elegant interfaces, use of modules,  new end games, support for several sparse matrix data structures, and new iterative algorithms for large sparse Jacobian matrices.",1996-07-01
SimFusion: A Unified Similarity Measurement Algorithm for Multi-Type Interrelated Web Objects,"In this paper, we use a Unified Relationship Matrix (URM) to represent a set of heterogeneous web objects (e.g., web pages, queries) and their interrelationships (e.g., hyperlink, user click-through relationships). We claim that iterative computations over the URM can help overcome the data sparseness problem (a common situation in the Web) and detect latent relationships among heterogeneous web objects, thus, can improve the quality of various information applications that require the combination of information from heterogeneous sources. To support our claim, we further propose a unified similarity-calculating algorithm, the SimFusion algorithm. By iteratively computing over the URM, the SimFusion algorithm can effectively integrate relationships from heterogeneous sources when measuring the similarity of two web objects. Experiments based on a real search engine query log and a large real web page collection demonstrate that the SimFusion algorithm can significantly improve similarity measurement of web objects over both traditional content based similarity-calculating algorithms and the cutting edge SimRank algorithm.",2004
Voice Navigation of Structured Web Spaces,"Voice Navigation of web spaces has become a reality in the last few years, partly due to the rapid adoption of VoiceXML and the increase in communication quality and computing power of cell-phones. This report discusses some of the approaches on how to convert the web content to a way that could be used from a voice-enabled phone. These approaches have different pros and cons when it comes to the usability of the voice-navigation of web spaces. This report discusses our work to produce voice navigation of web spaces placing usability as the highest criteria. We have designed of a voice user interface for pages with a fixed structure and user-specified content, such as My Yahoo! pages. For these types of pages, we have defined the voice navigation strategy that we will use and conducted an initial usability study on this navigation strategy. From the usability study we have obtained validation for some of our approaches, and learned some new concepts in voice navigation as well. With our findings, we are defining annotation tags that can be used to produce highly usable web pages over a phone user interface. In this paper we describe our initial study and the findings of the study.",2002
Persistent virtual identity in community networks: Impact to social capital value chains,"Community networks are digital infrastructures designed to strengthen bonds and build social capital between members of a community, facilitating accomplishment of goals. As we consider how community network implementations can be improved, we recognize the potential that social translucence and activity notification introduces to other forms of CSCW. We investigate how the underlying notion of persistent virtual identity---established at logon---impacts user perception of community networks and their social capital production process. To approach this question, we introduce a design model that reconciles various computer-mediated communication research contributions with support for typical community network scenarios of use. Using this model, we perform an inspection on existing community network implementations. Based on the insight gained through this analysis, we introduce a generic prototype that allows survey of user reaction to community network design elements under differing conditions of persistent virtual identity implementation and usage motivation---the results frame a value-chain understanding of conceptual tradeoffs.",2003
Scenario/Class Diagram Synthesis,The scenario-synthesis problem in requirements analysis is explored in this report.The approach suggested by Khriss et al.is adapted for the domain of Digital Libraries. The results of the synthesis along with the entire transformation process are elaborated in this report.,2003
Preconditioned Iterative Methods for Sparse Linear Algebra Problems Arising in Circuit Simulation,"The DC operating point of a circuit may be computed by tracking the zero curve of an associated artificial-parameter homotopy. Homotopy algorithms exist that are globally convergent with probability one for the DC operating point problem. These algorithms require computing the one-dimensional kernel of the Jacobian matrix of the homotopy mapping at each step along the zero curve, and hence the solution of a linear system of equations at each step. These linear systems are typically large, highly sparse, non-symmetric and indefinite. Several iterative methods which are applicable to such problems, including Craig's method, GMRES(k), BiCGSTAB, QMR, KACZ, and LSQR, are applied to a suite of test problems derived from simulations of actual bipolar circuits. Preconditioning techniques considered include incomplete LU factorization (ILU), sparse submatrix ILU, and ILU allowing restricted fill in bands or blocks. Timings and convergence statistics are given for each iterative method and preconditioner.",1992-05-01
Exploring the Analysis of Discourse by Means of a Working Model Using Flexible Natural Language Parsing,No abstract available.,1982
Special Valued L-groups,"Special elements and values have always been of interest in the study of lattice-ordered groups, arising naturally from totally-ordered groups and lexicographic extensions. Much work has been done recently with the class of lattice-ordered groups whose root system of regular subgroups has a plenary subset of special values. We call such l-groups special valued.  In this paper, we first show that several familiar structures of l-groups, namely polars, minimal prime subgroups, and the lex kernel, are recognizable from the lattice and the identity; that is, knowing which element of the lattice is the group identity, we can pick out in the lattice all the dements of polars, minimal primes, and the lex kernel. This then leads to an easy proof that special elements can be recognized from the lattice and the identity.  We then prove several results about the class S of special-valued l-groups. We give a simple and direct proof that S is closed with respect to joins of convex l-subgroups, incidentally giving a direct proof that S is a quasi torsion class. This proof is then used to show that the special-valued and finite-valued kernels of l-groups are recognizable from the lattice and the identity. We show also that the lateral completion of a special-valued l-group is special-valued and is an a*-extension of the original l-group. Our most important result is that the lateral completion of a completely-distributive normal-valued l-group is special-valued. This lends itself easily to a new and similar proof of Ball, Conrad, and Darnel's result that every normal-valued l-group can be l-embedded into a special-valued l-group. Readers familiar with the impact of the Conrad-Harvey-Holland Theorem on abelian l-groups will recognize the importance of the last theorem to the study of the class of normal-valued l-groups and to the study of proper varieties of l-groups, all of which are normal valued.",1984
Variable-Complexity Response Surface Approximations for Wing Structural Weight in HSCT Design,"A procedure for generating and using a polynomial approximation to wing bending material weight of a High Speed Civil Transport (HSCT) is presented.  Response surface methodology is used to fit a quadratic polynomial to data gathered from a series of structural optimizations.  Several techniques are employed in order to minimize the number of required structural optimizations and to maintain accuracy. First, another weight function based on statistical data is used to identify a suitable model function for the response surface.  In a similar manner, geometric and loading parameters that are likely to appear in the response surface model are also identified.  Next, simple analysis techniques are used to find regions of the design space where reasonable HCST  designs could occur.  The use of intervening variables along with analysis of variance reduce the number of polynomial terms in the response surface model function.  Structural optimization is then performed by the program GENESIS on a 28-node Intel Paragon.  Finally, optimizations of the HSCT are completed both with and without the response surface.",1996
Designing of a Community-based Translation Center,"Interfaces that support multi-lingual content can reach a broader community. We wish to extend the reach of CITIDEL, a digital library for computing education materials, to support multiple languages. By doing so, we hope that it will increase the number of users, and in turn the number of resources. This paper discusses three approaches to translation (automated translation, developer-based, and community-based), and a brief evaluation of these approaches. It proposes a design for an online community translation center where volunteers help translate interface components and educational materials available in CITIDEL.",2003
Grid Community Characteristics and their Relation to Grid Security,"The size, dynamics, composition and similar characteristics of Grid Communities constitute important data for Grid security requirements gathering and analysis. Collaborative Grid Communities are especially important as they constitute an important part of grid usage modes and demonstrate the need for more advanced Grid security solutions very clearly. This document reports the results of a survey conducted in the Fall of 2002 among members of the Grid community as to understand the needs of grid user and grid application developer communities today, provide information on and typical modes-of-use, and elicit requirements for future grid security systems.",2003
"Partitioned Frame Networks for Multi-Level, Menu-Based Interaction","Menu-based systems have continued to flourish because they present a simple interaction format that is adaptable to many diverse applications. The continued integration of menu-based interaction with increasingly sophisticated software systems, however, is resulting in complex, monolithic frame networks with several undesirable characteristics.  This paper presents a novel approach to frame network construction and menu-based interaction for application systems that support user task specifications. The approach is based on partitioning the conventional, monolithic frame network into a set of hierarchically structured, disjoint networks that preserves the original network topology while reducing its overall complexity and size. Moreover, by providing a menu-based interaction scheme that exploits this hierarchical structure, one can realize a system that supports a ""top-down"" approach to user task specification and user interaction at varying levels of sophistication.",1987
An Introduction to Visualizing Program Execution Dynamics with Chitra,"We describe a tool called Chitra, which serves two purposes. First, Chitra visualizes program execution sequences (PES's) collected by monitoring parallel and distributed program execution in the time, event, and frequency domains. Second, Chitra produces an empirical model of a PES. A case study applies Chitra to predict and evaluate the efficiency and fairness of a resource sharing algorithm at a range of parameter values, given observations of a few parameter values. Inefficient behavior is explained by examining the states that constitute the stochastic process model constructed by Chitra.",1992
Evaluation of Software Development Methodologies: A Final Report,"A procedure for evaluating software development methodologies is developed based on the linkage among objectives, principles, and attributes. The procedure utilizes metrics defined to recognize surface properties of the software, leading to evidence of the presence or absence of particular attributes. Application of the procedure to samples from the RNTDS and AEGXS Modular Development programs reveals weaknesses and strengths that should be addressed in efforts to produce a common CDS. The extension of the work is proposed",1986
Prospects for Automated Documentation Analysis in Support of Software Quality Assurance,"This report discusses the strengths and weaknesses of ADDS.  ADDS is an automated document synthesis system that exploits the benefits of reverse engineering.  Its generated documents are primarily used to support the maintenance activity.  Based on the content and format of these reports, recommendations are made to improve the overall effectiveness of ADDS.  These recommendations include the modifications/additions of reports and corresponding changes to ADDS.",1988
Towards Energy-Proportional Computing for Enterprise-Class Server Workloads,"Massive data centers housing thousands of computing nodes have become commonplace in enterprise computing, and the power consumption of such data centers is growing at an unprecedented rate. Adding to the problem is the inability of the servers to exhibit energy proportionality, i.e., provide energy-ecient execution under all levels of utilization, which diminishes the overall energy eciency of the data center. It is imperative that we realize eective strategies to control the power consumption of the server and improve the energy eciency of data centers. With the advent of Intel Sandy Bridge processors, we have the ability to specify a limit on power consumption during runtime, which creates opportunities to design new power-management techniques for enterprise workloads and make the systems that they run on more energy-proportional. In this paper, we investigate whether it is possible to achieve energy proportionality for an enterprise-class server workload, namely SPECpower ssj2008 benchmark, by using Intel's Running Average Power Limit (RAPL) interfaces. First, we analyze the power consumption and characterize the instantaneous power prole of the SPECpower benchmark at a subsystem-level using the on-chip energy meters exposed via the RAPL interfaces. We then analyze the impact of RAPL power limiting on the performance, per-transaction response time, power consumption, and energy eciency of the benchmark under dierent load levels. Our observations and results shed light on the ecacy of the RAPL interfaces and provide guidance for designing power-management techniques for enterprise-class workloads.",2012
Steady-State Size Distributions in Probabilistic Models of the Cell Division Cycle,"A model for the steady-state size distribution in an exponentially growing population of single cells is derived and studied. The model incorporates a general growth law for individual cells and a probability density for interdivision times. A uniqueness theorem is proved. and it is shown that no solution exists when individual cells grow exponentially. For linear growth, infinite series solutions are found in two specific cases. Statistical data are obtained for these solutions, and comparisons are made with the results of some numerical simulations and with a known experimental result.",1984
A Metric Tool for Predicting Source Code Quality from a PDL Design,"The software crisis has increased the demand for automated tools to assist software developers in the production of quality software. Quality metrics have given software developers a tool to measure software quality. These measurements, however, are available only after the software has been produced. Due to high cost, software managers are reluctant, to redesign and reimplement low quality software. Ideally, a life cycle which allows early measurement of software quality is a necessary ingredient to solving the software crisis. This paper describes an automated tool for predicting software quality at design time.",1987
CITIDEL Collection Building,The aim of this study is to facilitate the goals of the Computing and Information Technology Interactive Digital Educational Library (CITIDEL) by increasing the number of collections available to it. This study will help in achieving this goal by focusing on four diverse collections.,2003
Precision of Binary Matching Systems,"Inexpensive memory and fast processors have made it feasible to build binary matching inspection systems capable of very high measurement precision that operate at near-camera speeds. While the limitations of binary inspection are well known, its performance and cost advantage can be very large in situations in which it can be applied. When binary matching inspection systems are designed and when they are purchased and installed by end-users it becomes absolutely essential to understand the numerous sources that contribute to measurement error.  That makes it possible to make precise statements about the accuracy of components whose tolerances they are used to certify. This paper considers quantitative precision issues for the inspection  of strictly 2-dimensional objects using line scan cameras that integrate while the workpiece is in motion.",1987
Study and Redesign of a Semi-public Display: Online Enlightenment,"Semi-public displays are systems designed to strengthen awareness and collaboration among small co-located group environments. Placed in a semi-public space, Online Enlightenment is a physical device associated with MSN® Messenger to provide information regarding the online status of peers. The raison d'être of the system is to leverage group members’ awareness of their peers’ availability through changes of their online status in order to facilitate meeting scheduling, promote opportunistic collaboration, and foster project teamwork without introducing distraction. At an early stage of the development process, this paper presents the results of a usability study of the system and proposes a redesigned mock-up to address the identified deficiencies.",2007-05-01
"The Effects of Task, Task Mapping, and Layout Space on User Performance in Information-Rich Virtual Environments","How should abstract information be displayed in Information-Rich Virtual Environments (IRVEs)? There are a variety of techniques available, and it is important to determine which techniques help foster a user’s understanding both within and between abstract and spatial information types.  Our evaluation compared two such techniques: Object Space and Display Space. Users strongly prefer Display Space over Object Space, and those who use Display Space may perform better. Display Space was faster and more accurate than Object Space for tasks comparing abstract information. Object Space was more accurate for comparisons of spatial information. These results suggest that for abstract criteria, visibility is a more important requirement than perceptual coupling by depth and association cues. They also support the value of perceptual coupling for tasks with spatial criteria.",2006
Applying Structure and Code Metrics to Three Large-Scale Systems,"This work extends the area of research termed software metrics by applying measures of system structure and measures of system code to three realistic software products. Previous research in this area has typically been limited to the application of code metrics such as : lines of code, McCabe's Cyclomatic number, and Halstead's software science variables. However, this research also investigates the relationship of four structure metrics: Henry's Information Flow measure, Woodfield's Syntactic Interconnection Model, Yau and Collofello's Stability measure and McClure's Invocation complexity, to various observed measures of complexity such as, ERRORS, CHANGES and CODING TIME. These metrics are referred to as structure measures since they measure control flow and data flow interfaces between system components. Correlating the metrics to observed measures of complexity indicated that the Information Flow metric and the Invocation Measure typically performed as well as the three code metrics when project factors and subsystem factors were taken into consideration. However, it was generally true that no single metric was able to satisfactorily identify the variations in the data.",1985
An Object-oriented Approach to Semidefinite Programming,"An object-oriented design and implementation of a primal-dual algorithm for solving the semidefinite programming problem is presented.  The advantages of applying the object-oriented methodology to numerical computations, in particular to an interior point algorithm for semidefinite programming, or for solving other types of linear matrix inequalities are discussed.  One object-oriented design of the primal-dual algorithm and its implementation using C++ is presented.  The performance of the C++ implementation is compared with that of a procedural C implementation, and while the performance of the C++ implementation is comparable to that of the C implementation, the resulting code is easier to read, modify, and maintain.",1996-05-01
Software Process Reuse in an Industrial Setting,This paper describes a method for creating reusable processes and our experience using them in an industrial environment.  A notation and process for creating and tailoring reusable processes is described and applied to the building of a 120 process library at PRC Inc. Initial data collected on use of the library indicates large potential payoffs from process reuse such as a 10 to 1 improvement in the time to develop a project specific process.,1996
Communicational Measurement,"A software system is an aggregate of communicating modules. The interfaces supporting module (procedure, etc.) communication characterize the system.  Therefore, understanding these interfaces (areas of communication) gives a better description of system complexity.  Understanding in an empirical sense implies measuring, and measuring interfaces involves examining both the communicational environment and the exchanged data.  There are several different measures associated with the communication environment.  Obviously, the structure or nesting level at the communication ping is very interesting. The need to measure the data communicated also raises some very interesting questions concerned with data type and expressional form.  This paper reports on the efforts at Virginia Tech to measure, and thus capture, the complexities of software interfaces.  Analyzing an Ada system of 85,000 lines of code validated the measures proposed here.  The results of this research are very encouraging.",1991-05-01
Model Representation in Discrete Event Simulation: The Conical Methodology,"Beginning with a brief review and classification of model development approaches, we characterize the simulation model life cycle as comprised of seven phases:  the conceptual model, the communicative  model, the programmed model, the experimental model, model results, use of the model for integrated decision support, and the modification and  extension of of the model.  This characterization places severe  requirements on the task of model management (creation, acceptance, use,  revision or extension, and reuse).  The Conical methodology has been developed in response to the needs that  predominate the model  development  phases (from conceptual  model  to model  results).  Definitions used in the Conical Methodology are explained, and the approach is illustrated with a machine repairman example. An incomplete critigue of the result and the approach concludes the paper.",1981
Twenty Five Years of Fortran: A National Acm Lectureship Series Presentation,"In 1982 FORTRAN will have existed in the environment of computers, computing and computation for 25 years, making it one of the most successful of programming languages even if it is not the actual oldest still surviving language. The honor of being the Oldest still belongs to APT (Automatic Programmed Tool.) This report is the script of talk given at several institutions during the Spring of 1982 and serves as a Skeleton on which a broader history is to be developed.",1982
Pointwise Bias Error Bounds for Response Surface Approximations and Min-Max Bias Design,"Two approaches addressing response surface approximation errors due to model inadequacy (bias error) are presented, and a design of experiments minimizing the maximal bias error is proposed. Both approaches assume that the functional form of the true model is known and seek, at each point in design space, worst case bounds on the absolute error. The first approach is implemented prior to data generation. This data independent error bound can identify locations in the design space where the accuracy of the approximation fitted on a given design of experiments may be poor. The data independent error bound can easily be implemented in a search for a design of experiments that minimize the bias error bound as it requires very little computation. The second approach is to be used posterior to the data generation and provides tightened error bound consistent with the data. This data dependent error bound requires the solution of two linear programming problems at each point. The paper demonstrates the data independent error bound for design of experiments of two-variable examples. Randomly generated polynomials in two variables are then used to validate the data dependent bias-error bound distribution.",2004
Order Preserving Minimal Perfect Hash Functions and Information Retrieval,"Rapid access to information is essential for a wide variety of retrieval systems and applications.  Hashing has long been used when the fastest possible direct search is desired, but is generally not appropriate when sequential or range searches are also required.  This paper describes a hashing method, developed for collections that are relatively static, that supports both direct and sequential access.  The algorithms described give hash functions that are optimal in terms of time and hash table space utilization, and that preserve any a priori ordering desired.  Furthermore, the resulting order preserving minimal perfect hash functions (OPMPHFs) can be found using time and space that are linear in the number of keys involved and so are close to optimal.",1991
Nonorthogonal Transforms for Logical Design,No abstract available.,1977
Contextual Boundary Formation by Scan Line Matching,"In this paper an algorithm is given for generating linked edge boundaries between adjacent regions of different  gray levels. In contrast with peak following algorithms,  edges are treated as variable width regions, and the edge linking procedure is really a region grower.  Edge linking  is a parallel process on all the edges in pairs of adjacent scan lines, and contextual information in the direction of  the scan lines is used  to resolve ambiguous linking  situations. The procedure relies heavily  upon a one-  dimensional edge detector that defers the formation of local edge interpretations until more informed decisions can be made by the edge linking procedure.",1979
Geometric Performance Analysis of Mutual Exclusion: The Model,"This paper is motivated by the need to better understand parallel program run-time behavior.  The paper first formally describes a general model of program execution based on Djkstra's progress graphs.  The paper then defines a special case of the model representing two cyclic processes sharing mutually exclusive, reusable resources.  Processes synchronize through semaphore operations that are not embedded in conditionally executed code segments.  Model parameters are the times at which each semaphore operation completes and the independently derived, constant execution time required by each process between points of completion of semaphore operations.",1990
"Software Quality Measurement: Validation of a Foundational Approach (Final Report, Year Two)","This report discusses the second year findings of a proposed four year investigation effort that focuses on the assessment and prediction of software quality. The research exploits fundamental linkages among software engineering Objectives, Principles, and Attributes (the OPA framework). Process and documentation quality indicators (DQI) are presented relative to the OPA framework with an elaboration on their individual role in assessing and predicting software quality. The development of a document quality analyzer is discussed, with a particular emphasis being placed on the selected subset of DQI measures that it provides.",1992
Requirements for a Software Maintenance Methodology,"This project ventures into the domain of software maintenance methodologies, an area given relatively little attention in software engineering. Project efforts are divided into three tasks:  (1) definition of maintenance methodology requirements; (2) development of a model of the AEGIS maintenance process; and (3) specification of the requirements for an AEGIS maintenance methodology.  The focus of this report is on the third task. The research utilizes the Objectives/Principles/Attributes (OPA) framework developed for software quality assessment for time-critical, embedded systems.  The overriding maintenance objective is to ""realize desired changes in an efficient and effective manner.""  Four principles are identified that support achievement of this objective.  Eleven requirements are derived for an AEGIS maintenance methodology.  The AEGIS maintenance process model is used to determine the potential points where any of the requirements can be met.  Recommendations are made with respect to restructuring the process, noting the most appropriate place for meeting requirements, and acquisition or development of software utilities to support maintenance.  Recommended as an approach for meeting long-term needs is the AEGIS System Evolution Environment, supporting both systems and software maintenance activities.  [Appendices Not Included]",1991
Linking Simulation Model Specification and Parallel Execution through UNITY,"Chandy and Misra's UNITY is a computation model and proof system suitable for development of parallel (and distributed) programs through step-wise refinement of specifications.  UNITY supports the development of correct programs and the efficient implementation of those programs on parallel computer architectures.  This paper assesses the potential of UNITY for simulation model specification and implementation by developing a UNITY specification of the machine interference problem with a patrolling repairman service discipline. The conclusions reached are that the UNITY proof system can assist formal verification of simulation models and the UNITY mappings of programs to various computer architectures offer some potential for assisting the automatic implementation of simulation models on parallel architectures.  The paper gives some insights into the relationship of time flow mechanisms, parallel simulation protocols, and target parallel computer architectures.",1991
Identification of Space Curves from Two-Dimensional Perspective Views,This paper describes a new method to be used in the recognition of three-dimensional objects with curved surfaces from two-dimensional perspective views. The method requires for each three-dimensinal object a stored model consisting of a closed space curve representing some characteristic connected curved edges of the object. The input is a two-dimensional perspective projection of one of the stored models represented by an ordered sequence of points. The input is converted to a spline representation which is sampled at equal intervals to derive a curvature function. The Fourier transform of the curvature function is used to represent the shape. The actual matching is reduced to a minimization problem which is handled by the Levenberg-Marquardt algorithm [3].,1980
Divided Difference Methods for Galois Switching Functions,"An alternative is provided to a recently published method of  Benjauthrit and Reed for calculating the coefficients of the  polynomial expansion of a given function. The method herein is  an adaptation to finite fields of a method of Newton. The method  is exhibited for functions of one and two variables. The relative  advantages and disadvantages of the two methods are discussed.  Some empirical results are given for GF(9) and GF(16). It is  shown that functions with ""don't care"" states are represented  by a polynomial of minimal degree by this method.",1977
On the Computation of Minimum Encasing Rectangles and Set Diameters,Two new algorithms are described for the following  problems: given a set of N points in the plane determine (i) the rectangle of minimum area which will completely cover (or encase) the set and (ii) the two points that are farthest apart (diameter of the set). Both algorithms have O(NlogN) time complexity and are based upon a similar strategy.,1981
Design of Composite Laminates by a Genetic Algorithm with Memory,"This paper describes the use of a genetic algorithm with memory for the design of minimum thickness composite laminates subject to strength, buckling and ply contiguity constraints.  A binary tree is used to efficiently store and retrieve information about past designs.  This information is used to construct a set of linear approximations to the buckling load in the neighborhood of each member of the population of designs.  The approximations are then used to seek nearby improved designs in a procedure called local improvement.  The paper demonstrates that this procedure substantially reduces the number of analyses required for the genetic search.  The paper also demonstrates that the use of genetic algorithms helps find several alternate designs with similar performance, thus giving the designer a choice of alternatives.",1994
Reduced Sampling for Construction of Quadratic Response Surface Approximations Using Adaptive Experimental Design,"The purpose of this paper is to reduce the computational complexity per step from O(n^2) to O(n) for optimization based on quadratic surrogates, where n is the number of design variables.  Applying nonlinear optimization strategies directly to complex multidisciplinary systems can be prohibitively expensive when the complexity of the simulation codes is large. Increasingly, response surface approximations, and specifically quadratic approximations, are being integrated with nonlinear optimizers in order to reduce the CPU time required for the optimization of complex multidisciplinary systems. For evaluation by the optimizer, response surface approximations provide a computationally inexpensive lower fidelity representation of the system performance. The curse of dimensionality is a major drawback in the implementation of these approximations as the amount of required data grows quadratically with the number n of design variables in the problem. In this paper a novel technique to reduce the magnitude of the sampling from O(n^2) to O(n) is presented.  The technique uses prior information to approximate the eigenvectors of the Hessian matrix of the response surface approximation and only requires the eigenvalues to be computed by response surface techniques. The technique is implemented in a sequential approximate optimization algorithm and applied to engineering problems of variable size and characteristics. Results demonstrate that a reduction in the data required per step from O(n^2) to O(n) points can be accomplished without significantly compromising the performance of the optimization algorithm.  A reduction in the time (number of system analyses) required per step from O(n^2) to O(n) is significant, even more so as n increases. The novelty lies in how only O(n) system analyses can be used to approximate a Hessian matrix whose estimation normally requires O(n^2) system analyses.",2007
Achieving Asynchronous Speedup While Preserving Synchronous Semantics: An Implementation of Instructional Footprinting in Linda,"Linda is a coordination language designed to support process creation and inter-process communication within conventional computational languages.  Although the Linda paradigm touts architectural and language independence, it often suffers performance penalties, particularly on local area network platforms.  Instructional Footprinting is an optimization technique with the primary goal of enhancing the execution speed of Linda programs.  The two main aspects of Instructional Footprinting are instructional decomposition and code motion.  This paper addresses the semantic issues encountered when the Linda primitives, IN and RD, are decomposed and moved past other Linda operations.  Formal semantics are given as well as results showing significant speedup (as high as 64%) when Instructional Footprinting is used.",1993
Contragredient Transformations Applied to the Optimal Projection Equations,"The optimal projection approach to solving the H2 reduced order model problem produces two coupled, highly nonlinear matrix equations with rank conditions as constraints. It is not obvious from their original form how they can be differentiated and how some algorithm for solving nonlinear equations can be applied to them. A contragredient transformation, a transformation which simultaneously diagonalizes two symmetric positive semi-definite matrices, is used to transform the equations into forms suitable for algorithms for solving nonlinear problems. Three different forms of the equations obtained using contragredient transformations are given. An SVD-based algorithm for the contragredient transformation and a homotopy algorithm for the transformed equations are given, together with a numerical example.",1992
Transfer of Apl Workspaces: A Useful Solution,"Most suppliers of APL have not yet implemented the STAPL convention for trans¬mitting workspaces from one installation to another. This report describes three workspace representations which may be used on a DECsystem-lO for this purpose. Two are partial implementations of the 8TAPL convention: one for level 2 of the convention, the other for level 3. The third representation is a terminal transcript file which is to be used as an input file. In addition, these representations may be used to reduce the disk storage required for APL workspaces on the DECsystem-lO.",1978
Overview of a Guide for Electronic Theses and Dissertations,"This chapter provides an overview of a guide for electronic theses and dissertations that is being prepared as requested by UNESCO to help with the expansion of ETD activities around the world. It roughly follows the outline developed through discussions involving the many partners working on that guide, coordinated by Shalini Urs. It builds upon experiences related to the evolution of the Networked Digital Library of Theses and Dissertations, a federation of groups interested in ETD programs. It introduces key concepts, explains matters according to the interests of students and universities, highlights technical issues, recommends a scheme for expanding training, and suggests likely future activities.",2002
Probability-one Homotopies in Computational Science,"Probability-one homotopy algorithms are a class of methods for solving nonlinear systems of equations that,under mild assumptions,are globally convergent for a wide range of problems in science and engineering.Convergence theory, robust numerical algorithms,and production quality mathematical software exist for general nonlinear systems of equations, and special cases suc as Brouwer fixed point problems,polynomial systems,and nonlinear constrained optimization.Using a sample of challenging scientific problems as motivation,some pertinent homotopy theory and algorithms are presented. The problems considered are analog circuit simulation (for nonlinear systems),reconfigurable space trusses (for polynomial systems),and fuel-optimal orbital rendezvous (for nonlinear constrained optimization).The mathematical software packages HOMPACK90 and POLSYS_PLP are also briefly described.",2000
"On the Dynamic Response of Variable-Rate, Sampled-Data Systems","Time-domain analysis is used to derive criteria on the stability of sampled-data feedback systems comprising a time-varying plant, a time-varying non-linearity, and a sampler which may exhibit any given periodic or aperiodic sampling mode.  Three aspects of the systems dynamic response are given a unified treatment: boundedness, unboundedness, and asymptotic decay.  In addition to qualitative criteria, the results provide quantitative bounds on the system response, and indicate its explicit dependence on the sampling mode.",1993
The Formal Description of Minimal Basic,The purpose of this formal description of Minimal Basic is to provide an operational description which is not subject to the ambiguities of interpretation of natural language specifications.,1976
An Adaptive Noise Filtering Algorithm for AVIRIS Data with Implications for Classiﬁcation Accuracy,"This paper describes a new algorithm used to adaptively ﬁlter a remote sensing dataset based on signal-to-noise ratios (SNRs) once the maximum noise fraction (MNF) has been applied. This algorithm uses Hermite splines to calculate the approximate area underneath the SNR curve as a function of band number, and that area is used to place bands into “bins” with other bands having similar SNRs. A median ﬁlter with a variable sized kernel is then applied to each band, with the same size kernel used for each band in a particular bin. The proposed adaptive ﬁlters are applied to a hyperspectral image generated by the AVIRIS sensor, and results are given for the identiﬁcation of three different pine species located within the study area. The adaptive ﬁltering scheme improves image quality as shown by estimated SNRs, and classiﬁcation accuracies improved by more than 10% on the sample study area, indicating that the proposed methods improve the image quality, thereby aiding in species discrimination.",2008
Finding Straight Lines and Curves in Engineering Line Drawings,"This paper addresses the problem of distinguishing straight lines from curves in noisy gray tone images, and mathematically representing those lines and curves. A method for locating comers is discussed as well as criteria, based on spline representations, for classifying line segments as straight or curved. Results are given for several typical noisy engineering line drawings.",1987
Everywhere Energy-Efficient E-Computing,"This document outlines a vision for “green computing for a clean tomorrow” [Feng06].  The first piece of the vision is a bit pedestrian – holistic energy-efficient computing “in a box” – but serves as a foundation to a more audacious (tongue-in-cheek) vision of holistic energy-efficient computing “in a world.” As recently noted by IDC in an IBM presentation at the Gartner Data Center Summit, December 2006, the annual spending for power and cooling would match the annual budget for new server spending in 2007, as shown in the figure below.  In addition to cost, energy-efficient (power- aware) computing can enhance the reliability and availability of ever-increasingly dense computing systems, such as blades; it can also provide additional computational headroom when an institution has reached the limits of its power and cooling infrastructure, particularly when the infrastructure cannot be expanded any further [Feng08].",2008-05-01
Situational Analysis with the Task Mapping Model (TMM): A Tutorial,"The Task Mapping Model (TMM) is a user-centered analysis technique providing methods and notations for use in human-computer interaction (HCI) task analysis (Mayo & Hartson, 1993).  The primary function of the TMM is to synthesize new design requirements based on usability problems, user task descriptions, user class profiles, user knowledge requirements, and current user interface designs.  The TMM does not, however, derive new design specifications or high/low level interface designs.",1994
Parameter Estimation for a Mathematical Model of the Cell Cycle in Frog Eggs,"Parameter values for a kinetic model of teh nuclear replication-division cycle in frog eggs are estimated by fitting solutions of the kinetic equations (nonlinear ordinary differential equations) to a suite of experimental observations. A set of optimal parameter values is found by minimizing an objective function defined as the orthodonal distance between the data and the model. The differential equations are solved by LSODAR and the objective function is minimized by ODRPACK. The optimal parameter values are close to the ""guesstimates"" of the modelers who first studied this problem. These tools are sufficiently general to attack more complicated problems, where guesstimation is impractical or unreliable.",2002-08-01
Beyond Software Performance Visualization,"Performance visualization tools of the last decade have yielded new insights into the new behavior of sequential, parallel, and distributed programs.  However, they have three inherent limitations:  (1) They only display what happened in one execution of a program.  (This is dangerous when analyzing concurrent applications, which are prone to non-deterministic behavior.)  (2) A human uses one or more bandwidth-limited senses with a visualization tool.  (This limits the scalability of a visualization tool.) (3) The relationship of ""interesting"" program events are often separated in time by other events; thus discerning time dependent behavior often hinges on finding the ""right"" visualization - a possibly time-comsuming activity. CHITRA93 complements visualization systems, while alleviating these limitations.  CHITRA93 analyzes a set (or ensemble) of traces by combining the visualization of a few traces with a statistical analysis of the entire ensemble (overcoming (1)); and reduces the ensemble to empirical models that capture the time dependent relationships of ""interesting"" program events through application, programming language, and computer architecture independent analysis tools (addressing (2) and (3)).  CHITRA93 incorporates: transforms, such as aggregation, that simplify the ensemble and reduce the state space size of the models generated; a user interface that allows some transforms to be selected by editing the visualization with a mouse; homogeneity tests that allow partitioning of an ensemble; an efficient semi-Markov model generation algorithm whose computation time is linear in the sum of the lengths of the traces comprising the ensemble; and a CHAID-based model that can fathom non-Markovian relationships among transitions in the traces.  The use of CHITRA93 is demonstrated by partitioning ten parallel database traces with nearly 8,000 states into two homogeneous subsets, each modeled by a 20 state irreducible, periodic (non-Markovian) stochastic process.",1994-02-01
"A Framework for Precise, Reusable Task Abstractions","This paper is about modeling human-computer interaction for better understanding of interaction, leading to better designs and design representations.  We argue for a structured approach to task abstractions that includes semantics, as well as a framework and vocabulary for abstraction.  Our approach imposes a notational consistency that aids designer-implementer communication.  Including semantic considerations allows designers to specify interaction so that implementers can properly support the connection between illusion and reality.",1994-03-01
A Descriptive/Prescriptive Model for Menu-based Interaction,"As software systems continue to increase in sophistication and complexity, so do the interface requirements that support user interaction. To select the proper blend of ingredients that constitutes an adequate user interface, it is essential that the system designer have a firm understanding of the interaction process, i.e, how the selected dialogue format interacts with the user and with the underlying task software. One major approach to understanding the software design process and improving the quality of a product is through the use of models. The application of models to user/system interaction can provide the crucial feedback and innovative insights for designing and developing exemplary interactive systems. In this paper, we present one such model that describes as well as prescribes the critical elements for menu-based interaction and their interface dependencies. The model structure provides the flexibility for characterizing menu-based interactions that vary in levels of sophistication, and include 1) computational and decision capabilities based on task oriented actions, 2) user response reversal for error recovery, and 3) user directed movement. Finally, to illustrate the intrinsic power of our model, we present a descriptive narrative of two prominent menu-driven systems, Smalltalk and Zog, followed by a discussion of the model's prescriptive influence on the design and development of a third menu-based system, Omni.",1986
Creation of a Prolog Fact Base from the Collins English Dictionary,"Machine readable dictionaries may be invaluable components of future in-formation retrieval systems. The Virginia Tech department of Computer Science is fortunate to have available for research purposes machine readable versions of the Collins English Dictionary (CED) [HANK 79] and of the Oxford Advanced Learners Dictionary of Current English (OALDCE) [HORN 74] among others. McIllroy [MCIL 84] and Mitton [MIlT 85] have put forth separate efforts in cleaning up the OALDCE. The CED is only available in its original typesetting form, and it required a great deal of effort and patience to transform this into a relational lexicon.  A number of recent efforts have been made to generate relational lexicons from dictionaries (e.g., [AHLS 81], [AHLS 83], [AHLS 85], [WHIT 83], [EVEN 78] ). The lexicon that will be discussed in this paper consists of a large set of relations in the form of PROLOG facts. This knowledge base will become a very important entity in the analysis and retrieval sub-systems of the CODER project [FOXE 86].",1988-05-01
A Grassroots Approach to Graduate Teaching Assistant Mentoring,"Graduate students, whether master's or doctoral candidates, benefit greatly from their academic experiences.  However, graduate school is not limited to course work and research, but it also includes teaching experiences as graduate teaching assistants (GTAs).  Although GTAs are technically proficient in course materials, other factors can cause teaching experiences to go awry for them, their students, or the course supervisor.  These factors arise out of a need for quality training on issues including pedagogy, interaction resolution, organizational concerns, and professional matters.  This paper provides a grassroots approach to improve teachine techniques through GTA mentoring.  GTAs are encouraged, with materials supplied here, to seek out and consult with more experienced GTAs who will serve as their mentors.",1993
Mining Novellas from PubMed Abstracts using a Storytelling Algorithm,"Motivation: There are now a multitude of articles published in a diversity of journals providing information about genes, proteins, pathways, and entire processes. Each article investigates particular subsets of a biological process, but to gain insight into the functioning of a system as a whole, we must computationally integrate information across multiple publications. This is especially important in problems such as modeling cross-talk in signaling networks, designing drug therapies  for combinatorial selectivity, and unraveling the role of gene interactions in deleterious phenotypes, where the cost of performing combinatorial screens is exorbitant.  Results: We present an automated approach to biological knowledge discovery from PubMed abstracts, suitable for unraveling combinatorial relationships. It involves the systematic application of a `storytelling' algorithm followed by compression of the stories into `novellas.' Given a start and end publication, typically with little or no overlap in content, storytelling identifies a chain of intermediate publications from one to the other, such that neighboring publications have significant content similarity. Stories discovered thus provide an argued approach to relate distant concepts through compositions of related concepts. The chains of links employed by stories are then mined to find frequently reused sub-stories, which can be compressed to yield novellas, or compact templates of connections. We demonstrate a successful application of storytelling and novella finding to modeling combinatorial relationships between introduction of extracellular factors and downstream cellular events.  Availability: A story visualizer, suitable for interactive exploration of stories and novellas described in this paper, is available for demo/download at  https://bioinformatics.cs.vt.edu/storytelling.",2007
Effects of Active Exploration and Passive Observation on Spatial Learning in a CAVE,"This experiment was a modification of Paul N. Wilson's 1999 study entitled ""Active Exploration of a Virtual Environment Does Not Promote Orientation or Memory for Objects."" It was hoped that changing the immersion level from a standard desktop monitor to a more immersive CAVE environment would change the results of this experiment. All subjects explored a three-dimensional virtual environment in a CAVE. Active subjects were given controls to choose their own path and explore the environment. Passive subjects watched a playback tour through the virtual environment. A unique active subject determined the tour for each passive subject. Each subject was asked to remember the objects they saw, their locations, and the floor plan of the environment. Afterward, subjects were asked to indicate the direction to another location that was not visible from the current location. Other object memory tests required recalling the location of each object and indicating it on a plan view of the environment. Similar to Wilson's experiment, this experiment yielded no significant indication that active exploration or passive observation changes the level of spatial learning.",2002-07-01
Personalized Nuance-Oriented Interaction in Virtual Environments,Personalized Nuance-Oriented Interaction in Virtual Environments,2001
Power Saving Experiments for Large Scale Global Optimization,"Green computing, an emerging ﬁeld of research that seeks to reduce excess power consumption in high performance computing (HPC), is gaining popularity among researchers. Research in this ﬁeld often relies on simulation or only uses a small cluster, typically 8 or 16 nodes, because of the lack of hardware support. In contrast, System G at Virginia Tech is a 2592 processor supercomputer equipped with power aware components suitable for large scale green computing research. DIRECT is a deterministic global optimization algorithm, implemented in the mathematical software package VTDIRECT95. This paper explores the potential energy savings for the parallel implementation of DIRECT, called pVTdirect, when used with a large scale computational biology application, parameter estimation for a budding yeast cell cycle model, on System G. Two power aware approaches for pVTdirect are developed and compared against the CPUSPEED power saving system tool. The results show that knowledge of the parallel workload of the underlying application is beneficial for power management.",2009
Multiple Foci Drill-Down through Tuple and Attribute Polyarchies in Tabular Data.,"Information analysis often involves decomposing data into sub-groups to allow for comparison and identification of relationships.  Breakdown Visualization provides a mechanism to support this analysis through user guided drill-down of polyarchical metadata.  This metadata describes multiple hierarchical structures for organizing tuple aggregation and table attributes.  This structure is seen in sport statistics, financial data, organizational structures, and other fields.  A spreadsheet format enables comparison of visualizations at each level of the hierarchy.  Breakdown Visualization allows users to drill-down a single hierarchy then pivot into another hierarchy within the same view.  We utilize a fix and move technique that allows users to select multiple foci for drill-down.  We present an analysis scenario that demonstrates how Breakdown Visualization can be used to perform financial statement analysis",2002-06-01
An Algorithmic Solution to the Minimax Resource Allocation Problem with Multimodal Functions,"The problem of allocation is one of the most widely investigated topics in the area of mathematical optimization because of its broad applicability to different classes of real world problems.  The basic idea is that, given some type of resource whose total amount is N, we want to partition and allocate the total resource over n activities to minimize some objective function, F, whose value represents the ""cost"" of the allocation.  As can be seen from various research results in this area, the procedures to solve discrete and continuous resource allocation problems can be significantly different.  One obvious basic difference is that the discrete problem can be exactly solved by exhaustive enumeration, while the continuous problem cannot.",1993
GridWeaver: A Fully-Automatic System for Microarray Image Analysis Using Fast Fourier Transforms,"Experiments using microarray technology generate large amounts of image data that are used in the analysis of genetic function. An important stage in the analysis is the determination of relative intensities of spots on the images generated. This paper presents GridWeaver, a program that reads in images from a microarray experiment, automatically locates subgrids and spots in the images, and then determines the spot intensities needed in the analysis of gene function. Automatic gridding is performed by running Fast Fourier Transforms on pixel intensity sums. Tests on several data sets show that the program responds well even on images that have significant noise, both random and systemic.",2008
The Requirements for Effective Hardware Description Languages,"The design of hardware description languages (HDL's) is  considered with respect to their structural and functional  properties rather than their syntactic forms. The contents  of an idealized HDL are contrasted with those of nine  existing languages, chosen so as to typify a wide range of usage, numerous citations in the literature, and a diversity of source.",1975
CHITRA94: A Tool to Dynamically Charaterize Ensembles of Traces for Input Data Modeling and Output Analsis,"CHITRA94 is the third generation of a comprehensive system to visualize, transform, statistically analyze, and model ensembles of trace data and validate the resultant models.  The tool is useful for deriving input data models from trace data as well as for analysis of trace data output by a simulation.  The tool uses an stochastic (possibly no-Markovian) process as its fundamental modeling component. The tool is unique in several respects.  First, it focuses on the dynamic characterization of systems.  Consequently it includes tests for homogeneity of ensembles, stationarity of individual traces, and the ability to partition ensembles into transient and stationary pieces. Second, CHITRA94 is a scalable performance tool: its methods are designed to work with an arbitrary number of traces in an ensemble so that a user can examine the variability of system behaviors across different traces (representing different observation periods, different system configurations, etc.).  To analyze multiple traces, the user can either (1) map the traces to a model that characterizes the dynamic behavior or (2) use a novel visualization, the mass evolution graph, that shows the probability mass distribution evolution of one or more traces.  A mass evolution graph is easily constructed from trace data, and the derivative of the paths it contains yield an estimate of probability mass as a function of time.  Third, it analyzes categorical, not just numerical, time series data.  Categorical data arises naturally in many computer and communication system modeling problems.  Fourth, it includes a library of transforms that reduce the state space of the stochastic process generated.  Fifth, CHITRA94 is implemented as a user extensible collection of small programs, organized as a library, which allows the user to write new library modules that use existing modules to automate analysis procedures.  Instead, CHITRA94 may be invoked from command line, under a GUI, or integrated with another tool (e.g., a simulation model development or CASE tool).  The use of CHITRA94 is illustrated on a variety of trace data, and the extensibility is illustrated on the problem of partitioning an ensemble of 60 traces of compressed entertainment video into mutually exclusive, exhaustive, and homogeneous subsets from which a hierarchical workload model is derived.",1994-06-01
Parallel Discrete Event Simulation: A Modeling MethodologicalPerspective,"The field of parallel discrete event simulation is entering a period of self-assessment.  Fifteen years of investigation has seen great strides in techniques for efficiently executing discrete event simulations on parallel and distributed machines.  Still, the discrete event simulation community at large has failed to recognize much of these results.  One reason for this is perhaps a disagreement in the focus and purpose of the parallel discrete event simulation research community (primarily computer scientists) and the discrete event simulation community (a widely diverse group including operations researchers, statisticians, as well as computer scientists).  An examination of the parallel discrete event simulation problem from a modeling methodological perspective illustrates some of these differences and reveals potentials for their resolution.",1994
The JONES Microprocessor: Design Details,This technical report is no longer available.,1987
A Probability-one Homotopy Algoithm for Non-Smooth Equations and Mixed Complementarity Problems,"A probability-one homotopy algorithm for solving nonsmooth equations is described. This algorithm is able to solve problems involving highly nonlinear equations,where the norm of the residual has non-global local minima.The algorithm is based on constructing homotopy mappings that are smooth in the interior of their domains.The algorithm is specialized to solve mixed complementarity problems through the use of MCP functions and associated smoothers.This specialized algorithm includes an option to ensure that all iterates remain feasible.Easily satisfiable sufficient conditions are given to ensure that the homotopy zero curve remains feasible,and global convergence properties for the MCP algorithm are developed.Computational results on the MCPLIB test library demonstrate the effectiveness of the algorithm.",2000
Scheduling of Load Balancing Across Single-Channel Broadcast Networks,"The problem of optimizing the balancing of processing load originating at the various sites of heterogeneous processors is examined.  The optimal amounts of load exchange among the sending and receiving processors are derived.  The necessary and sufficient condition for the absence of synchronization delay is derived.  The minimum communication capacity needed for the optimal load exchange is determined.  Although not unique, the specified optimal load transfer schedule is amenable to simple implementation.  Practical implementation of the specified bandwidth partitioning may employ any appropriate scheme of communication multiplexing available on the given nwork.  An example with specific problem parameters is used to illustrate the determination and implementation of the optimal load transfer schedule.",1993
Data and Activity Representation for Grid Computing,"Computational grids are becoming increasingly popular as an infrastructure for computa- tional science research. The demand for high-level tools and problem solving environments has prompted active research in Grid Computing Environments (GCEs). Many GCEs have been one-o development eorts. More recently, there have been many eorts to dene component ar- chitectures for constructing important pieces of a GCE. This paper examines another approach, based on a `data-centric' framework for building powerful, context-aware GCEs spanning mul- tiple layers of abstraction. We describe a scheme for representing data and activities in a GCE and outline various tools under development which use this representation.",2002-07-01
Scalable Storage for Digital Libraries,I propose a storage system optimised for digital libraries.  Its key features are its heterogeneous scalability; its integration and exploitation of rich semantic metadata associated with digital objects; its use of a name space; and its aggressive performance optimisation in the digital library domain.,2002-10-01
A Domain For Functional Programming Systems,"A domain for functional programming systems is proposed. This domain is the powerset of a set of items where items are either atomic or ordered pairs of items. The structure of the domain is detennined by the relation 'is weaker than' and by three basic operations, under which the domain is closed: Union, the cartesian product, and the operation of application.",1981
Software Metrics and the Object-Oriented Paradigm,"Software metrics are in use to guide current software development practices.  As commercial organizations make use of the benefits of the object-oriented paradigm, the desire to apply metrics to that paradigm has logically followed.  However, standard procedural metrics are limited in their ability to describe true object-oriented designs and code, and in some aspects fail outright.  This paper describes the difficulties in applying standard metrics to object-oriented code and defines a set of metrics which are specifically geared toward the features which make the object-oriented approach unique.",1990
Specifying and Inheriting Concurrent Behavior in an Actor-BasedObject-Oriented Language,"Using CCS behavior equations to specify and reason about the observable behavior of concurrent objects, we demonstrate that a language mechanism called a behavior set can be used to capture the behavior of actor-like objects.  Using behavior equations as a formal representation of concurrent object behavior results in the explication of a mapping from the domain of objects to a domain of behavior sets. We call this mapping the behavior function.  By expressing relevant object states, behavior sets and the behavior function as first-class, inheritable, and mutable entities in a concurrent object-oriented language, we have defined the conditions which must be met in order to inherit concurrent behavior free of known anomalies.",1990
A Survey of Probability-One Homotopy Methods for Engineering Optimization,"Probability-one homotopy methods are a class of algorithms for solving nonlinear systems of equations that are accurate, robust, and converge from an arbitrary starting point almost surely.  These globally convergent homotopy techniques have been successfully applied to solve Brouwer fixed point problems, polynomial systems of equations, discretizations on nonlinear two-point boundary value problems based on shooting, finite differences, collocation, and finite elements, and Galerkin approximations to nonlinear partial differential equations. This paper surveys the basic theory of globally convergent probability-one homotopy algorithms relevant to optimization, describes some computer algorithms and mathematical software, and applies homotopy theory to unconstrained optimization, constrained optimization, and global optimization of polynomial programs.  In addition, two realistic engineering applications (optimal design of composite laminated plates and fuel-optimal orbital satellite maneuvers) are presented.",1990
Effect of Touch Screen Target Location on User Accuracy,"Users can be frustrated by touch screen applications that inaccurately record their touches.  Enlarging touch sensitive regions can improve touch accuracy, but few specific quantitative guidelines are available.  This paper reports on a controlled experiment that investigated the effect of target location and horizontal viewing location on user accuracy.  Measurements showed that persons tended to touch below the target, with touch distance increasing as the target location moved down the screen.  In addition, they tended to touch toward the sides of the screen.  Using collected data for each of nine screen sectors, graphs were prepared that show the relationship between touch target size and expected accuracy.  For example, a 36 mm-squared target in the top left sector would be expected to record accurately 99% of its touches.  The empirically-derived, quantitative guidelines will help designers create screens that decrease user errors and frustrations.",1990
An Active Set Algorithm for Tracing Parametrized Optima,Optimization problems often depend on parameters that define constraints or objective functions.  It is often necessary to know the effect of a change in a parameter on the optimum solution.  An algorithm is presented here for tracking paths of optimal solutions of inequality constrained nonlinear programming problems as a function of a parameter. The proposed algorithm employs homotopy zero-curve tracing tecnniques to track segments where the set of active constraints is unchanged.  The transition between segments is handled by considering all possible sets of active constraints and eliminating nonoptimal ones based on the signs of the Lagrange multipliers and the derivatives of the optimal solutions with respect to the parameter.,1990
Local Search for the Retrieval Layout Problem,"An information graph is a data representation for object-oriented databases in which each object is a vertex and each relationship between objects is an edge.  The retrieval layout problem is to arrange the storage of an information graph on a physical medium so as to use storage efficiently and to allow rapid navigation along paths in the graph.  This paper describes an empirical study of the performance of various local search heuristics for the retrieval layout problem, including local optimization, simulated annealing, tabu search, and genetic algorithms.  In addition, the hierarchical hybrid approach is introduced.",1993-08-01
Some Performance Tests of Convex Hull Algorithms,"The two-dimensional convex hull algorithms of Graham, Jarvis, Eddy, and Akl and Toussaint are tested on four different planar point distributions. Some modifications are discussed for both the Graham and Jarvis algorithms. Timings taken of FORTRAN implementations indicate that the Eddy and Akl-Toussaint algorithms are superior on uniform distributions of poi nts in the plane. The Graham algorithm outperformed the others on those distributions where most of the points were on or near the boundary of the hull.",1983
A Spatial Data Structure,No abstract available.,1979
The Decomposition of Rademacher-Walsh Spectra,No abstract available.,1977
Digital Library logging using XML,This project explores the various ways in which relevant statistics can be extracted from digital library logs collected in XML. A set of potential statistics that can be used for performing clickstream analysis are listed. Clickstream analysis deals with the path taken by the user when he/she is using the digital library site. This project also involves visualization of the statistics collected. Visualizations are an intuitive way to represent raw data and they can help in gaining more insight into the statistics. The target digital library was CITIDEL and the XML logs collected from this digital library were used in the project. We also designed and developed a prototype for collection of statistics and visualizing them. Implementation of the tools was done using Java and PHP. JpGraph was used for building visualizations in PHP.,2003
Model Representation in Discrete Event Simulation: I. Prospects for Developing Documentation Standards,"The representation of model dynamics, i.e., the time-related interaction among entities in the model, poses the major unresolved problem in simulation model description. This report identifies the issues relevant to the development of model documentation standards. The argument is advanced that documentation and model specification share common objectives and should be coincident. Prior approaches to the generalization of simulation model specification are reviewed with regard to their potential as documentation tools. Their inadequacies stimulate the proposal for  the ""conical methodology"", which emphasizes a top-down, structured model definition phase followed by bottom-up specification. To implement the conical methodology, a simulation model specification and documentation language (SMSDL) is needed. The characteristics of a SMSDL are identified. Recent publications suggest that certain program generator research and the DELTA Project are pursuing objectives similar to those cited for a SMSDL.",1978
A History of Discrete Event Simulation Programming Languages,"The history of simulation programming languages is organized as a progression in periods of similar developments.  The five periods, spanning 1955-1986, are labeled:  The Period of Search (1955-1960); The Advent (1961-1965); The Formative Period (1966-1970); The Expansional Period (1971-1978); and The Period of Consolidation and Regeneration (1979-1986).  The focus is on recognizing the people and places that have made important contributions in addition to the nature of the contribution. A balance between comprehensive and in-depth treatment has been reached by providing more detailed description of those languages which have or have had major use.  Over 30 languages are mentioned, and numerous variations are described in the major contributors.  A concluding summary notes the concepts and techniques either originating with simulation programming languages or given significant visibility by them.",1993
Distributed Garbage Collection of Active Objects,"This paper shows how to perform distributed automatic garbage collection of objects possessing their own thread of control.  The relevance of garbage collection and concurrent objects used in the paper is explained.  The collector is comprised of a collection of independent local collectors, one per node, loosely coupled to a distributed global collector.  The mutator (application), the local collectors and the global collector run concurrently.  The synchronization necessary to achieve correct and efficient concurrent operation between the collectors and the mutator is presented in detail.  An interesting aspect of the distributed collector is the termination algorithm: the collector algorithm running on one node, which considers itself to be ""done,"" may become ""undone"" by the action of a collector algorithm on another node.",1990
Geometric Performance Analysis of Semaphore Programs,"A key problem in designing parallel programs that achieve a desired performance goal is the ability to analyze exactly program performance, given a specification of the process syncronization structure and the execution timings of all code segments.  The problem solved in this paper is to derive, for all possible process starting times, the set of all possible limit cycle execution sequences in which a process blocks. This paper makes two contributions.  First, it employs a novel analysis method that derives timed execution sequences from a geometric model of program execution, called timed progress graphs.  Second, it solves the timed progress graph not by a computational geometric algorithm, as employed by most solutions in the literature to untimed progress graphs, but by an analytic solution.",1993-08-01
Managerial Issues in Developing a Quality Metrics Program,"Software quality metrics are used to determine error prone code due to excessive complexity.  These results can be used to guide testing efforts and predict future maintenance needs.  However, implementing a quality metrics program involves many subtle issues which complicate the development and use of a metrics methodology.  Many of theses issues are managerial in nature.  This paper examines some managerial elements of designing and implementing a quality metrics program.  Previous studies which incorporate a metrics methodology into two different commercial environments are used to demonstrate the difficulties in implementation and approach.",1990
Positivity Conditions for Quartic Polynomials,"Simple necessary and sufficient conditions that a quartic polynomial f(z) be non-negative for z greater than or equal to 0 or a is less than or equal to z and z is less than or equal to b are derived, and illustrated geometrically.  The geometry provides considerable insight and suggests various approximations and computational simplifications. The theory is applied to monotone quintic spline interpolations, giving necessary and sufficient conditions and an algorithm for monotone Hermite quintic interpolation.",1990
A Working Paper On The Development Of Multiprocessor Architectures For Supporting Secure Database Management,No abstract available.,1978
Overview of Digital Library Components and Developments,"Digital libraries are being built upon a firm foundation of prior work as the high-end information systems of the future.  A component architecture approach is becoming popular, with well established support for key components like the repository, especially through the Open Archives Initiative.  We consider digital objects, metadata, harvesting, indexing, searching, browsing, rights management, linking, and powerful interfaces.  Flexible interaction will be possible through a variety of architectures, using buses, agents, and other technologies.  The field as a whole is undergoing rapid growth, supported by advances in storage, processing, networking, algorithms, and interaction.  There are many initiatives and developments, including those supporting education, and these will certainly be of benefit in Latin America.",2002
A Geometric Problem in Simplicial Cones with Applications to Linear Complementarity Problems,"We consider the following geometric question: suppose we are given a simplicial cone K in R^n. Can we find a point @) in the interior of K satisfying the property that the orthogonal projection of @) onto the linear hull of every face of K is in the relative interior of that fence? This question plays an important role in determining whether a certain class of linear complementarity problems (LCP 's) can be solved efficiently by a pivotal algorithm. The answer to this question is always in the affirmative if n=2, but not so for n=3. We establish some conditions for the answer to this question to be yes, and relate them to other well known properties of square matrices.  e.g., world: simplicial cones, orthogonal projections, faces, linear complementarity problem, LCP, pivotal algorithms, P-matrices, symmetric positive definite matrices, 2-matrices, M-matrices.",1986
Multifaceted Web Services: An Approach to Secure and Scalable Grid Scheduling,A multifaceted or multi-interface web service is a web service that offers interfaces to clients and to other peer web services.  The multifaceted web service uses a generic parameter-based approach developed in this paper to allow for collaboration between web servers.  This parametric approach solves security and scalability problems and is found to be applicable in many situations.  Such an approach is necessary to permit easy collaboration and secure resource sharing.  We describe one instantiation of such an approach in the form of a global grid scheduler for data parallel (or Single Program Multiple Data) programs.  This scheduler demonstrates the usefulness of our approach in current business environments where administrative policies are a major factor in scheduling decisions.,2002-10-01
Open Peer to Peer Technologies,"Peer-to-peer applications allow us to separate out the concepts of authoring information and publishing that same information. It allows for decentralized application design, something that is both an opportunity and a challenge.  All the peer-to-peer applications, in various ways, return the content, choice, and control to ordinary users. Tiny end points on the Internet, sometimes even without knowing each other, exchange information and form communities. In these applications there are no more clients and servers, instead the communication takes place between cooperating peers. There are many applications nowadays which are being labeled as peer-to-peer. A way to examine the distinction of whether an application is peer-to-peer or not is to check on the owner of the hardware that the service runs on. Like Napster, if the huge part of the hardware that Napster runs on is owned by the Napster users on millions of desktops then it is peer-to-peer. Peer-to-peer is a way of decentralizing not only features, but costs and administration also. By decentralizing data and therefore redirecting users so they download data directly from other user's computers, Napster reduced the load on its servers to the point where it could cheaply support tens of millions of users. The same principle is used in many commercial peer-to-peer systems. In short peer-to-peer cannot only distribute files. It can also distribute the burden of supporting network connections. The overall bandwidth remains the same as in centralized systems, but bottlenecks are eliminated at central sites and equally importantly, at their ISPs.  Search techniques are important to making peer-to-peer systems useful. But there is a higher level of system design and system use. Topics like trust, accountability and metadata have to be handled before searching is viable.",2002-09-01
An Implementation of D. H. Warren's PROLOG Machine on a Vax 11/780,"This report describes, in considerable detail, the implementation of a Prolog compiler on a VAX. This implementation is modeled after Warren's Abstract Machine (WAM) for Prolog. The report clarifies and gives examples of the most important concepts in Warren's dissertation. In this regard, this report serves as a tutorial for others who wish to implement a Prolog compiler.",1986
Complexity Measurement of a Graphical Programming Language,"For many years the software engineering community has been attacking the software reliability problem on two fronts: First via design methodologies, languages and tools as a precheck on quality and second by measuring the quality of produced software as a postcheck.  This research attempts to unify the approach to creating reliable software by providing the ability to measure the quality of a design prior to its design implementation.  Using a graphical design language in an effort to support cognitive science research, we have successfully defined and applied Software Quality Metrics to graphical designs in an effort to predict software quality early in the software lifecycle.  Metrics values from the Graphical Design are input to predictor equations, provided in this paper, to give metric values for the resultant source code.",1987
Beyond Harvesting: Digital Library Components as OAI Extensions,"Reusability always has been a controversial topic in Digital Library (DL) design. While componentization has gained momentum in software engineering in general, there has not yet been broad DL standardization in component interfaces. Recently, the Open Archives Initiative (OAI) has begun to address this by creating a standard protocol for accessing metadata archives. It is proposed that this protocol be extended to act as the glue that binds together various components of a typical DL. In order to test the feasibility of this approach, a set of protocol extensions was created, implemented, and integrated as components of production and research DLs. The performance of these components was analyzed from the perspective of execution speed, network traffic, and data consistency. On the whole, this work has simultaneously revealed the feasibility of such OAI extensions for component interaction, and has identified aspects of the OAI protocol that constrain such extensions.",2002
Structural Processing of Visual Information,"One lesson that has been learned from previous approaches to scene analysis is that local methods are insufficient for extracting reliable information about the contents of a scene. Two different procedures that have been tried in order to remedy this deficiency are the use of knowledge via a priori information and internal models and multilevel analysis based on hierarchies of representations such as cone systems. It does not seem appropriate to drive the very first levels of analysis by a priori knowledge. It is doubtful that it will be possible to use knowledge in a way general and versatile enough to direct low level processing, and there is a need for some powerful data driven mechanisms that might at a later stage invoke internal models. It would seem more appropriate to obtain some crude global information through glancing or planning at low resolution levels that can drive a more scrutinous analysis at high resolution levels. While hierarchal systems are therefore good, the way they are currently being constructed is not necessarily good.  In this context the issue of low level representation becomes more and more important, and not enough attention has been paid to this issue. Even Marr's provocative ideas about his primal sketch do not go to a sufficient level of analysis, and it is felt that more of the workload should be thrown onto the first processing levels. In this paper is posited a comprehensive hierarchal data structure that requires no decisions and therefore no parameters for its construction. The technique does not require preselected windows, but rather uses context-dependent criteria. The data structure is versatile, easily computed, and invertible in the sense that the original image is completely recoverable.",1977
Globally Convergent Homotopy Methods: A Tutorial,"The basic theory for probability one globally convergent homotopy algorithms was developed in 1976, and since then the theory; algorithms, and applications have considerably expanded.  These are algorithms for solving nonlinear systems of (algebraic) equations, which are convergent for almost all choices of starting point. Thus they are globally convergent with probability one. They are applicable to Brouwer fixed point problems, certain classes of @erofin mg problems unconstrained optimization, linearly constrained optimization, nonlinear complementarity, and the discretizations of nonlinear two-point boundary value problems based on shooting, finite differences, collocation, and finite elements. A mathematical software package, HOMPACK, exists that implements several different strategies and handles both dense and sparse problems. Homotopy algorithms are closely related to ODE algorithms, and make heavy use of ODE techniques. Homotopy algorithms for some classes of nonlinear systems, such as polynomial systems, exhibit a large amount of coarse grain parallelism. These and other topics are discussed in a tutorial fashion.",1987
Optimal Trajectory Planning for a Space Robot Docking with a Moving Target via Homotopy Algorithms,The mathematical formulation of optimal trajectory planning for a space robot docking with a moving target is derived.  The calculus of variations is applied to the problem so that the optimal robot trajectory can be obtained directlt from the target information without first planning the trajectory of the end-effector.  The nonlinear two-point boundary value problem resulting from the problem formulation is solved numerically by a globally convergent homotopy algorithm.  The algorithm guarantees convergence to a solution for an arbitrarily chosen initial guess.  Numerical simulation for three examples demonstrates the approach.,1993
A Markov Model of Cyclic Structured Programs,The paper defines a class of flow graphs which possess neither absorbing nor transient states. It gives a necessary and sufficient condition that any transition matrix associated with such a program  be primitive. With a fixed flowgraph is associated an equivalence class of programs and an equivalence class of transition matrices. The paper investigates the hypothesis that the equivalence class of processes generated by the programs is modeled by the behavior of the equivalence class of eigenvectors generated by the transition matrices. The geometric implications are considered as well as the statistical behavior of the model as applied to the first fourteen algorithms of the Communications of the ACM.,1977
Program Testing and Conditional Correctness,"It is shown that some beliefs about program testing are incorrect. A new notion of correctness, conditional correctness, is defined. It is then shown that conditional correctness, which can in principle be achieved by  testing, is not accomplished by such methods as ""testing all branches"" or ""testing all paths"". The latter method  is proven to be not only insufficient but also highly redundant. Rules for establishing conditional correctness by testing are given and illustrated by an example.",1976
Ensemble-based chemical data assimilation II: Real observations,"Data assimilation is the process of integrating observational data and model predictions to obtain an optimal representation of the state of the atmosphere. As more chemical observations in the troposphere are becoming available, chemical data assimilation is expected to play an essential role in air quality forecasting, similar to the role it has in numerical weather prediction. Considerable progress has been made recently in the development of variational tools for chemical data assimilation.  In this paper we assess the performance of the ensemble Kalman filter (EnKF) and compare it with a state of the art 4D-Var approach. We analyze different aspects that affect the assimilation process, investigate several ways to avoid filter divergence, and investigate the assimilation of emissions. Results with a real model and real observations show that EnKF  is a promising approach for chemical data assimilation. The results also point to several issues on which further research is necessary.",2006-03-01
Comparison of an Object-Oriented Programming Language to a Procedural Programming Language for Effectiveness in Program Maintenance,"New software tools and methodologies make claims that managers often believe intuitively, without evidence. Many unsupported claims have been made about object-oriented programming. However, without scientific evidence, it is impossible to accept these claims as valid. Although experimentation has been done in the past, most of the research is very recent and the most relevant research has serious drawbacks. This paper describes an experiment which compares the maintainability of two functionally equivalent systems in order to explore the claim that systems developed with object-oriented languages are more easily maintained than those programmed with procedural languages. We found supporting evidence that programmers produce more maintainable code with an object oriented language than a standard procedural language.",1988
A Sensory Input System for Autonomous Mobile Robots,"In order to accomplish navigation in an similar world a robot must be able to build and update its own world map continuously and in real time. This paper proposes a sensory input system based on the fusion of simple low-resolution vision with directed high-resolution sonar. The basic idea is to use a simple vision system to locate the position in which an obstacle lies, and then use an ultrasonic rangefinder to determine the depth of the object and to gain clues about its shape. By fusing two simple systems we attempt to exploit the strengths of each while maintaining an acceptable computational cost. An idealized example is given and we discuss the possibilities and some of the problems.",1987
"Validation, Verification, and Testing Techniques Throughout the Life Cycle of a Simulation Study","Life cycle validation, verification, and testing (VV&T) is extremely important for the success of a simulation study.  This paper surveys current software VV&T techniques and current simulation model VV&T techniques and describes how they can all be applied throughout the life cycle of a simulation study.  The processes and credibility assessment stages of the life cycle are described and the applicability of the VV&T techniques for each stage is stated.  A glossary is provided to explicitly define important terms and VV&T techniques.",1994-08-01
Simulation Model Specifications: On the Role of Representation in the Model-Centered Sciences,"The old adage that ""a problem correctly formulated is half solved"" is rarely challenged.  Model-centered problem solving relies critically on a correct model.  Complete, clearly stated assumptions, a precise statement of objectives, and an adequate representation of the model of the system under study are readily accepted as responsibilities of the modeler.  But just how important are both the process and product of model representation?  The modeler's responsibilities in producing a correct model can be either significantly aided or drastically inhibited by the representational mechanism - the language - being used.  The comprehensibility of the model, subsequent extension, adaptation, or maintenance of the model, are highly dependent on the representational language.  Language for model representation is therefore an indispensible tool.  The pivotal role of this tool is described through its relationship to modeling methodology, model development environments, and automated model diagnosis.  The need for an emphasis on the capabilities for conceptual expressiveness, in contrast with execution efficiency is discussed.  While software engineering research clearly advocates this redefinition of emphasis, we observe that much of the research in the rising disciplines within parallel computation appears to ignore the lessons of this history.",1994
Project GEOSIM: The First Two Modules,"We present our initial report on Project GeoSim:  a multidisciplinary effort involving members of the Departments of Geography and Computer Science at Virginia Tech to develop computer-aided instruction (CAI) software for the teaching of introductory geography.  Project GeoSim will apply the immense capabilities of Geographic Information Systems (GIS) and simulation to the teaching of geography, beginning at the first geography course. Our computer-aided educational materials will be usable by a wide range of students, focusing in particular on first-year undergraduate and high school students.  Initially we will provide a series of computerized laboratory modules applicable to several introductory geography courses.  These modules must meet several criteria such as being highly interactive, appropriate for computer novices, incorporating a Geographic Information System (GIS) and simulation when appropriate, and able to run on a wide range of moderately priced equipment.  This paper presents the overall project, and describes the working prototypes for two of these modules:  Mental Maps and International Population.",1991
Globally Optimal Transmitter Placement for Indoor Wireless Communication Systems,"In this paper, a global optimization technique is applied to solve the optimal transmitter placement problem for indoor wireless systems. An efficient pattern search algorithm ---DIRECT (DIviding RECTangles) of Jones, Perttunen, and Stuckman(1993)---has been connected to a parallel 3D radio propagation ray tracing modeler running on a 200-node Beowulf cluster of Linux workstations. Surrogate functions for a parallel WCDMA (wideband code division multiple access) simulator were used to estimate the system performance for the global optimization algorithm. Power converage and BER(bit error rate) are considered as two different criteria for optimizing locations of a specified number of transmitters across the feasible region of the design space. This paper briefly describes the undrelying radio propagation and WCDMA simulations and focuses on the design issues of the optimization loop.",2002-08-01
Parametric Design Optimization of Uncertain Ordinary Differential Equation Systems,"This work presents a novel optimal design framework that treats uncertain dynamical systems described by ordinary differential equations. Uncertainty in multibody dynamical systems comes from various sources, such as: system parameters, initial conditions, sensor and actuator noise, and external forcing. The inclusion of uncertainty in design is of paramount practical importance because all real-life systems are affected by it. Designs that ignore uncertainty often lead to poor robustness and suboptimal performance. In this work uncertainties are modeled using Generalized Polynomial Chaos and are solved quantitatively using a least-square collocation method. The uncertainty statistics are explicitly included in the optimization process. Systems that are nonlinear, have active constraints, or opposing design objectives are shown to benefit from the new framework. Specifically, using a constraint-based multi-objective formulation, the direct treatment of uncertainties during the optimization process is shown to shift, or off-set, the resulting Pareto optimal trade-off curve.     A nonlinear vehicle suspension design problem, subject to parametric uncertainty, illustrates the capability of the new framework to produce an optimal design that accounts for the entire family of systems within the associated probability space.",2011
Analysis of an Enumeration Algorithm for Unordered K-partitions of N,"In many combinatorial problems, the need to compute the number of unrestricted partitions of n or to enumerate over the set of unrestricted partitions  of n occurs frequently. Let two positive integers k and n be given, the set  of unordered k-tup1es (a1,a2,..., ak) of positive integers such that a1+a2+...+ak=n is called the unordered k-partitions of n. In this paper, an algorithm to  enumerate the set is presented and analyzed. The running time of the algorithm  is shown to be linearly proportional to the number of elements in the set.  The number of unordered k-partitions of n, f_k(n), is given in a recurrence  relation as a byproduct. With these results, to enumerate the unrestricted  partitions of n, we need only to enumerate successfully the unordered 1-  partition of n, 2-partitions of n, ... , up to n-partitions of n.",1974
Searching One Multiplier in Glm,"A unified approach is developed for one-dimensional GLM. The major result is  a convergence theorem for interval reduction. Comparative analysis of bisection,  linear interpolation and tangential approximation reveals the relative advantages of tangential approximation.",1974
"Index for ""History of Computing in the Twentieth Century""",No abstract available.,1983
Computational Geometric Performance Analysis of Limit Cycles in Timed Transition Systems,"Certain parallel software performance evaluation problems are equivalent to computational geometric problems.  Consider a timed transition system representing a parallel program: a set of variables, a set of states, an initial state, a transition function mapping a set to a set of successor states, and a description of the time between transitions.  Given a timed transition system, the paper solves four problems: (1) state the necessary and sufficient conditions for a TES to contain a LCES; (2) given the initial starting time of each process, find a representation of the set of all possible TESs; (3) determine if any initial process starting times exist that lead to a LCES in which no process ever blocks; and (4) find the set of all possible LCESs.",1993-09-01
Ensemble-based Chemical Data Assimilation III: Filter Localization,"Data assimilation is the process of integrating observational data and model predictions to obtain an optimal representation of the state of the atmosphere. As more chemical observations in the troposphere are becoming available, chemical data assimilation is expected to play an essential role in air quality forecasting, similar to the role it has in numerical weather prediction. Considerable progress has been made recently in the development of variational tools for chemical data assimilation. In this paper we implement and assess the performance of a localized  ``perturbed observations'' ensemble Kalman filter (LEnKF). We analyze different settings of the ensemble localization, and investigate the joint assimilation of the state, emissions and boundary conditions. Results with a real model and real observations show that LEnKF is a promising approach for chemical data assimilation. The results also point to several issues on which future research is necessary.",2006-03-01
Computer-Supported Collaborative Production,"This paper proposes the concept of collaborative production as a focus of concern within the general area of collaborative work. We position the concept with respect to McGrath's framework for small group dynamics and the more familiar collaboration processes of awareness, coordination, and communication (McGrath 1991). After reviewing research issues and computer-based support for these interacting aspects of collaboration, we turn to a discussion of implications for how to design improved support for collaborative production. We illustrate both the challenges of collaborative production and our design implications with a collaborative map-updating scenario drawn from the work domain of geographical information systems.",2006-03-01
A Polynomial Chaos Based Bayesian Approach for Estimating Uncertain Parameters of Mechanical Systems – Part I: Theoretical Approach,"This is the first part of a two-part article.  A new computational approach for parameter estimation is proposed based on the application of the polynomial chaos theory.  The polynomial chaos method has been shown to be considerably more efficient than Monte Carlo in the simulation of systems with a small number of uncertain parameters. In the new approach presented in this paper, the maximum likelihood estimates are obtained by minimizing a cost function derived from the Bayesian theorem.  Direct stochastic collocation is used as a less computationally expensive alternative to the traditional Galerkin approach to propagate the uncertainties through the system in the polynomial chaos framework.  This approach is applied to very simple mechanical systems in order to illustrate how the cost function can be affected by undersampling, non-identifiablily of the system, non-observability, and by excitation signals that are not rich enough.  When the system is non-identifiable, regularization techniques can still yield most likely values among the possible combinations of uncertain parameters resulting in the same time responses than the ones observed.  This is illustrated using a simple spring-mass system.  Possible applications of this theory to the field of vehicle dynamics simulations include the estimation of mass, inertia properties, as well as other parameters of interest.  In the second part of this article, this new parameter estimation method is illustrated on a nonlinear four-degree-of-freedom roll plane model of a vehicle in which an uncertain mass with an uncertain position is added on the roll bar.",2007
On Data Types,"A semantic model of types is proposed. This model interprets types as elements in an augmented domain constructed with the Smyth powerdomain constructor. In this domain types approximate the values of which they are types. Within this model, a type of an application f(x) is found by applying a type of f to a type of x. This becomes the basis of type checking and type inference.  The model accomodates in a natural way type hierarchies, polymorphic functions, and recursive polymorphic types. A number of examples are worked out in some detail.",1983
Root-heavy Directory Tree on Direct Access Storage Devices,No abstract available.,1974
Graph Embeddings and Simplicial Maps,"An undirected graph is viewed as a simplicial complex.  The notion of a graph embedding of a guest graph in a host graph is generalized to the realm of simplicial maps.  Dilation is redefined in this more general setting.  Lower bounds on dilation for various guest and host graphs are considered.  Of particular interest are graphs that have been proposed as communication networks for parallel architectures.  Bhatt et al. provide a lower bound on dilation for embedding a planar guest graph in a butterfly host graph.  Here, this lower bound is extended in two directions.  First, a lower bound that applies to arbitrary guest graphs is derived, using tools from algebraic topology.  Second, this lower bound is shown to apply to arbitrary host graphs through a new graph-theoretic measure, called bidecomposability. Bounds on the bidecomposability of the butterfly graph and of the k-dimensional torus are determined.  As corollaries to the main lower bound theorem, lower bounds are derived for embedding arbitrary planar graphs, genus g graphs, and k-dimensional meshes in a butterfly host graph.",1993
An Empirical Study of Maintenance Activities in an Object OrientedSystem,"Decades of research on maintenance activities in the procedural paradigm has produced several conclusions.  Among these conclusions are recommendations that a reduction in maintenance cost could be achieved by a more controlled design process, by more rigorous testing of potential problem areas earlier in the life cycle.  With the increasing emphasis on the object oriented paradigm, the authors performed an empirical study of the maintenance patterns in a commercial object oriented system.  Although this is a preliminary study, intuition is presented as insight into the object oriented maintenance activities.",1993
The Logical Foundations of Microlanguages,"After the consideration of two recent examples of instruction sets for microprogrammable computers, the article sketches known and new results about complete sets of functions which appear to be applicable to microlanguage development. Some needed areas of research are pointed out. Functional completeness is linked to research in control primitives for machines.",1974
Analysis of Bottlenecks in International Internet Links,"We report on preliminary results of analysis into the sources of congestion in  international Internet routes from the United States to several countries:  Brazil and South Africa.  Using the pathchar and traceroute tools, we identified which links in a variety of routes are a bottleneck.  The measures used were  throughput, round trip delay, and router queueing delays.  Measurements with  ping were used to validate the round trip times obtained with pathchar.",1998-12-01
DOMINO: A Multifaceted Conceptual Framework for Visual Simulation Modeling,"The purpose of this paper is to present a new conceptual framework for visual simulation modeling, the multifaceD coOnceptual fraMework for vIsual simulatioN mOdeling (DOMINO). The DOMINO provides both design and implementation guidance to furnish a broad range of support during the model development life cycle; enables the modeler to work under the object-oriented paradigm; guides the modeler in graphically structuring a visual simulation model at multiple levels of abstraction; enables the extraction of sufficient information from the modeler so that the model execution can be visualized; embodies a WYSIWYR (What You See Is What You Represent) philosophy and enables the modeler to represent a system as it is naturally perceived; among many other features.",1992
A Computer Implementation of the Transformations of Formulas into Prenex Normal Form,The purpose of this paper is to explain a computerized process whereby any well-formed formula (wff) of first order predicate calculus can be moved to its prenex normal form (PNF). The main features of the program demonstrate some of the interesting capabilities of the WATFIV compiler in utilizing Markov Algorithms.,1974
Introduction to Pest Control Using the Watfiv Compiler,The purpose of this paper is to consolidate into a brief guide procedures  to aid in debugging FORTRAN programs which are being executed under the control  of a WATFIV compiler.,1974
Recent Developments in Document Clustering,"This report aims to give a brief overview of the current state of document clustering research and present recent developments in a well-organized manner.  Clustering algorithms are considered with two hypothetical scenarios in mind: online query clustering with tight efficiency constraints, and offline clustering with an emphasis on accuracy.  A comparative analysis of the algorithms is performed along with a table summarizing important properties, and open problems as well as directions for future research are discussed.",2007-10-01
Design and Implementation of a Massively Parallel Version of DIRECT,"This paper describes several massively parallel implementations for a global search algorithm DIRECT. Two parallel schemes take different approaches to address DIRECT's design challenges imposed by memory requirements and data dependency. Three design aspects in topology, data structures, and task allocation are compared in detail. The goal is to analytically investigate the strengths and weaknesses of these parallel schemes, identify several key sources of inefficiency, and experimentally evaluate a number of improvements in the latest parallel DIRECT implementation. The performance studies demonstrate improved data structure efficiency and load balancing on a 2200 processor cluster.",2006
A Design Tool Used to Quantitatively Evaluate Student Projects,"In the last decade, the field of Computer Science has undergone a revolution. It has started the move from a mysterious art form to a detailed science. The vehicle for this progress has been the rising popularity of the field of Software Engineering. This innovative area of computer science has brought about a number of changes in the way we think of, and work with, the development of software. Due to this renovation, a field that started with little or no design techniques and unstructured, unreliable software has progressed to a point where a plethora of techniques exist to improve the quality of a program design as well as that of the resultant software. The popularity of structured design and coding techniques prove that there is widespread belief that the overall product produced using these ideas is somehow better, and statistics seem to indicate that this belief is true. Until recently, however, there existed no technique for quantitatively showing one program better than its functional equivalent. In the past few years, the use of software quality metrics seems to indicate that such a comparison is not only possible, but is also valid.       The advent of Software Engineering has demanded that most universities offer a Software Engineering course which entails a ""Real-World"" group project. Students participating in the class design a system using a program design language (PDL). Other students then write code from the design and finally the design team integrates the modules into a working system. For a complete description of the class see [HENS83] and [TOMJ87].",1987
The Dynamic Creation And Modification of Heuristics in a Learning Program,"POLY FACT is a learning program that attempts to factor multivariable polynomials. The program has been successful in factoring polynomials (in simplified form) with a maximum of 84 terms, each term consisting of as many as five variables and a maximum degree of 67. The complexity of this learning task placed unusual requirements on the representation of  heuristics. By using the first-order predicate calculus notation, we enable the creation and modification of heuristics dynamically during program execution. Constraints on the creation process are implemented in a series of tables by which one can alter the flexibility given to the program. Execution of heuristics begins with a translation of the predicate calculus representation to a reverse Polish string, followed by the interpretive evaluation of the Polish string. A general procedure for developing and implementing the predicate calculus representation is suggested.",1973
"Establishing Software Development Process Control: Technical Objectives, Operational Requirements, and the Foundational Framework","This paper proposes a foundational framework for establishing control over the software development process. Critical objectives stressed include (a) the complementary integration of maintenance and development activities, (b) the identification and definition of a (semi-) automated data collection and analysis process which employs quality indicators that are definitively linked to the existence of process and product attributes, and (c) the formulation and use of control methods that are designed to work within the defined automated process and to provide decision support capabilities. The significance and necessity of these objectives are established through an examination of the Abstraction Refinement Model, the Objectives/Principles/Attributes Framework and the Software Quality Indicator concept.",1992
An Algorithm for Generating the Set of Fundamental Cycles in a Graph,No abstract available.,1982
Dynamic Load Distribution Optimization in Heterogeneous MultipleProcessor Systems,"We examine the problem of optimizing the distribution of the m interacting modules of a given work load on a parallel system with p heterogeneous processors.  Average-valued parameters are used to model the intermodule coupling of the work load and its execution and communication times on the diverse system processors. We derive an analytical optimality criterion to minimize a multi-metric objective function representing a weighted combination of work load completion time, communication cost, resource utilization cost, and processor idle-time.",1993
Hanging an Elastic Ring,"A thin flexible elastic circular ring is hung at one point. The ring deforms due to its own weight. The problem depends on a non-dimensional parameter B representing the relative importance of density and length to rigidity. The heavy elastica equations are solved by perturbation for small B, by a quasi-Newton method for intermediate B, and by a homotopy method for large B. The approximate results show good agreement with numerical integration for B < 20.",1980
Visualizing and Modeling Categorical Time Series Data,"Categorical time series data can not be effectively visualized and modeled using methods developed for ordinary data.  The arbitrary mapping of categorical data to ordinal values can have a number of undesirable consequences.  New techniques for visualizing and modeling categorical time series data are described, and examples are presented using computer and communications network traces.",1995-06-01
An Empirical Study of Representation Methods for Reusable SoftwareComponents,"An empirical study of methods for representing reusable software components is described.  Thirty-five subjects searched for reusable components in a database of UNIX tools using four different representation methods: attribute-value, enumerated, faceted, and keyword.  The study used Proteus, a reuse library system that supports multiple representation methods.  Searching effectiveness was measured with recall, precision, and overlap.  Search time for the four methods was also compared.  Subjects rated the methods in terms of preference and helpfulness in understanding components.  Some principles for constructing reuse libraries, based on the results of this study, are discussed.",1994
Polynomial Response Surface Approximations for the Multidisciplinary Design Optimization of a High Speed Civil Transport,"Surrogate functions have become an important tool in multidisciplinary design optimization to deal with noisy functions, high computational cost, and the practical difficulty of integrating legacy disciplinary computer codes. A combination of mathematical, statistical, and engineering techniques, well known in other contexts, have made polynomial surrogate functions viable for MDO. Despite the obvious limitations imposed by sparse high fidelity data in high dimensions and the locality of low order polynomial approximations, the success of the panoply of techniques based on polynomial response surface approximations for MDO shows that the implementation details are more important than the underlying approximation method (polynomial, spline, DACE, kernel regression, etc.). This paper surveys some of the ancillary techniques—statistics, global search, parallel computing, variable complexity modeling—that augment the construction and use of polynomial surrogates.",2001
An Analysis of Conjunctive Goal Planning,"This thesis develops a formal theory of planning, using a simple paradigm of planning that has been previously explored in work such as GPS, HACKER, STRIPS and NOAH. This thesis analyzes the goal interactions that occur when the goal is stated as a conjunction of subgoals. In this analysis we assume that the problem has a finite state space, and that operators are reversible.  Graph theory can be used to characterize these subgoal interactions. The entire state space is treated as a graph, and each subgoal or conjunction of  subgoals defines a subgraph. Each subgraph is composed of one or more connected components. Solving each subgoal by choosing a connected component that contains a final goal state is a necessary and sufficient condition for solving any planning problem.  In the worst case, analyzing goal interactions is shown to be no more effective than enumerating the state space and searching. This complexity proves that no complete algorithm can solve all planning problems in linear time. The technique of goal ordering is analyzed, along with several extensions to that technique. While a generalization of goal ordering is possible, in the worst case generating the goal order requires as much computation as solving the problem by a brute-force search.  A technique called capability analysis, derived from the connect component results, uses first-order logic to find the constraints that must apply as subgoals are achieved. A partial implementation uses counterfactual logic to identify the components of a world state that prevent the remaining subgoals from being achieved.",1986
Using Group and Subsystem Level Analysis to Validate Software Metrics on Commercial Software Systems,"This paper reports the results of a study which examined the relationship between a collection of software metrics and the development data (such as errors and coding time) of three commercially produced software systems. The software metrics include both measures of system interconnectivity and measures of system code. This study revealed strong relationships between the metrics and the development data when individual components were aggregated by structure (into subsystems) or by similarity (into groups). The subsystem and group results imply that research and application of metrics should be focused above the component level. The group results also imply that metrics can guide the effective application of project resources by identifying those groups which, for example, will contain a disproportionately large fraction of errors.  Finally, the study showed the overall utility of two interconnectivity metrics: Henry and Kafura's information flow metric and McClure's invocation metric.  This result is significant because interconnectivity metrics can be applied early in the life cycle.",1988
Minimal Parameter Homotopies for the L2 Optimal Model Order Reduction Problem,"The problem of finding a reduced order model, optimal in the L2 sense, to a given system model is a fundamental one in control system analysis and design. The problem is very difficult without the global convergence of homotopy methods, and a number of homotopy based approaches have been proposed. The issues are the number of degrees of freedom, the well posedness of the finite dimensional optimization problem, and the numerical robustness of the resulting homotopy algorithm. Homotopy algorithms based on several formulations are developed and  compared here. The main conclusions are that dimensionality is inversely  related to numerical well conditioning and algorithmic efficiency is inversely related to robustness of the algorithm.",1992
What if There Were Desktop Access to the Computer Science Literature?,"What if there was an electronic computer science library? Consider the possibilities of having your favorite publications available within finger's reach. Consider project Envision, an ongoing effort to build a user-centered database from the computer science literature. This paper describes our first year progress, stressing the motivation underlying project Envision, user-centered development, and overall design.",1992
Low Error Path Planning for a Synchro-Drive Mobile Robot,"A path-planning system is presented which, given any points, calculates a low-error path for a synchro-drive robot. The path reduces dead-reckoning errors by using gentle, constant curvature turns for switching directions of the robot's travel---yet direct and minimal length travel are maintained. The system has been implemented and tested with a Vectrobot mobile platform.",1986
Spatial Reasoning in Remotely Sensed Data,"Photointerpreters employ a variety of implicit spatial  models to provide interpretations from remotely sensed  aerial or satellite imagery.  The process of making the  implicit models explicit and the subsequent use of explicit models in computer processing is difficult.  	In this paper one application is illustrated: 	how  ridges and valleys can be automatically interpreted from  LANDSAT imagery of a mountainous area and how a relative  elevation terrain model can  be constructed from this  interpretation.  It is shown how an illumination model is  being used to explain many of the features of a LANDSAT  image.  Finally, it is shown how to examine valleys for the  possible presence of streams or rivers and it is shown how a  spatial relational model can be set up to make a final  interpretation of the river drainage network.",1981
Verification and Validation: What Impact Should Project Size and Complexity Have on Attendant V&V Activities and Supporting Infrastructure?,"The size and complexity of Modeling and Simulation (M&S) application continue to grow at a significant rate. The focus of this panel is to examine the impact that such growth should have on attendant Verification and Validation (V&V) activities. Two prominent considerations guiding the panel discussion are: (1) Extending the current M&S development objectives to include quality characteristics like maintainability, reliability, and reusability -- the current modus operandi focuses primarily on correctness, and (2) Recognizing the necessity and benefits of tailoring V&V activities commensurate with the size of the project, i.e., one size does not fit all. In this paper we provide six questions and four sets of responses to those questions. These questions and responses are intended to foster additional thought and discussion on topics crucial to the synthesis of quality M&S applications.",1999-11-01
A Visual Simulation Support Environment Based on the DOMINO Conceptual Framework,"The purpose of this paper is to present a Visual Simulation Support Environment (VSSE) based on the multifaceD coOnceptual fraMework for vIsual simulatioN mOdeling (DOMINO). The ever-increasing complexity of visual simulation model development is undeniable. There is a need for automated support throughout the entire visual simulation model development life cycle. This support is furnished by the VSSE which is composed of integrated software tools providing computer-aided assistance in the development and execution of a visual simulation model. The VSSE has been jointly developed with the DOMINO. Its architecture consists of three layers: hardware and operating system, kernel VSSE, minimal VSSE, and VSSEs. This paper focuses on the minimal VSSE toolset. Evaluation of the VSSE shows that it adequately satisfies all of its 13 design objectives.",1992
"The Swan User's Manual, Version 1.1",Swan is a data structure visualization program.  Its main purpose is to allow the user to visualize the data structures used in a C/C++ program.  Swan is specifically designed to support visualization of programs implementing various graph algorithms.,1995-08-01
HSCT Configuration Design Space Exploration Using Aerodynamic Response Surface Approximations,"A method has been developed to generate and use polynomial approximations to the range and cruise drag components in a highly constrained, multidisciplinary design optimization of a High Speed Civil Transport configuration.  The method improves optimization performance by eliminating the numerical noise present in the analyses through the use of response surface methodology.  In our implementation, we fit quadratic polynomials within variable bounds to data gathered from a series of numerical analyses of different aircraft designs.  Because the HSCT optimization process contains noise and suffers from a nonconvex design space even when noise is filtered out, multiple optimization runs are performed from different starting points with and without the response surface models in order to evaluate their effectiveness.  It is shown that response surface methodology facilitates design space exploration, allowing improvements in terms of both convergence performance and computational effort when multiple starting points are required.",1999-03-01
Massively Parallel Simulations with Application to Queueing Networks,"This paper investigates two parallel simulation methodologies, multiple replication and parallel regenerative simulation.  Problems of applying these methodologies to massively parallel simulations are identified.  Two approaches (MR-PI and PR) are proposed to overcome some of these problems.  The MR-PI approach, based on multiple replication simulation, uses a pilot simulation to reduce the initial transient bias by starting the simulation from a state that is representative of the steady-state conditions.  The PR approach allows approximate regeneration by using substate matching for models that do no regenerate frequently.  Empirical results suggest that the proposed approaches can produce very accurate results.",1995-02-01
Cross-Platform Presentation of Interactive Volumetric Imagery,"Volume data is useful across many disciplines, not just medicine. Thus, it is very important that researchers have a simple and lightweight method of sharing and reproducing such volumetric data. In this paper, we explore some of the challenges associated with volume rendering, both from a classical sense and from the context of Web3D technologies. We describe and evaluate the pro- posed X3D Volume Rendering Component and its associated styles for their suitability in the visualization of several types of image data. Additionally, we examine the ability for a minimal X3D node set to capture provenance and semantic information from outside ontologies in metadata and integrate it with the scene graph.",2012
The Evaluation of Software Quality: An Empirical Approach to Validation,"This interim report outlines activities related to the Software Quality Assessment Project funded by the JLC/CRM. Reported activities reflect investigative efforts performed at the Systems Research Center (Virginia Tech) and at the Melpar Division of E-Systems. Accordingly, this report discusses (1) the development and installation of a document analyzer, (2) the development and refinement of document quality and process indicators, and (3) our approach to data collection. An outline of planned activities for the next reporting period is also presented.",1992
Identifying Native Applications with High Assurance,"The work described in this paper investigates the problem of identifying and deterring stealthy malicious processes on a host. We point out the lack of strong application iden- tication in main stream operating systems. We solve the application identication problem by proposing a novel iden- tication model in which user-level applications are required to present identication proofs at run time to be authenti- cated by the kernel using an embedded secret key. The se- cret key of an application is registered with a trusted kernel using a key registrar and is used to uniquely authenticate and authorize the application. We present a protocol for secure authentication of applications. Additionally, we de- velop a system call monitoring architecture that uses our model to verify the identity of applications when making critical system calls. Our system call monitoring can be integrated with existing policy specication frameworks to enforce application-level access rights. We implement and evaluate a prototype of our monitoring architecture in Linux as device drivers with nearly no modication of the ker- nel. The results from our extensive performance evaluation shows that our prototype incurs low overhead, indicating the feasibility of our model.",2011
"Between a Rock and a Cell Phone: Social Media Use during Mass Protests in Iran, Tunisia and Egypt","In this paper we examine the use of social media, and especially Twitter, in Iran, Tunisia and Egypt during the mass political demonstrations and protests in June 2009, December 2010 - January 2011, and February 2011, respectively.  We compare this usage with methods and findings from other studies on the use of Twitter in emergency situations, such as natural and man-made disasters.  We draw on our own experiences and participant-observations as an eyewitness in Iran (first author), and on Twitter data from Iran, Tunisia and Egypt. In these three cases, Twitter filled a unique technology and communication gap at least partially.  We summarize suggested directions for future research with a view of placing this work in the larger context of social media use in conditions of crisis and social convergence.",2011-05-01
Affordances and Feedback in Nuance-Oriented Interfaces,"Virtual Environments (VEs) and perceptive user interfaces must deal with complex users and their modes of interaction. One way to approach this problem is to recognize users’ nuances (subtle conscious or unconscious actions). In exploring nuance-oriented interfaces, we attempted to let users work as they preferred without being biased by feedback or affordances in the system. The hope was that we would discover the users’ innate models of interaction. The results of two user studies were that users are guided not by any innate model but by affordances and feedback in the interface. So, without this guidance, even the most obvious and useful components of an interface will be ignored.",2001
Parallel Adaptive GMRES Implementations for Homotopy Methods,"The success of homotopy methods in solving large-scale optimization problems and nonlinear systems of equations depends heavily on the solution of large sparse nonsymmetric linear systems on parallel architectures.  Iterative solution techniques, such as GMRES(k), favor parallel implementations.  However, their straightforward parallelization usually leads to a poor parallel performance because of global communication incurred by processors.  One variation of GMRES(k) considered here is to adapt the restart value k for any given problem and use Householder reflections in the orthogonalization phase to achieve high accuracy and reduce the communication overhead.",1997-11-01
Uncertainty Quantification and Apportionment in Air Quality Models using the Polynomial Chaos Method,"Simulations of large-scale physical systems are often affected by the uncertainties in data, in model parameters, and by incomplete knowledge of the underlying physics. The traditional deterministic simulations do not account for such uncertainties. It is of interest to extend simulation results with ``error bars'' that quantify the degree of uncertainty. This added information provides a confidence level for the simulation result. For example, the air quality forecast with an associated uncertainty information is very useful for making policy decisions regarding environmental protection. Techniques such as Monte Carlo (MC) and response surface are popular for uncertainty quantification, but accurate results require a large number of runs. This incurs a high computational cost, which maybe prohibitive for large-scale models. The polynomial chaos (PC) method was proposed as a practical and efficient approach for uncertainty quantification, and has been successfully applied in many engineering fields. Polynomial chaos uses a spectral representation of uncertainty. It has the ability to handle both linear and nonlinear problems with either Gaussian or non-Gaussian uncertainties.   This work extends the functionality of the polynomial chaos method to Source Uncertainty Apportionment (SUA), i.e., we use the polynomial chaos approach to attribute the uncertainty in model results to different sources of uncertainty. The uncertainty quantification and source apportionment are implemented in the Sulfur Transport Eulerian Model (STEM-III). It allows us to assess the combined effects of different sources of uncertainty to the ozone forecast. It also enables to quantify the contribution of each source to the total uncertainty in the predicted ozone levels.",2007
Representing Knowledge About Words,"Most on-line lexicons contain only semantic information.  Semantic information is usually stored elsewhere, in a form consistent with representation of the syntactic information.  This paper reports on research toward developing a large on-line lexicon from machine-readable dictionaries, which contains both syntactic and semantic information in uniform style.  The fundamental theory is that of one of the relational lexicon; we describe relational lexicons, discuss our extensions to the usual theory of relational lexicons, rehearse very quickly some of the relations we are dealing with, and show how information for some simple entries is stored.",1989
Proceedings of the Fourth Annual Virginia Tech Center for Human-Computer Interaction Research Experience for Undergraduates (REU) Symposium,"Virginia Tech's Center for Human-Computer Interaction presents the project abstracts for the REU 2009 symposium. The REU (Research Experience for Undergraduates) program provides undergraduate students from various universities with the opportunity to spend eight weeks at Virginia Tech, working with our faculty and graduate students on research projects using the state-of-the-art technology and laboratories assembled here.  The REU program is sponsored primarily by the National Science Foundation (IIS-0851774, IIS-0552732).  Additional support was provided by the NSF (CNS-0540509), the VT CS Department CSRC, and IBM Research.",2009
"Collecting, Analyzing and Visualizing Tweets using Open Source Tools","This tutorial will teach participants how to collect, analyze and visualize results from twitter data.  We will demonstrate several different free, open-source web-based tools that participants can use to collect twitter data (e.g., Archivist, 140kit.com, TwapperKeeper), and show them a few different methods, tools or programs they can use to analyze the data in a given collection.  Finally, we will show participants visualization tools and programs they can use to present the analyses, such as tag clouds, graphs and other data clustering techniques.  As much as possible this will be a hands-on tutorial, so participants can learn by making their own twitter data collection, analysis and visualization as part of the tutorial.",2011
Modeling Networks with Dynamic Topologies,"Dynamic hierarchical networks represent an architectural strategy for employing adaptive behavior in applications sensitive to highly variable external demands or uncertain internal conditions. The characteristics of such architectures are described, and the significance of adaptive capability is discussed. The necessity for assessing cost/benefit tradeoffs leads to the use of queueing network models. The general model, a network of M/M/1 queues in a random environment, is introduced and then is simplified so that the links may be treated as isolated M/M/1 queues in a random environment. This treatment yields a formula for approximate mean network delay by combining matrix-geometric results (mean queue length and mean delay) for the individual links. A discrete event simulation model is defined as a basis for cross-validation of the analytic model. Conditions under which the analytic model is considered valid are identified through comparison of the two models.",1988-05-01
Issues Related to the Explication of Process-Product Relationships in DoD-Std-21677 and DoD-Std-2168,"This paper is a discussion of issues related to the thesis entitled, ""The Explication of Process-Product Relationships in DoD-STD-2167 and DoD-STD-2168 via an Augmented Data Flow Diagram Model.""  In particular, the major results of the above thesis are viewed in light of the draft standards DoD-STD-2167 and DoD-STD-2168 (both dated 1 April 1987), and the issue of development objectives is explored.  The ideas presented in this paper represent the author's opinion and are speculative in nature due to the fact that, at present, the revised DoD standards are in draft form, and the issue of development objectives has not yet been thoroughly investigated.",1988
Microprogrammable Cellular Automata,"This paper reports research into cellular automata with two binary inputs, two binary outputs, and an octal control variable. A set of control variables is chosen and it is shown that any function of three variables can be realized by a 2 x 2 array of cells, any function of four variables by a 2 x 6 array of cells. A construction based on the Shannon Decomposition Theorem is given for the realization of functions of more than four variables. The existence of a more efficient construction is conjectured. A definition of the circuit  defining the cell is given as well as an implementation using NAND gates. A practical configuration of the cells is suggested and fault correction is discussed.",1973
"BABES: Brushing+Linking, Attributes, and Blobs Extension to Storyboard","In this day and age, people not only deal with data but deal  with vast amounts of data which needs to be sorted and made sense of.  A subset of these people are intelligence analysts who sort through an  enormous amount of data that need to be organized to uncover plots and  subplots. We are proposing a tool called BABES (Brushing+Linking,  Attributes, and Blobs Extension to Storyboard) that will enable the  intelligence analyst to sort through data efficiently, uncover plots and  subplots using the brushing and linking and attributes features and work  with multiple subplots at the same time using the concept of ’blobs’.",2008
Anticipating and Mitigating the Professional Challenge to Independent Verification and Validation,"Independent Verification and Validation faces three classes of challenges: the Technical Challenge, the Management Challenge, and the Professional Challenge.  In this paper we focus on the Professional Challenge, and, in particular, the four phases that characterize it: Denial, Anger, Cooperation and Dependence.  We believe that to implement an effective IV&V effort, one must understand the relationship among the phases and the critical issues underlying them.  For each of the phases we (a) provide a characteristic description, (b) discuss how they affect the IV&V effort, (c) present representative issues and examples, and (d) describe steps to reduce the adverse impact of the three detrimental phases.  The examples provided are those we have encountered while serving in an IV&V capacity; ""lessons learned"" guide our suggestions for addressing phase-specific issues.",1998-03-01
A Taxonomical Review of Object-Oriented Simulation Model Verification and Validation Techniques,"The goal of this paper is to provide a taxonomical review of verification and validation (V&V) techniques for assessing the accuracy of object-oriented simulation models and to present guidelines for the selection of the techniques based on their effectiveness and costs.  The techniques are evaluated based on the identified indicators that relate the techniques to their error detecting capability and the costs that measure the ease-of-use aspects.  The paper introduces a preliminary object-oriented model fault taxonomy based on a comprehensive literature survey and authors' experience.  The fault taxonomy not only compares and contrasts the error detection capabilities of the techniques but also provides a basis and guidance for detecting specific faults likely to occur due to complexities of object-oriented paradigm such as object/class hierarchies. state-dependent behavior, dynamic and diverse patterns of interactions, among the objects.  The guidelines and the comparative analysis of the object-oriented V&V techniques allow practitioners to select the techniques based on the benefits and costs.",1998-03-01
Implementation of an Enhanced Distributed Object Model using Actors,"Most common object models of distributed object systems have a limited set of object-oriented features, lacking the advanced features of 'polymorphism' (an abstraction mechanism that represents a quality or state of being able to assume different forms) and 'concurrency' (the ability to have more than one thread of execution in an object simultaneously).  The lack of support for advanced features is a serious limitation because it restricts the development of new components and limits reuse of existing components that use these advanced features.  In this paper, the Interoperable Common Object Model (ICOM) centered on statically typed object-oriented languages is presented.  The ICOM model is an attempt to elevate common object models (with the advanced features of polymorphism and concurrency) closer to the object models of statically typed object-oriented languages. Specific features of the ICOM model include: remote inheritance, method overloading, parameterized types, and guard methods.  This paper focuses on how the actor model and reflection techniques are used to develop a uniform implementation framework for the ICOM object model in C++ and Modula-3.  Two key features of the ICOM objects are discussed, remote inheritance and atomicity, along with the distributed compilation architecture of the ICOM framework emphasizing the role of the actor model in creating a simple underlying framework is emphasized.",1998-03-01
"An Empirical Study of Reuse, Quality, and Productivity",This paper presents an analysis of four sets of industrial data to determine if software reuse is correlated with higher levels of software quality and productivity.,1997-08-01
The User-Reported Critical Incident Method at a Glance,"The over-arching goal of this work is to discuss the user-reported critical incident method, a cost-effective remote usability evaluation method for real-world applications involving real users, doing real tasks in real work environments.  Several methods have been developed for conducting usability evaluation without direct observation of a user by an evaluator.  However, contrary to the user-reported critical incident method, none of the existing remote evaluation methods (nor even traditional laboratory-based evaluation) meets all the following criteria: - data are centered around critical incidents that occur during task performance; - tasks are performed by real users; - users are located in normal working environment; - users self-report own critical incidents; - data are captured in day-to-day task situations; - no direct interaction is needed between user and evaluator during an evaluation session; - there is a cost-effective way to capture data; and - data are high quality and therefore relatively easy to convert into usability problems.",1997-07-01
Effect of a Sawtooth Boundary on Couette Flow,The relative tagential motion of a smooth plate and a corrugated plate separated by a viscous fluid is studied.  The full Navier-Stokes equations are solved using Hermite collocation and Newton's method.  Detailed streamlines and vorticity distributions are determined.  The increased drag due to corrugation is found to be substantial.,1998
Proceedings of the RESOLVE Workshop 2002,Proceedings of the RESOLVE Workshop 2002,2002
Parameter Estimation for Mechanical Systems Using an Extended Kalman Filter,"This paper proposes a new computational approach based on the Extended Kalman Filter (EKF) in order to apply the polynomial chaos theory to the problem of parameter estimation, using direct stochastic collocation. The Kalman filter formula is used at each time step in order to update the polynomial chaos of the uncertain states and the uncertain parameters. The main advantage of this method is that the estimation comes in the form of a probability density function rather than a deterministic value, combined with the fact that simulations using polynomial chaos methods are much faster than Monte Carlo simulations. The proposed method is applied to a nonlinear four degree of freedom roll plane model of a vehicle, in which an uncertain mass with an uncertain position is added on the roll bar. A major drawback was identified: the EKF can diverge when using a high sampling frequency, which might prevent the use of enough data to obtain accurate results when a low sampling frequency is necessary. When applying the polynomial chaos theory to the EKF, numerical errors can accumulate even faster than in the general case due to the truncation in the polynomial chaos expansions, which is illustrated on a simple example. An alternative EKF approach which consists of applying the filter formula on all the observations at once usually yields better results, but can still sometimes fail to produce very accurate results. Therefore, using different sampling rates in order to verify the coherence of the results and comparing the results to a different approach is strongly recommended.",2008-09-01
On Extrapolated Multirate Methods,In this manuscript we construct extrapolated multirate discretization methods that allow to efficiently solve problems that have components with different dynamics. This approach is suited for the time integration of multiscale ordinary and partial differential equations and provides highly accurate discretizations. We analyze the linear stability properties of the multirate explicit and linearly implicit extrapolated methods. Numerical results with multiscale ODEs illustrate the theoretical findings.,2008-07-01
Adaptive Key Protection in Complex Cryptosystems with Attributes,"In the attribute-based encryption (ABE) model, attributes (as opposed to identities) are used to encrypt messages, and all the receivers with qualifying attributes can decrypt the ciphertext. However, compromised attribute keys may affect the communications of many users who share the same access control policies. We present the notion of forward-secure attribute-based encryption (fs-ABE) and give a concrete construction based on bilinear map and decisional bilinear Diffie-Hellman assumption. Forward security means that a compromised private key by an adversary at time t does not break the confidentiality of the communication that took place prior to t. We describe how to achieve both forward security and encryption with attributes, and formally prove our security against the adaptive chosen-ciphertext adversaries. Our scheme is non-trivial, and the key size only grows polynomially with logN (where N is the number of time periods). We further generalize our scheme to support the individualized key-updating schedule for each attribute, which provides a finer granularity for key management. Our insights on the required properties that an ABE scheme needs to possess in order to be forward-secure compatible are useful beyond the specific fs-ABE construction given. We raise an open question at the end of the paper on the escrow problem of the master key in ABE schemes.",2012
Query Composition: Why Does It Have to Be So Hard?,"Project Envision, a large research effort at Virginia Tech, focuses on developing a user centered multimedia database from the computer science literature with full-text searching and full-content retrieval capabilities.  User interviews indicate that people have trouble composing queries.  Widely available boolean retrieval systems present problems with both syntax and logic.  Natural language queries for vector space retrieval systems are easier to compose but users complain that they do not understand the matching principles used; users also complain that they have too little control over the search and fear being overwhelmed by an enormous retrieval set.  We describe the Envision query window which has as a usability goal making query composition easy while increasing user control.  Results of formative usability evaluation and subsequent redesign are discussed.",1993
Measurement of Software Maintainability and Reusability in the Object Oriented Paradigm,"The Metrics Group of Virginia Tech has been studying the object oriented paradigm and how it relates to software maintenance and software reusability in an attempt to make programmers more productive. Software reuse is the key to increased productivity within the software development process.  By reusing existing software, time and effort are saved in the testing and maintenance phases of a software product. The object oriented paradigm is designed to enhance software reusability through encapsulation and inheritance (CoxB86, Meye87).  This paper describes the results of three studies on object oriented metrics and also a reusability study currently in progress.  The first study investigates the maintainability of object oriented software versus procedural software.  The second one examines the relationship between programmer productivity, software reuse, and the object oriented paradigm.  The third study proposes and validates a new suite of object oriented metrics based on the MOOSE metrics developed by Chidamber and Kemerer (Chid91).  These new metrics are shown to predict maintainability from object oriented designs and source code.",1994-05-01
"Software Quality Measurement: Assessment, Prediction and Validation","Research addressing three challenging problems is described: (1) software quality assessment, (2) software quality prediction, and (3) the validation of both capabilities.  The three are the focus of an empirical investigation conducted by a university research team using a development project by a Navy contractor as a laboratory.  The predominantly Ada implementation total over 100,000 lines of source code, with approximately 40,000 included in this study.  Using a procedure that couples two fundamental concepts: the Objectives/Principles/Attributes framework with software quality indicators, this longitudinal research project represents the most comprehensive effort yet to validate an approach to assessing software quality.  Additionally, the more challenging problem of predicting software quality is tackled through an extraction and analysis of both process and product characteristics.  Automated tools applied to both programs and documentation are described in the context of their contributing roles in measurement.  Sources of difficulty in automating the assessment are identified.  The basis for statistical validation are described, and anticipated hypotheses are examined.",1994
Cell Cycle Modeling for Budding Yeast with Stochastic Simulation Algorithms,"For biochemical systems, where some chemical species are represented by small numbers of molecules, discrete and stochastic approaches are more appropriate than continuous and deterministic approaches. The continuous deterministic approach using ordinary differential equations is adequate for understanding the average behavior of cells, while the discrete stochastic approach accurately captures noisy events in the growth-division cycle. Since the emergence of the stochastic simulation algorithm (SSA) by Gillespie, alternative algorithms have been developed whose goal is to improve the computational efficiency of the SSA. This paper explains and empirically compares the performance of some of these SSA alternatives on a realistic model. The budding yeast cell cycle provides an excellent example of the need for modeling stochastic effects in mathematical modeling of biochemical reactions. This paper presents a stochastic approximation of the cell cycle for budding yeast using Gillespie’s stochastic simulation algorithm. To compare the stochastic results with the average behavior, the simulation must be run thousands of times. Many of the proposed techniques to accelerate the SSA are not effective on the budding yeast problem, because of the scale of the problem or because underlying assumptions are not satisfied. A load balancing algorithm improved overall performance on a parallel supercomputer.",2008-11-01
Efficient Computation of Voronoi Diagrams,It is generally agreed that some kind of separation is needed between the human-computer dialogue and the computational component of an interactive software system.  This paper addresses two major issues:,1988
Revisiting the Speed-versus-Sensitivity Tradeoff in Pairwise Sequence Search,"The Smith-Waterman algorithm is a dynamic programming method for determining optimal local alignments between nucleotide or protein sequences. However, it suffers from quadratic time and space complexity. As a result, many algorithmic and architectural enhancements have been proposed to solve this problem, but at the cost of reduced sensitivity in the algorithms or signiﬁcant expense in hardware, respectively. Hence, there exists a need to evaluate the tradeoffs between the different solutions. This motivation, coupled with the lack of an evaluation metric to quantify these tradeoffs leads us to formally deﬁne and quantify the sensitivity of homology search methods so that tradeoffs between sequence-search solutions can be evaluated in a quantitative manner. As an example, though the BLAST algorithm executes signiﬁcantly faster than Smith-Waterman, we ﬁnd that BLAST misses 80% of the signiﬁcant sequence alignments. This paper then presents a highly efﬁcient parallelization of the Smith-Waterman algorithm on the Cell Broadband Engine, a novel hybrid multicore architecture that drives the PlayStation 3 (PS3) game consoles, and emulates BLAST by repeatedly executing the parallelized Smith-Waterman algorithm to search for a query in a given sequence database. Through an innovative mapping of the optimal Smith-Waterman algorithm onto a cluster of PlayStation 3 nodes, our implementation delivers a 10-fold speed-up over a high-end multicore architecture and an 88-fold speed-up over a non-accelerated PS3.  Finally, we compare the performance of our implementation of the Smith-Waterman algorithm to that of BLAST and the canonical Smith-Waterman implementation, based on a combination of three factors — execution time (speed), sensitivity, and the actual cost of de-ploying each solution. In the end, our parallelized Smith-Waterman algorithm approaches the speed of BLAST while maintaining ideal sensitivity and achieving low cost through the use of PlayStation 3 game consoles.",2008
Impact of Network Sharing in Multi-core Architectures,"As commodity components continue to dominate the realm of high-end computing, two hardware trends have emerged as major contributors to this - high-speed networking technologies and multi-core architectures. Communication middleware such as the Message Passing Interface (MPI) use the network technology for communicating between processes that reside on different physical nodes while using shared memory for communicating between processes on different cores within the same node. Thus, two conflicting possibilities arise: (i) with the advent of multi-core architectures, the number of processes that reside on the same physical node and hence share the same physical network can potentially increase significantly resulting in {\em increased} network usage and (ii) given the increase in intra-node shared-memory communication for processes residing on the same node, the network usage can potentially {\em reduce} significantly. In this paper, we address these two conflicting possibilities and study the behavior of network usage in multi-core environments with sample scientific applications. Specifically, we analyze trends that result in increase or decrease of network usage and derive insights on application performance based on these. We also study the sharing of different resources in the system in multi-core environments and identify the contribution of the network in this mix. Finally, we study different process allocation strategies and analyze their impact on such network sharing.",2008-03-01
Measurement of ADA Throughout the Software Development Life Cycle,"Quality enhancement has now become a major factor in software production.  Software metrics have demonstrated their ability to predict source code complexity at design time and to predict maintainability of a software system from source code.  Obviously metrics can assist software developers in the enhancement of quality.  Tools which automatically generate metrics for Ada are increasing in popularity. This paper describes an existing tool which produces software metrics for Ada that may be used throughout the software development life cycle. This tool, while calculating established metrics, also calculates a new structure metric that is designed to capture communication interface complexity.  Measuring designs written using Ada as a PDL allows designers early feedback on possible problem areas in addition to giving direction on testing strategies.",1989
Learning as a Problem Solving Tool,"This paper explores the use of learning as a practical tool in problem  solving. The idea that learning should and eventually will be a vital component  of most Artificial Intelligence programs is pursued.  Current techniques in learning systems are compared. A detailed discussion  of the problems of representing, modifying, and creating heuristics is given.  Some of the questions asked (and answered) in the paper are: (1) how does the  choice of representation affect the potential for learning?, (2) what techniques  have been used to date and how do they compare?, i.e. first-order predicate  calculus vs. production rules vs. Winston's representation, and (3) exactly how  are heuristics modified in the existing systems and what do these techniques  have in common? A discussion of the credit assignment problem as it relates  to learning under the various schemes of representation is also presented.",1974
The Nonlinear Stability of a Heavy Rigid Plate Supported by Flexible Columns,"A heavy rigid platform is supported by thin elastic legs. The governing equations for large deformations are formulated and solved numerically by homotopy and quasi-Newton methods. Nonlinear phenomena such as non-uniqueness, catastrophe and hysteresis are found. A global critical load for nonlinear stability is introduced.",1992
Continuous Iterative Guided Spectral Class Rejection Classification Algorithm: Part 2,"This paper describes in detail the continuous iterative guided spectral class rejection (CIGSCR) classification method based on the iterative guided spectral class rejection (IGSCR) classification method for remotely sensed data. Both CIGSCR and IGSCR use semisupervised clustering to locate clusters that are associated with classes in a classification scheme. In CIGSCR and IGSCR, training data are used to evaluate the strength of the association between a particular cluster and a class, and a statistical hypothesis test is used to determine which clusters should be associated with a class and used for classification and which clusters should be rejected and possibly reﬁned. Experimental results indicate that the soft classification output by CIGSCR is reasonably accurate (when compared to IGSCR), and the fundamental algorithmic changes in CIGSCR (from IGSCR) result in CIGSCR being less sensitive to input parameters that inﬂuence iterations. Furthermore, evidence is presented that the semisupervised clustering in CIGSCR produces more accurate classifications than classification based on clustering without supervision.",2009
Note on the Effectiveness OF Stochastic Optimization Algorithms for Robust Design,"Robust design optimization (RDO) uses statistical decision theory and optimization techniques to optimize a design over a range of uncertainty (introduced by the manufacturing process and unintended uses). Since engineering ob jective functions tend to be costly to evaluate and prohibitively expensive to integrate (required within RDO), surrogates are introduced to allow the use of traditional optimization methods to ﬁnd solutions. This paper explores the suitability of radically diﬀerent (deterministic and stochastic) optimization methods to solve prototypical robust design problems. The algorithms include a genetic algorithm using a penalty function formulation, the simultaneous perturbation stochastic approximation (SPSA) method, and two gradient-based constrained nonlinear optimizers (method of feasible directions and sequential quadratic programming). The results show that the fully deterministic standard optimization algorithms are consistently more accurate, consistently more likely to terminate at feasible points, and consistently considerably less expensive than the fully nondeterministic algorithms.",2008-05-01
A Method for Assessing the Development Process of Small Organizations,This paper describes a two-phase method for assessing the s.oftware development process of small organizations. The initial phase involves using the Software Engineering Institute (SEI) assessment method to establish the activities comprising the development process. The second phase consists of follow-up interviews to discover the implementation technique for each existing activity and investigate the effectiveness of the techniques. Procedures for generating recommendations are outlined. The data acquired is statistically valid and allows discovery of significant problem areas. Recommendations based on the SEI framework and accepted software engineering practices are described. The assessment method is evaluated and future work is outlined.,1992
Studying Group Decision Making in Affinity Diagramming,"Affinity diagramming is a commonly used contextual design practice for which many tools have been developed.  However, experts and novices alike eschew tool use, instead using traditional paper and whiteboard methods.  This paper presents observations of traditional affinity diagramming sessions, focusing on three areas of consideration—shared awareness, cognitive offloading, and understanding, organizing and searching—that are important for collaborative tools. Specific design requirements for each of these three areas are described.",2008
Re-engineering with reuse: A case study,This paper describes a case study in reuse and reengineering. A C based metrics system was re-engineered to C++ using standard reusable components and a design pattern.,2007
Abstraction Mechanisms in Support of Top-Down and Bottom-Up Task Specification,"Abstraction is a powerful mechanism for describing objects and relationships from multiple, yet consistent, perspectives. When properly applied to interface design, abstraction mechanisms can provide the interaction flexibility and simplicity so desperately needed and demanded by today's diverse user community. Fundamental to achieving such goals has been the integration of visual programming techniques with a unique blend of abstraction mechanisms to support user interaction and task specification. The research presented in this paper describes crucial abstraction mechanisms employed within the Taskmaster environment to support top-down and bottom-up task specification. In particular, this paper (a) provides an overview of the Taskmaster environment, (b) describes top-down specification based on multi-level, menu-driven interaction and (c) describes bottom-up specification based on cutset identification and pseudo-tool concepts.",1988
Microcomputer Based Database Management Systems in Support of Office Automation,"The evolutionary advancements in microprocessor technology as it relates to database management systems (DBMSs) are dis¬cussed. Practice and experience with five commercially available database management systems are reported, based mostly on data gathered from a series of interviews focusing on comparison among systems.  Several prototype systems specifically designed to meet the needs of office information systems are identified, their conceptual framework ascertained and capabilities described. Finally, remarks on the limitations and future of microcomputer based DBMSs are made.",1983
Homotopy methods for constraint relaxation in unilevel reliability based design optimization,"Reliability based design optimization is a methodology for finding optimized designs  that are characterized with a low probability of failure. The main ob jective in reliability  based design optimization is to minimize a merit function while satisfying the reliability  constraints. The reliability constraints are constraints on the probability of failure corre-  sponding to each of the failure modes of the system or a single constraint on the system  probability of failure. The probability of failure is usually estimated by performing a relia-  bility analysis. During the last few years, a variety of different techniques have been devel-  oped for reliability based design optimization. Traditionally, these have been formulated  as a double-loop (nested) optimization problem. The upper level optimization loop gen-  erally involves optimizing a merit function sub ject to reliability constraints and the lower  level optimization loop(s) compute the probabilities of failure corresponding to the failure  mode(s) that govern the system failure. This formulation is, by nature, computationally  intensive. A new efficient unilevel formulation for reliability based design optimization was  developed by the authors in earlier studies. In this formulation, the lower level optimiza-  tion (evaluation of reliability constraints in the double loop formulation) was replaced by its corresponding first order Karush-Kuhn-Tucker (KKT) necessary optimality conditions  at the upper level optimization. It was shown that the unilevel formulation is computation-  ally equivalent to solving the original nested optimization if the lower level optimization is  solved by numerically satisfying the KKT conditions (which is typically the case), and the  two formulations are mathematically equivalent under constraint qualification and general-  ized convexity assumptions. In the unilevel formulation, the KKT conditions of the inner  optimization for each probabilistic constraint evaluation are imposed at the system level as  equality constraints. Most commercial optimizers are usually numerically unreliable when  applied to problems accompanied by many equality constraints. In this investigation an  optimization framework for reliability based design using the unilevel formulation is de-  veloped. Homotopy methods are used for constraint relaxation and to obtain a relaxed  feasible design. A series of optimization problems are solved as the relaxed optimization  problem is transformed via a homotopy to the original problem. A heuristic scheme is  employed in this paper to update the homotopy parameter. The proposed algorithm is  illustrated with example problems.",2007
"Visual Interactive Simulation: History, Recent Developments, and Major Issues","Visual Interactive Simulation (VIS) has dominated discrete-event simulation in the United Kingdom throughout the eighties. Conceived and initially implemented by Hurrion, who also coined the phrase, VIS first gained widespread exposure through the package SEE-WHY. The ideas behind VIS are fundamentally different from what is referred to in the United States as animation, since the prime motivater is user interaction with the running simulation, rather than just portrayal of the simulation. This paper presents a short history of VIS, and discusses some of the research and development that has been undertaken in the the United Kingdom and North America.  Following presentation of an example of VIS, the state of VIS is discussed, and a number of generally accepted guidelines for doing VIS are presented. a number of recent developments in VIS, many of them also relevant to animation, are discussed, and four major issues in the research and practice of VIS are presented.",1986-07-01
Object-Oriented Metrics Which Predict Maintainability,"Software metrics have been studied in the procedural paradigm as a quantitative means of assessing the software development process as well as the quality of software products.  Several studies have validated that various metrics are useful indicators of maintenance effort in the procedural paradigm.  However, software metrics have rarely been studied in the object oriented paradigm.  Very few metrics have been proposed to measure object oriented systems, and the proposed ones have not been validated.  This research concentrates on several object oriented software metrics and the validation of these metrics with maintenance effort in two commercial systems.  Statistical analyses of a prediction model incorporating ten metrics are performed.  In addition, a more compact model with fewer metrics was sought, analyses performed, and also presented.",1993
The Role of Automatic Digitizers in Computer Aided-design,The incorporation of facilities for automatically digitizing documents  can considerably enhance the power of computer graphics in computer-aided-design. A hardware-software process is described which rapidly and economically digitizes hard-copy drawings and translates them into computer data structures which faithfully represent both their geometry (including line widths) and their topology (line connectivities). These data structures may be directly interfaced to standard computer graphic manipulation software or they may serve as a source of information for analytic routines. The system thus allows drawings to serve as an immediate source of data for existing CAD processes. The result is that the tedium and inaccuracies associated with inputting graphic information has largely been removed so that the human may better concentrate his energies  in the tasks of design and analysis.,1974
Discrete Second Order Adjoints in Atmospheric Chemical Transport Modeling,"Atmospheric chemical transport models (CTMs) are essential tools for the study of air pollution, for environmental policy decisions, for the interpretation of observational data, and for producing air quality forecasts. Many air quality studies require sensitivity analyses, i.e., the computation of derivatives of the model output with respect to model parameters. The derivatives of a cost functional (defined on the model output) with respect to a large number of model parameters can be calculated efficiently through adjoint sensitivity analysis. While the traditional (first order) adjoint models give the gradient of the cost functional with respect to parameters, second order adjoint models give second derivative information in the form of products between the Hessian of the cost functional and a user defined vector.  In this paper we discuss the mathematical foundations of the discrete second order adjoint sensitivity method and present a complete set of computational tools for performing second order sensitivity studies in three-dimensional atmospheric CTMs. The tools include discrete second order adjoints of Runge Kutta and of Rosenbrock time stepping methods for stiff equations together with efficient implementation strategies. Numerical examples illustrate the use of these computational tools in important applications like sensitivity analysis, optimization, uncertainty quantification, and the calculation of directions of maximal error growth in three-dimensional atmospheric CTMs.",2007
Convergence analysis of hybrid cellular automata for topology optimization,"The hybrid cellular automaton (HCA) algorithm was inspired by the structural adaptation of bones to their ever changing mechanical environment. This methodology has been shown to be an eﬀective topology synthesis tool. In previous work, it has been observed that the convergence of the HCA methodology is aﬀected by parameters of the algorithm. As a result, questions have been raised regarding the conditions by which HCA converges to an optimal design. The objective of this investigation is to examine the conditions that guarantee convergence to a Karush-Kuhn-Tucker (KKT) point. In this paper, it is shown that the HCA algorithm is a ﬁxed point iterative scheme and the previously reported KKT optimality conditions are corrected. To demonstrate the convergence properties of the HCA algorithm, a simple cantilevered beam example is utilized. Plots of the spectral radius for projections of the design space are used to show regions of guaranteed convergence.",2009-03-01
A Hybrid Approach to Estimating Error Covariances in Variational Data Assimilation,"Data Assimilation (DA) involves the combination of observational data with the underlying dynamical principles governing the system under observation. In this work we combine the advantages of the two prominent advanced data assimilation systems, the 4D-Var and the ensemble methods. The proposed method consists of identifying the subspace spanned by the major 4D-Var error reduction directions. These directions are then removed from the background covariance through a Galerkin-type projection. This generates an updated error covariance information at both end points of an assimilation window. The error covariance information is updated between assimilation windows to capture the ``error of the day''. Numerical results using our new hybrid approach on a nonlinear model demonstrate how the background covariance matrix leads to an error covariance update that improves the 4D-Var DA results.",2009-03-01
Analysis of the Fitness Effect of Compensatory Mutations,"We extend our previous work on the ﬁtness effect of the ﬁxation of deleterious mutations on a population by incorporating the effect of compensatory mutations. Compensatory mutations are important in the sense that they make the deleterious mutations less deleterious, thus reducing the genetic load of the population. The essential phenomenon underlying compensatory mutations is the nonindependence of mutations in biological systems. Therefore, it is an important phenomenon that cannot be ignored when considering the ﬁxation and ﬁtness effect of deleterious mutations. Since having compensatory mutations essentially changes the distributional shapes of deleterious mutations, we can consider the effect of compensatory mutations by comparing two distributions where one distribution reﬂects the reduced ﬁtness effects of deleterious mutations with the inﬂuence of compensatory mutations. We compare different distributions of deleterious mutations without compensatory mutations to those with compensatory mutations, and study the effect of population sizes, the shape of the distribution, and the mutation rates of the population on the total ﬁtness reduction of the population.",2008
An O(N log N) Expected Time Merge Heuristic for the Planar ETSP,"We discuss a new heuristic for solving the Euclidean Traveling Salesman Problem (ETSP).  This heuristic is a convex hull-based method and makes use of the Delaunay triangulation of the set of cities to compute a tour for the given set of cities. We conjecture that the expected running time for this algorithm is O(N log N), implying that a new and faster ETSP heuristic is now available.",1988
Data Driven Surrogate Based Optimization in the Problem Solving Environment WBCSim,"Large scale, multidisciplinary, engineering designs are always difficult due to the complexity and dimensionality of these problems. Direct coupling between the analysis codes and the optimization routines can be prohibitively time consuming due to the complexity of the underlying simulation codes. One way of tackling this problem is by constructing computationally cheap(er) approximations of the expensive simulations, that mimic the behavior of the simulation model as closely as possible. This paper presents a data driven, surrogate based optimization algorithm that uses a trust region based sequential approximate optimization (SAO) framework and a statistical sampling approach based on design of experiment (DOE) arrays. The algorithm is implemented using techniques from two packages—SURFPACK and SHEPPACK that provide a collection of approximation algorithms to build the surrogates and three different DOE techniques—full factorial (FF), Latin hypercube sampling (LHS), and central composite design (CCD)—are used to train the surrogates. The results are compared with the optimization results obtained by directly coupling an optimizer with the simulation code. The biggest concern in using the SAO framework based on statistical sampling is the generation of the required database. As the number of design variables grows, the computational cost of generating the required database grows rapidly. A data driven approach is proposed to tackle this situation, where the trick is to run the expensive simulation if and only if a nearby data point does not exist in the cumulatively growing database. Over time the database matures and is enriched as more and more optimizations are performed. Results show that the proposed methodology dramatically reduces the total number of calls to the expensive simulation runs during the optimization process.",2009
Analysis of Function Component Complexity for Hypercube Homotopy Algorithms,"Probability-one homotopy algorithms are a class of methods for solving nonlinear systems of equations that globally convergent from an arbitrary starting point with probability one.  The essence of these homotopy algorithms is the construction of a homotopy map p-sub a and the subsequent tracking of a smooth curve y in the zero set p-sub a to the -1 (0) of p-sub a.  Tracking the zero curve y requires repeated evaluation of the map p-sub a, its n x (v + 1) Jacobian matrix Dp-sub a and numerical linear algebra for calculating the kernel of Dp-sub a. This paper analyzes parallel homotopy algorithms on a hypercube, considering the numerical algebra, several communications topologies and problem decomposition strategies, functions component complexity, problem size, and the effect of different component complexity distributions.  These parameters interact in complicated ways, but some general principles can be inferred based on empirical results.",1990
On the Benefits of a Maintainability Methodology,"A metrics methodology can dramatically reduce the problems associated with software maintenance.  However, several issues must be addressed in order to develop and use these techniques successfully.  This paper defines a metrics methodology which is designed to deliberately integrate maintainability into software as it is being developed.  The benefits of using this approach are discussed.  Then several issues which complicate the development and use of the methodology are examined.  Previous maintenance studies which incorporate the methodology into two different commercial environments are used to demonstrate the difficulties in implementation and contrast the differences in approach.",1990
Definability of Boolean Function Over Many-value Boolean Algebra,"In this paper, the definability of functions over B_s is first briefly  discussed. We then give necessary and sufficient conditions on the definable  functions over B_2, boolean algebra of four values. An efficient algorithm is  also presented for finding the defining boolean expressions for the definable  functions over B_2, The result is then extended to the functions over B_s.",1974
A Polynomial Chaos Based Bayesian Approach for Estimating Uncertain Parameters of Mechanical Systems – Part II: Applications to Vehicle Systems,"This is the second part of a two-part article. In the first part, a new computational approach for parameter estimation was proposed based on the application of the polynomial chaos theory. The maximum likelihood estimates are obtained by minimizing a cost function derived from the Bayesian theorem. In this part, the new parameter estimation method is illustrated on a nonlinear four-degree-of-freedom roll plane model of a vehicle in which an uncertain mass with an uncertain position is added on the roll bar. The value of the mass and its position are estimated from periodic observations of the displacements and velocities across the suspensions. Appropriate excitations are needed in order to obtain accurate results. For some excitations, different combinations of uncertain parameters lead to essentially the same time responses, and no estimation method can work without additional information. Regularization techniques can still yield most likely values among the possible combinations of uncertain parameters resulting in the same time responses than the ones observed. When using appropriate excitations, the results obtained with this approach are close to the actual values of the parameters. The accuracy of the estimations has been shown to be sensitive to the number of terms used in the polynomial expressions and to the number of collocation points, and thus it may become computationally expensive when a very high accuracy of the results is desired.  However, the noise level in the measurements affects the accuracy of the estimations as well. Therefore, it is usually not necessary to use a large number of terms in the polynomial expressions and a very large number of collocation points since the addition of extra precision eventually affects the results less than the effect of the measurement noise. Possible applications of this theory to the field of vehicle dynamics simulations include the estimation of mass, inertia properties, as well as other parameters of interest.",2007
Parallel Cost Analysis of Adaptive GMRES Implementations for Homotopy Methods,"The success of homotopy methods in solving large-scale optimization problems and nonlinear systems of equations depends heavily on the solution of large sparse nonsymmetric linear systems on parallel architectures.  Iterative solution techniques, such as GMRES(k), favor parallel implementations.  However, their straightforward parallelization usually leads to a poor parallel performance because of global communication incurred by processors.  One variation on GMRES(k) considered here is to adapt the restart value k for any given problem and use Householder reflections in the orthogonalization phase to achieve high accuracy and to reduce the communication overhead.  The Householder transformations can be performed without global communications and modified to utilize an arbitrary row distribution of the coefficient matrix.  The effect of this modification on the GMRES(k) performance is discussed here, as well as the abilities of parallel GMRES implementations using Householder reflections to maintain fixed efficiency with increase in problem size and number of processors. Theoretical communication cost and isoefficiency analyses are compared with experimental results on an Intel Paragon, Gray T3E, and IBM SP2.",1997-12-01
Semi-Supervised Learning of Hidden Markov Models via a Homotopy Method,"Hidden Markov model (HMM) classifier design is considered for analysis of sequential data, incorporating both labeled and unlabeled data for training; the balance between labeled and unlabeled data is controlled by an allocation parameter lambda in [0, 1), where lambda = 0 corresponds to purely supervised HMM learning (based only on the labeled data) and lambda = 1 corresponds to unsupervised HMM-based clustering (based only on the unlabeled data). The associated estimation problem can typically be reduced to solving a set of fixed point equations in the form of a “natural-parameter homotopy”. This paper applies a homotopy method to track a continuous path of solutions, starting from a local supervised solution (lambda = 0) to a local unsupervised solution (lambda = 1). The homotopy method is guaranteed to track with probability one from lambda = 0 to lambda = 1 if the lambda = 0 solution is unique; this condition is not satisfied for the HMM, since the maximum likelihood supervised solution (lambda = 0) is characterized by many local optimal solutions. A modified form of the homotopy map for HMMs assures a track from lambda = 0 to lambda = 1. Following this track leads to a formulation for selecting lambda in [0, 1) for a semi-supervised solution, and it also provides a tool for selection from among multiple (local optimal) supervised solutions. The results of applying the proposed method to measured and synthetic sequential data verify its robustness and feasibility compared to the conventional EM approach for semi-supervised HMM training.",2006
Knowledge Representation Issues in Default Reasoning,"Most existing approaches to reasoning in uncertainty and with incomplete information appeal to formal theories, with relatively little attention to the phenomena they are intended to capture.  This has had two major consequences.  First, it has led to the spurious disputes, in which participants criticize alternative approaches in the belief that they are competing, when in fact they are investigating different aspects of related phenomena, and should ultimately be viewed as cooperative efforts.  Second, it has led to wasted efforts of models which fail to reflect important aspects of kinds of reasoning which they are trying to capture, because the representational requirements have not been adequately spelled out.  This paper delineates several different kinds of reasoning in uncertainty, establishes some directions within the field, and attempts to begin setting some ground rules for representational adequacy.",1989
Efficient Computation of Voronoi Diagrams,"Voronoi diagrams have advantages of representing geometric information in a compact form which makes them well suited for robot path planning.  However, researchers have often found them too complex and inefficient.  In this paper, two alternate views of Voronoi diagrams, the Standard [SHAMM75a] and the generalized [KIRKD79] are reviewed.  A new method to approximate the Voronoi diagram is developed while showing how all three can be effectively used in applications. Finally, the three methods are compared showing their relative strengths and weaknesses for robot path planning.",1988
An Interactive Simulation Description Interpreter,"Inter_Sim, an interactive tool for developing and running discrete simulation models by interpreting a descriptive language, is presented. A running simulation, when halted, remains suspended.  Aftern extension, alteration, or investigation of the description, the simulation runs from the suspended state.  Thus the user views the simulation model as a running model, rather than a piece of static text.  The strengths and weaknesses of such an approach are discussed; other methods of interactive development are reviewed.",1986
Teaching Protection in Computing: A Research-oriented Graduate Course,"This paper describes a graduate course entitled ""Protection in  Computing"" given at Virginia Tech.  The course emphasizes selected  computer Science and research aspects of protection.  Following a  general course description, the various topics and reading references  are detailed. A chronological course outline indicates the sequence  of coverage and shows the correlation of reading references to the  topical areas. The use of oral presentations is described.",1983
On Implementing Graph Representations,"The three most common graph representations, namely the adjacency matrix, one way adjacency lists and adjacency multilist, are implemented in PASCAL and their performance evaluated for twelve graphs typical to computer network configurations.  We show that both adjacency multilist and one way adjacency lists are preferred over the adjacency matrix representation. Although their implementation is Slightly more complicated it out performs the latter by a factor of at least 5.",1983
Collaborative Storyboarding: Artifact-Driven Construction of Shared Understanding,"Collaborative storyboarding, with a focus on aggregating designers’ expertise in the storyboarding process, offers the opportunity for a group of designers to make progress toward creating a visual narrative for a new interface or technology, but it requires the designers to work together to explore ideas, differentiate between options, and construct a common solution. Important in collaborative storyboarding is the shared understanding that emerges among the designers and the obstacles they face in establishing that understanding. This paper defines a model for collaborative storyboarding, presents a study that explores group interactions in collaborative storyboarding, and analyzes the interactions using the distributed cognition and common ground theories. Our findings demonstrate that joint interaction and enthusiastic efforts within each phase lead to active information exchanges and shared understanding among the members of the group.",2009
Increasing the Precision of Distant Pointing for Large High-Resolution Displays,"Distant pointing at large displays allows rapid cursor movements, but can be problematic when high levels of precision are needed, due to natural hand tremor and track-ing jitter. We present two ray-casting-based interaction techniques for large high-resolution displays – Absolute and Relative Mapping (ARM) Ray-casting and Zooming for Enhanced Large Display Acuity (ZELDA) – that ad-dress this precision problem. ZELDA enhances precision by providing a zoom window, which increases target sizes resulting in greater precision and visual acuity. ARM Ray-casting increases user control over the cursor position by allowing the user to activate and deactivate relative map-ping as the need for precise manipulation arises. The results of an empirical study show that both approaches improve performance on high-precision tasks when compared to basic ray-casting. In realistic use, however, performance of the techniques is highly dependent on user strategy.",2008-09-01
Tracking Structural Optima as a Function of Available Resources by a Homotopy Method,"Optimization problems are typically solved by starting with an initial estimate and proceeding iteratively to improve it until the optimum is found. The design points along the path from the initial estimate to the optimum are usually of no value. The present work proposes a strategy for tracing a path of optimum solutions parameterized by the amount of available resources. The paper specifically treats the optimum design of a structure to maximize its buckling load. Equations for the optimum path are obtained using Lagrange multipliers, and solved by a homotopy method. The solution path has several branches due to changes in the active constraint set and transitions from unimodal to bimodal solutions. The Lagrange multipliers and second-order optimality conditions are used to detect branching points and to switch to the optimum solution path. The procedure is applied to the design of a foundation which supports a column for maximum buckling load. Using the total available foundation stiffness as a homotopy parameter, a set of optimum foundation designs is obtained.",1988
Achieving Very High Order for Implicit Explicit Time Stepping: Extrapolation Methods,"In this paper we construct extrapolated implicit-explicit time stepping methods that allow to efficiently solve problems with both stiff and non-stiff components. The proposed methods can provide very high order discretizations of ODEs, index-1 DAEs, and PDEs in the method of lines framework. These methods are simple to construct, easy to implement and parallelize. We establish the existence of perturbed asymptotic expansions of global errors, explain the convergence orders of these methods, and explore their linear stability properties. Numerical results with stiff ODEs, DAEs, and PDEs illustrate the theoretical findings and the potential of these methods to solve multiphysics multiscale problems.",2008-07-01
CampProf: A Visual Performance Analysis Tool for Memory Bound GPU Kernels,"Current GPU tools and performance models provide some common architectural insights that guide the programmers to write optimal code. We challenge these performance models, by modeling and analyzing a lesser known, but very severe performance pitfall, called 'Partition Camping', in NVIDIA GPUs.  Partition Camping is caused by memory accesses that are skewed towards a subset of the available memory partitions, which may degrade the performance of memory-bound CUDA kernels by up to seven-times. No existing tool can detect the partition camping effect in CUDA kernels.  We complement the existing tools by developing 'CampProf', a spreadsheet based, visual analysis tool, that detects the degree to which any memory-bound kernel suffers from partition camping. In addition, CampProf also predicts the kernel's performance at all execution configurations, if its performance parameters are known at any one of them. To demonstrate the utility of CampProf, we analyze three different applications using our tool, and demonstrate how it can be used to discover partition camping. We also demonstrate how CampProf can be used to monitor the performance improvements in the kernels, as the partition camping effect is being removed.  The performance model that drives CampProf was developed by applying multiple linear regression techniques over a set of specific micro-benchmarks that simulated the partition camping behavior. Our results show that the geometric mean of errors in our prediction model is within 12% of the actual execution times. In summary, CampProf is a new, accurate, and easy-to-use tool that can be used in conjunction with the existing tools to analyze and improve the overall performance of memory-bound CUDA kernels.",2010-10-01
Proceedings of the Second Annual Virginia Tech Center for Human-Computer Interaction Research Experience for Undergraduates (REU) Symposium,"Virginia Tech's Center for Human-Computer Interaction presents the project abstracts for the REU ’07 symposium. The REU (Research Experience for Undergraduates) program provides undergraduate students from various universities with the opportunity to spend eight weeks at Virginia Tech, working with our faculty and graduate students on research projects using the state-of-the-art technology and laboratories assembled here. The REU program is sponsored by a National Science Foundation grant IIS-0552732.",2008-07-01
Simulation Model Development: The Multidimensionality of the Computing Technology Pull,"Recent computing advances in artificial intelligence, computer graphics, human-computer inter-face, networking, parallel processing, and software engineering pull simulation model development in different directions. This paper discusses the impacts of significant advances in these areas on simulation model development, overviews how our Simulation Model Development Environment design is influenced by the recent advances, and speculates on the impacts of predicted future advances in these areas on simulation model development. The paper concludes that the computing advances in the six areas can be integrated under a simulation environment to provide an extremely powerful platform for simulation model development and execution.",1988-11-01
The Parallel Performance of Schwarz Splitting,"We describe several experiments with implementations of Schwarz splitting algorithms for the numerical solution of elliptic partial differential equations.  Results from experiments comparing several variations of the basic scheme in the context of real parallel implementations are presented.  Both shared-memory and distributed-memory architectures are represented.  Various possibilities for accelerating the Schwarz iteration and for solving the subdomain problems are considered, with particular emphasis on the resulting parallel performance.",1990
On the Minimal Total Path Length of a Spanning Tree,The notions of a balance node and the total path length with respect to a node u of a spanning tree are defined. We show that the total path length of a spanning tree with respect to u is minimal if and only if u is a balance node. An algorithm is also given to locate a balance node. A proof of the correctness of the algorithm is given and the complexity of the algorithm is analyzed.,1974
A Mathematical Programming Formulation for the Budding Yeast Cell Cycle,"The budding yeast cell cycle can be modeled by a set of ordinary differential equations with 143 rate constant parameters. The quality of the model (and an associated vector of parameter settings) is measured by comparing simulation results to the experimental data derived from observing the cell cycles of over 100 selected mutated forms. Unfortunately, determining whether the simulated phenotype matches experimental data is difficult since the experimental data tend to be qualitative in nature (i.e., whether the mutation is viable, or which development phase it died in). Because of this, previous methods for automatically comparing simulation results to experimental data used a discontinuous penalty function, which limits the range of techniques available for automated estimation of the differential equation parameters. This paper presents a system of smooth inequality constraints that will be satisfied if and only if the model matches the experimental data. Results are presented for evaluating the mutants with the two most frequent phenotypes. This nonlinear inequality formulation is the first step toward solving a large-scale feasibility problem to determine the ordinary differential equation model parameters.",2007
Development of a Collaborative Design Tool for Structural Analysis in an Immersive Virtual Environment,"This paper contains the results of an on-going collaborative research effort by the departments of Architecture and Computer Science of Virginia Polytechnic Institute and State University, U.S.A., to develop a computer visualization application for the structural analysis of building structures. The VIRTUAL-SAP computer program is being developed by linking PC-SAP4 (Structural Analysis Program), and virtual environment software developed using the SVE (Simple Virtual Environment) library. VIRTUAL-SAP is intended for use as a collaborative design tool to facilitate the interaction between the architect, engineer, and contractor by providing an environment that they can walk-through and observe the consequences of design alterations. Therefore, this software can be used as an interactive computer-aided analysis of building systems.",2001
Developing Human-Computer Interface Models and Representation Techniques(Dialogue Management as an Integral Part of Software Engineering),"The Dialogue Management Project at Virginia Tech is studying the poorly understood problem of human-computer dialogue development. This problem often leads to low usability in human-computer dialogues. The Dialogue Management Project approaches solutions to low usability in interfaces by addressing human-computer dialogue development as an integral and equal part of the total system development process. This project consists of two rather distinct, but dependent, parts. One is development of concepts for dialogue management, and the other is implementation of a dialogue management system (DMS) to evaluate these concepts. The goal of this paper is to describe our approach to the development of two of these conceptual aspects and how we oriented those toward the needs of practical implementation. The two conceptual aspects are   (a) a structural, descriptive model of human-computer interaction, and (b) Techniques for representing both the behavioral (end-user's) view and the constructional (developer's) view of dialogue.  The approach to their development was a technology transfer process that was part of a two-year university/industry research liaison between the Dialogue Management Project and IBM Federal Systems Division (FSD), now called Systems Integration Division. Part of this liaison was aimed at moving our research ideas and results into a real-world dialogue development environment. Following presentation of the technical problems and solutions, the paper concludes with a discussion of results of our liaison and by raising and addressing some questions of mutual interest that arose during our cooperative interaction.",1987
Content in the Context of 4D-Var Data Assimilation. II: Application to Global Ozone Assimilation,"Data assimilation obtains improved estimates of the state of a physical system by combining imperfect model results with sparse and noisy observations of reality. Not all observations used in data assimilation are equally valuable. The ability to characterize the usefulness of different data points is important for analyzing the effectiveness of the assimilation system, for data pruning, and for the design of future sensor systems. In the companion paper [Sandu et al.(2011)] we derived an ensemble-based computational procedure to estimate the information content of various observations in the context of 4D-Var. Here we apply this methodology to quantify two information metrics (the signal and degrees of freedom for signal) for satellite observations used in a global chemical data assimilation problem with the GEOS-Chem chemical transport model. The assimilation of a subset of data points characterized by the highest information content, gives analyses that are comparable in quality with the one obtained using the entire data set.",2011-11-01
GPU-based Streaming for Parallel Level of Detail on Massive Model Rendering,"Rendering massive 3D models in real-time has  long been recognized as a very challenging problem because of the limited computational power and memory space available in a workstation. Most existing rendering techniques, especially level of detail (LOD) processing, have suffered from their sequential execution natures, and does not scale well with the size of the models.  We present a GPU-based progressive mesh simplification approach which enables the interactive rendering of large 3D models with hundreds of millions of triangles. Our work contributes to the massive rendering research in two ways. First, we develop a novel data structure to represent the progressive LOD mesh, and design a parallel mesh simplification algorithm towards GPU architecture.  Second, we propose a GPU-based streaming approach which adopt a frame-to-frame coherence scheme in order to minimize the high communication cost between CPU and GPU. Our results show that the parallel mesh simplification algorithm and GPU-based streaming approach significantly improve the overall rendering performance.",2011
Multidisciplinary Design Optimization with Mixed Integer Quasiseparable Subsystems,"Numerous hierarchical and nonhierarchical decomposition strategies for the optimization of large scale systems, comprised of interacting subsystems, have been proposed. With a few exceptions, all of these strategies have proven theoretically unsound. Recent work considered a class of optimization problems, called quasiseparable, narrow enough for a rigorous decomposition theory, yet general enough to encompass many large scale engineering design problems. The subsystems for these problems involve local design variables and global system variables, but no variables from other subsystems. The objective function is a sum of a global system criterion and the subsystems' criteria. The essential idea is to give each subsystem a budget and global system variable values, and then ask the subsystems to independently maximize their constraint margins. Using these constraint margins, a system optimization then adjusts the values of the system variables and subsystem budgets. The subsystem margin problems are totally independent, always feasible, and could even be done asynchronously in a parallel computing context. An important detail is that the subsystem tasks, in practice, would be to construct response surface approximations to the constraint margin functions, and the system level optimization would use these margin surrogate functions. The present paper extends the quasiseparable necessary conditions for continuous variables to include discrete subsystem variables, although the continuous necessary and sufficient conditions do not extend to include integer variables.",2004
Globally Convergent Homotopy Methods for Large Scale Engineering Optimization,"Probability-one homotopy methods are a class of algorithms for solving nonlinear systems of equations that are accurate, robust, and converge from an arbitrary starting point almost surely. These new globally convergent homotopy techniques have been successfully applied to solve Brouwer fixed point problems, polynomial systems of equations, discretizations of non- linear two-point boundary value problems based on shooting, finite differences, collocation, and finite elements, and finite difference, collocation, and Galerkin approximations to nonlinear partial differential equations. This paper surveys the basic theory of globally convergent homotopy algorithms, describes some computer algorithms and mathematical software, applies homotopy theory to unconstrained and constrained optimization, and presents two realistic engineering applications (optimal design of composite laminated plates and fuel-optimal orbital satellite maneuvers ).",1989
Providing Reusability and Learning Support in the Simulation Model Development Environment,"The Premodels Manager, one of the tools of the Simulation Model Development Environment (SMDE), is required to enable a user to locate and reuse components of successfully completed simulation studies and learn from past experience. This paper presents the SMDE Premodels Manager and describes how it provides reusability and learning support. Objectives are set forth and a design is established and implemented on a Sun workstation. The SMDE Premodels Manager consists of four working windows, three access windows, and three support windows. It uses the SMDE Premodels Database which is a highly modifiable repository of documentation on successfully completed simulation studies. It is evaluated with respect to the design objectives and is shown to provide effective reusability and learning support within the SMDE.",1992
A Tutorial View Of Simulation Model Development,"Working from the background of simulation language developments, we develop an understanding of the current status of simulation model development. Factors characterizing the current status include a shift in emphasis from program to model, more commitment to modeling tools, and the lingering impedance of simulation language isolation. Current and future needs are identified. Specific approaches to meeting these needs are cited in an extensive description of current research, and in summary we conclude that the technology of simulation model deyelopment is in a transitional period that portends more rapid changes for the future.",1983
Dynamically Reconfigurable Networks: Concept Evaluation Through Simulation,The dynamically reconfigurable hierarchical network offers decided advantages in reliability and survivability. The concept is evaluated strictly in terms of message transmission delays for demand scenarios that vary widely. A SIMULA 67 model provides the experimental basis for obtaining preliminary results that show the reductions in transmission delays to be significant and to increase with the size of the network.,1988
A Lexical Relation Hierarchy,"An extensive literature now exists documenting various lexical relations for representing information about words. This report summarizes the lexical relations recognized in a variety of sources. In addition, we claim that lexical relations themselves form not a class but a taxonomy, with a rich hierarchical structure. We present the outlines of this taxonomy, organize relations identified in a number of works under the taxonomy, and then give a condensed report of over 100 relations derived from the compendium, organized by their hierarchical status.",1989
WBCSim: An Environment for Modeling Wood-Based Composites Manufacturing,"This paper describes a computing environment WBCSim that increases the productivity of wood scientists conducting research on wood-based composite materials and manufacturing processes. WBCSim integrates Fortran 90 simulation codes with a Web-based graphical front end, an optimization tool, and various visualization tools. WBCSim serves as an example for the design, construction, and evaluation of small-scale problem solving environments. WBCSim supports six models. A detailed description of the system architecture and a typical scenario of usage are presented, along with optimization and visualization features.",2002
A Distributed Genetic Algorithm with Migration for the Design of Composite Laminate Structures,"This paper describes the development of a general Fortran 90 framework for the solution of composite laminate design problems using a genetic algorithm (GA).  The initial Fortran 90 module and package of operators result in a standard genetic algorithm (sGA).  The sGA is extended to operate on a parallel processor, and a migration algorithm is introduced.  These extensions result in the distributed genetic algorithm with migration (dGA).  The performance of the dGA in terms of cost and reliability is studied and compared to a sGA baseline, using two types of composite laminate design problems.  The nondeterminism of GAs and the migration and dynamic load balancing algorithm used in this work result in a changed (diminished) workload, so conventional measures of parallelizability are not meaningful.  Thus, a set of experiments is devised to characterize the run time performance of the dGA.",1998-08-01
"Denserks: Fortran Sensitivity Solvers Using Continuous, Explicit Runge-kutta Schemes","DENSERKS is a Fortran sensitivity equation solver package designed for integrating models whose evolution can be described by ordinary differential equations (ODEs). A salient feature of DENSERKS is its support for both forward and adjoint sensitivity analyses, with built-in integrators for both first and second order continuous adjoint models. The software implements explicit Runge-Kutta methods with adaptive timestepping and high-order dense output schemes for the forward and the tangent linear model trajectory interpolation. Implementations of six Runge-Kutta methods are provided, with orders of accuracy ranging from two to eight. This makes DENSERKS suitable for a wide range of practical applications. The use of dense output, a novel approach in adjoint sensitivity analysis solvers, allows for a high-order cost-effective interpolation. This is a necessary feature when solving adjoints of nonlinear systems using highly accurate Runge-Kutta methods (order five and above). To minimize memory requirements and make long-time integrations computationally efficient, DENSERKS implements a two-level checkpointing mechanism. The code is tested on a selection of problems illustrating first and second order sensitivity analysis with respect to initial model conditions. The resulting derivative information is also used in a gradient-based optimization algorithm to minimize cost functionals dependent on a given set of model parameters.",2007-10-01
Continuous Iterative Guided Spectral Class Rejection Classiﬁcation Algorithm: Part 1,"This paper outlines the changes necessary to convert the iterative guided spectral class rejection (IGSCR) classification algorithm to a soft classification algorithm. IGSCR uses a hypothesis test to select clusters to use in classification and iteratively reﬁnes clusters not yet selected for classification. Both steps assume that cluster and class memberships are crisp (either zero or one). In order to make soft cluster and class assignments (between zero and one), a new hypothesis test and iterative reﬁnement technique are introduced that are suitable for soft clusters. The new hypothesis test, called the (class) association signiﬁcance test, is based on the normal distribution, and a proof is supplied to show that the assumption of normality is reasonable. Soft clusters are iteratively reﬁned by creating new clusters using information contained in a targeted soft cluster. Soft cluster evaluation and reﬁnement can then be combined to form a soft classification algorithm, continuous iterative guided spectral class rejection (CIGSCR).",2009
An Empirical Study of a Repeatable Method for Reengineering Procedural Software Systems to Object- Oriented Systems,"This paper describes a repeatable method for reengineering a procedural  system to an object-oriented system. The method uses coupling metrics to assist a domain  expert in identifying candidate objects. An application of the method to a simple program  is given, and the effectiveness of the various coupling metrics are discussed. We perform  a detailed comparison of our repeatable method with an ad hoc, manual reengineering  effort based on the same procedural program. The repeatable method was found to be  effective for identifying objects. It produced code that was much smaller, more efficient,  and passed more regression tests than the ad hoc method. Analysis of object-oriented  metrics indicated both simpler code and less variability among classes for the repeatable  method.",2009
Accelerating Electrostatic Surface Potential Calculation with Multiscale Approximation on Graphics Processing Units,"Tools that compute and visualize biomolecular electrostatic surface potential have been used extensively for studying biomolecular function. However, determining the surface potential for large biomolecules on a typical desktop computer can take days or longer using currently available tools and methods. This paper demonstrates how one can take advantage of graphic processing units (GPUs) available in today’s typical desktop computer, together with a multiscale approximation method, to significantly speedup such computations. Specifically, the electrostatic potential computation, using an analytical linearized Poisson Boltzmann (ALPB) method, is implemented on an ATI Radeon 4870 GPU in combination with the hierarchical charge partitioning (HCP) multiscale approximation. This implementation delivers a combined 1800-fold speedup for a 476,040 atom viral capsid.",2009
Using Interactive Maps in Community Applications,"Interactive maps provide unique ways to support community applications. In particular, they enable new collaborative activities. Map-based navigation supports a community environment as well as virtual tours. Interactive maps can also function as a tool in collecting historical information and discussing new spatial layouts. These examples indicate the numerous opportunities for interactive maps to support collaboration.",2001
A Specification Language To Assist In Analysis Of Discrete Event Simulation Models,"The use of effective development environments for discrete event simulation models should reduce development costs and improve model performance. A model specification language to be used in a model development environment is defined. This approach is intended to reduce modeli ng costs by interposing an intermediate form between a conceptual model (the model as it exists in the mind of the modeler) and an executable representation of that model. As a model specification is being constructed, the incomplete specification can be analyzed to detect some types of errors and to provide some types of model documentation. The primitives to be used in this specification language,  called a Condition Specification, are carefully defined. A specification for the classical patrolling repairman model is used as an example to illustrate this language. Some types of diagnostics which are possible based on such a representation are summarized, as well as some model specification properties which are untestable.",1983
Synthesis-Oriented Situational Analysis in User Interface Design,"Analytic evaluation is a term describing a class of techniques for examining a representation of a user interface design, discovering design flaws and/or predicting user task performance.  In our work with analytic evaluation, we have observed limitations on the effectiveness and efficiency of analytic techniques for formative evaluation supporting the iterative design and re-design cycle.  Here we support those observations with arguments based on theoretical limitations of the models underlying these techniques.  By way of comparison we discuss desirable characteristics for an alternative approach.  In our search for such an alternative, we have developed the Task Mapping Model, a substantively different approach to analysis for supporting the user interface design.  We briefly describe the Task Mapping Model and give some examples illustrating its desirable characteristics.",1993
A Note on Ledgard's Minilanguage 2 And a Proposal for an Alternative,"This note reviews and discusses the concepts of assignment statements  which occur within programming languages and as exemplified by Minilanguage 2 by Ledgard [3]. Observing that the description of assignment by  Ledgard depends on the existence of a nominative addressing scheme, an alternative vignette of language is proposed which is based on type directed addressing systems.",1974
Derandomized Vector Sorting,An instance of the vector sorting problem is a sequence of k-dimensional vectors of length n.  A solution to the problem is a permutation of the vectors such that in each dimension the length of the longest decreasing subsequence is O(sqrt(n)).  A random permutation solves the problem.  Here we derandomize the obvious probabilistic algorithm and obtain a deterministic O(kn^3.5) time algorithm that solves the vector sorting problem.  We also apply the algorithm to a book embedding problem.,1998-08-01
"Streams, Structures, Spaces, Scenarios, Societies (5S): A Formal Model for Digital Libraries","Digital libraries (DLs) are complex information systems and therefore demand formal foundations lest development efforts diverge and interoperability suffers. In this paper, we propose the fundamental abstractions of Streams, Structures, Spaces, Scenarios, and Societies (5S), which contribute to define digital libraries rigorously and usefully. Streams are sequences of abstract items used to describe static and dynamic content. Structures can be defined as labeled directed graphs, which impose organization. Spaces are sets of abstract items and operations on those sets that obey certain rules. Scenarios consist of sequences of events or actions that modify states of a computation in order to accomplish a functional requirement. Societies comprehend entities and the relationships between and among them. Together these abstractions relate and unify concepts, among others, of digital objects, metadata, collections, and services required to formalize and elucidate “digital libraries”. The applicability, versatility and unifying power of the theory is demonstrated through its use in three distinct applications: building and interpretation of a DL taxonomy, analysis of case studies of digital libraries, and utilization as a formal basis for a DL description language.",2001
Globally Optimized Parameters for a Model of Mitotic Control in Frog Egg Extracts,"DNA synthesis and nuclear division in the developing frog egg are controlled by fluctuations in the activity of M-phase promoting factor (MPF). The biochemical mechanism of MPF regulation is most easily studied in cytoplasmic extracts of frog eggs, for which careful experimental studies of the kinetics of phosphorylation and dephosphorylation of MPF and its regulators have been made. In 1998 Marlovits et al. used these data sets to estimate the kinetic rate constants in a mathematical model of the control system originally proposed by Novak and Tyson. In a recent publication, we showed that a gradient-based optimization algorithm finds a locally optimal parameter set quite close to the Marlovits estimates. In this paper, we combine global and local optimization strategies to show that the refined Marlovits parameter set, with one minor but significant modification to the Novak-Tyson equations, is the unique, best-fitting solution to the parameter estimation problem.",2004
Practical Minimal Perfect Hashing Functions for Large Databases,"We describe the first practical algorithms for finding minimal perfect hash functions that have been used to access very large databases (i.e., having over 1 million keys).  This method extends earlier work wherein an 0(n-cubed) algorithm was devised, building upon prior work by Sager that described an 0(n-to the fourth) algorithm.  Our first linear expected time algorithm makes use of three key insights: applying randomness whereever possible, ordering our search for hash functions based on the degree of the vertices in a graph that represents word dependencies, and viewing hash value assignment in terms of adding circular patterns of related words to a partially filled disk.  Our second algorithm builds functions that are slightly more complex, but does not build a word dependency graph and so approaches the theoretical lower bound on function specification size.  While ultimately applicable to a wide variety of data and file access needs, these algorithms have already proven useful in aiding our work in improving the performance of CD-ROM systems and our construction of a Large External Network Database (LEND) for semantic networks and hypertext/hypermedia collections. Virginia Disc One includes a demonstration of a minimal perfect hash function running on a PC to access a 130,198 word list on that CD-ROM. Several other microcomputer, minicomputer, and parallel processor versions and applications of our algorithm have also been developed. Tests including those wiht a French word list of 420,878 entries and a library catalog key set with over 3.8 million keys have shown that our methods work with very large databases.",1990
On the Convergence of a Class of Derivative-free Minimization Algorithms,"A convergence analysis is presenzed for a general class of derivative-free algorithms for minimizing a function f(x) whose analytic form of the gradient and the Hessian is impractical to obtain. The class of algorithms accepts finite difference approximation to the gradient with step-sizes chosen according to the following rule: if x, 2 are two successive iterate points and h, h are the corresponding step-size, then two conditions are required. The algorithms also maintain an approximation to the second derivative matrix and require the change in x made by each iteration is subject to a bound that is also revised automatically. The convergence theorems have the features that the starting point xl needs not be close to the true solution and f(x) needs not be convex. Furthermore, despite of the fact that the second derivative approximation may not converge to the true Hessian at the solution, the rate of convergence is still Q-superlinear. The theory is also shown to be applicable to a modification of Powell's dog-leg algorithm.",1975
The Role of Automatic Digitizers in Demography,"The incorporation of facilities for automatically digitizing maps and drawings can considerably enhance the power of the computer in demographical research. A hardware-software process is described which can rapidly, economically, and automatically digitize hard-copy maps and drawings. The resultant computer data structures are a faithful and accurate representation of both the original geometry and topology and may thus serve as a direct source of data for analysis or display. The potential uses for this system appears to be large for, though maps and drawings are of fundamental importance to demographers, their transduction into computers has historically been stymied by the problems associated with tedious, inaccurate, and incomplete manual methods.",1974
Local Area Networks and the Dynamic Hierarchy: A Tutorial,"The technology of the 197O's and 80's has introduced electronics equipment with capabilities that were not considered possible only a few short years ago. The impact has been widespread, providing innovations in business, professional, military, and government  applications with considerable political, economic, and social ramifications. With the rapid increase in capabilities has come a corresponding growth in the need to better manage these high-tech resources in order to realize their potential. A primary key to achieving this end is the cost effective, rapid, and reliable processing of information through networking.",1986-05-01
Integrating Metrics into a Large-Scale Software Development Environment,"Software metrics have been shown to be indicators of software complexity. These complexity indicators identify error-prone, unmaintainable software, which if identified early can be rewritten and/or thoroughly tested. However, the integration of metric use into commercial situations faces practical resistance from budget and deadline restrictions. This paper presents an experiment that introduces a nondisruptive method for integrating metrics into a large-scale commercial software development environment.",1989
Divided Difference Methods for Finite Fields,The Reed-Muller Decomposition Theorem is shown to be a special case  of a theorem of Newton. Divided difference methods are developed  for the general case of any finite field. The Newton Interpolation Theorem is proved for functions of one variable and stated for  functions of two variables. Empirical results are given for  some two place functions over GF(9) and GF(16).,1975
Development of an AEGIS Maintenance Methodology Task 2: Models of the AEGIS Maintenance Process,"Covering the Task 2 effort in the development of an AEGIS software maintenance methodology, the derivation of requirements from maintenance principles is summarized.  Three models of the maintenance process provide the basis for identifying points of application.  The mid-level (aggregate) model is the major focus of the methodology definition. Recommendations address the potential uses of different modeling perspectives, better balance between products and process assessment, use of metrics, and the selection of maintenance tools.",1990
Faculty Computer Literacy Project Final Report,"The emergence of computing as one of the fundamental disciplines threatens to overwhelm our future leaders unless some actions are taken immediately to provide a measure of literacy in the field to the student population now in the Universities of the country. A number of barriers to this exist; (1) there does not exist an accepted curriculum for computer literacy and literacy is often confused with programming language knowledge, (2) the faculty and equipment resources cannot be provided through computer science departments without enormous infusions of money and time, and (3) there do not exist qualified faculty in the ""PhD pipeline"" to staff such a program even if salaries could be made competitive with industry.",1984
A Methodology for Synthesis of Interface Design Requirements,This report is no longer available - TR-93-20 replaces it.,1993
Task-Oriented Representation of Asynchronous Interfaces,"A simple, task-oriented notation for describing user actions in asynchronous user interfaces is introduced. This User Action Notation (UAN) allows the easy association of actions with feedback and system state changes as part of a set of asynchronous interface design techniques, by avoiding the verbosity and potential vagueness of prose. Use within an actual design and implementation project showed the UAN to be expressive, concise, and highly readable because of its simplicity. The task-and user-oriented techniques are naturally asynchronous and a good match for object-oriented implementation. Levels of abstraction are readily applied to allow definition of primitive tasks for sharing and reusability and to allow hiding of details for chunking. The UAN provides a critical articulation point, bridging the gap between the task viewpoint of the behavioral domain and the event-driven nature of the object-oriented implementational domain. The potential for UAN task description analysis may address some of the difficulties in developing asynchronous interfaces.",1988-05-01
Modern Homotopy Methods in Optimization,"Probability-one homotopy methods are a class of algorithms for solving nonlinear systems of equations that are accurate, robust, and converge from an arbitrary starting point almost surely. These new techniques have been successfully applied to solve Brouwer faced point problems, polynomial systems of equations, and discretizations of nonlinear two-point boundary value problems based on shooting, finite differences, collocation, and finite elements. This paper summarizes the theory of globally convergent homotopy algorithms for unconstrained and constrained optimization, and gives some examples of actual application of homotopy techniques to engineering optimization problems.",1988
Spiraling Toward Usability: An Integrated Design Environment and Management System,"Decades of innovation in designing usable (and unusable) interfaces have resulted in a plethora of guidelines, usability engineering methods, and other design tools. However, novice developers often have difficulty selecting and utilizing theory-based design tools in a coherent design process. This work introduces an integrated design environment and knowledge management system, LINK-UP. The central design record (CDR) module, provides tools to enable a guided, coherent development process. The CDR aims to prevent breakdowns occurring between design and evaluation phases both within the development team and during design knowledge reuse processes. We report on results from three case studies illustrating novice designers use of LINK-UP.  A design knowledge IDE incorporating a CDR can help novice developers craft interfaces in a methodical fashion, while applying, verifying, and producing reusable design knowledge. Although LINK-UP supports a specific design domain, our IDE approach can transfer to other domains.",2005-09-01
Evaluation of a Location Linked Notes System,"We present a location-aware messaging system that lets users read and post notes tied to a particular location. We developed multiple clients (desktop, PDA and cell phone) so that users could choose the most contextually-appropriate device to interact with the system. We allowed remote access and authoring to avoid imposing artificial restrictions on users. We report on our initial evaluation of the system. The goal of the evaluation was to explore novel potential uses of the system and to identify users' preferences regarding the different system features. In our evaluation, we found that users were receptive of this system for leaving and receiving location-targeted reminders. They also overwhelmingly approved of the remote access and authoring capability, and suggested scenarios where these features would be crucial. We discuss our experiences building the system and our findings from the initial evaluation.",2004
Refinding is Not Finding Again,"A challenging problem for Internet users today is how to refind information that they have seen before. We believe that finding and refinding are different user activities and require different types of support. The problem of how to find information on the web is studied extensively---new search algorithms, support for natural language queries, and innovative document indexing techniques are common topics in information retrieval research; visualizations of documents, and task support for finding are topics in human-computer interaction. But refinding has only recently begun to receive attention. In this article, we present evidence to support the claim that information refinding is a different activity than information finding. We present results that show how refinding is different from finding and suggest ways to improve web information seeking tools and designs tosupport refinding information.",2005
General Purpose Visual Simulation System: A Functional Description,"The purpose of the research described herein is to develop a software system that aids a simulationist in constructing and executing a general purpose discrete event visual simulation model.  A literature review has shown the need for an integrated visual simulation system that provides for the graphical definition and interactive specification of the model while maintaining application independence.  The General Purpose Visual Simulation System (GPVSS) developed in this research meets this need by assisting a simulationist to: (1) graphically design a simulation model and its visualization, (2) interactively specify the model's logic, and (3) automatically generate the executable version of the model in C programming language, while maintaining domain independence.  GPVSS is developed on a Sun 3/160C computer workstation using SunView graphical interface.  It consists of over 11,000 lines of documented code.  GPVSS has been successfully treated in many case studies.",1989
"Analysis, Modeling and Optimization of Multiprocessing Execution Time","A new approach is presented for the analytical modeling of the execution times of a partitioned program running on parallel processors of a multiprocessor or distributed computed system.  The model represents the execution time of the individual processors as well as the aggregate system both in the deterministic and stochastic contexts. The analytical model encompasses a broader class of multiprocessing situations and formulates a mode accurate analytical representation of the execution times than has hitherto been presented in recent literature. The representation expresses the processor execution times in terms of the program module run times, the internal intermodule communication times, interprocessor (external) times and the number of modules assigned to each processor. A criterion is derived on the optimal assignment policy for minimizing execution time, or its statistical mean in the stochastic representation.",1989
Human-Computer Interface Development: Concepts and Systems for its Management,"Human-computer interface management, from a computer science viewpoint, focuses on the process of developing quality human computer interfaces, including their representation, design, implementation, execution, evaluation, and maintenance. This survey presents important concepts of interface management: dialogue independence, structural modeling, specification, rapid prototyping, holistic software engineering, control structures, and support environments, including User Interface Management System Tools.  Several systems for human-computer interface management are presented to illustrate these concepts.",1986
Interactive Digital Video Authoring and Prototyping,"This paper reports on a joint research project between Virginia Tech and the NCR Corporation on Digital Video Interaction (DVI) technology. In particular, it discusses:",1990
Performance Modeling and Analysis of a Massively Parallel DIRECT— Part 1,"Modeling and analysis techniques are used to investigate the performance of a massively parallel version of DIRECT, a global search algorithm widely used in multidisciplinary design optimization applications. Several highdimensional benchmark functions and real world problems are used to test the design effectiveness under various problem structures. Theoretical and experimental results are compared for two parallel clusters with different system scale and network connectivity. The present work aims at studying the performance sensitivity to important parameters for problem configurations, parallel schemes, and system settings. The performance metrics include the memory usage, load balancing, parallel efficiency, and scalability. An analytical bounding model is constructed to measure the load balancing performance under different schemes. Additionally, linear regression models are used to characterize two major overhead sources—interprocessor communication and processor idleness, and also applied to the isoefficiency functions in scalability analysis. For a variety of highdimensional problems and large scale systems, the massively parallel design has achieved reasonable performance. The results of the performance study provide guidance for efficient problem and scheme configuration. More importantly, the generalized design considerations and analysis techniques are beneficial for transforming many global search algorithms to become effective large scale parallel optimization tools.",2007
WBCSim: A Prototype Problem Solving Environment for Wood-Based Composites Simulations,"This paper describes a computing environment named WBCSim that is intended to  increase the productivity of wood scientists conducting research on wood-based  composite materials.  WBCSim integrates Fortran 77-based simulation codes with  a graphical front end, an optimization tool, and a visualization tool.  WBCSim  serves as a prototype for the design, construction, and evaluation of larger  scale problem solving (computing) environments.  Several different wood-based  composite material simulations are supported.  A detailed description of the  prototype's software architecture and a typical scenario of use are presented. The system converts output from the simulations to the Virtual Reality  Modeling Language (VRML) for visualizing simulation results.",1998-12-01
Unit Tangent Vector Computation for Homotopy Curve Tracking on aHypercube,"Probability-one homotopy methods are a class of methods for solving nonlinear systems of equations that are globally convergent from an arbitrary starting point.  The essence of all such algorithms is the construction of an appropriate homotopy map and subsequent tracking of some smooth curve in the zero set of the homotopy map.  Tracking a homotopy curve involves finding the unit tangent vectors at different points along the zero curve.  Because of the way a homotopy map is constructed, the unit tangent vector at each point in the zero curve of a homotopy map (symbols) is in the kernel of the Jacobian matrix (symbols).  Hence tracking the zero curve of a homotopy map involves finding the kernel of the Jacobian matrix (symbols).  The Jacobian matrix (symbols) is a n x (n + 1) matrix with full rank.  Since the accuracy of the unit tangent vector is very important, on orthogonal factorization instead of an LU factorization of the Jacobian matrix is computed.  Two related factorizations, namely QR and LQ factorization, are considered here.  This paper presents computational results showing the performance of several different parallel orthogonal factorization/triangular system solving algorithms on a hypercube. Since the purpose of this study is to find ways to parallelize homotopy algorithms, it is assumed that the matrices are small, dense, and have a special structure such as that of the Jacobian matrix of a homotopy map.",1989
A Real Time Robot Arm Collision Detection System,"A data structure and update algorithm are presented for a prototype real time collision detection safety system for a multi-robot environment.  The data structure is a variant of the octree, which serves as a spacial index. An octree recursively decomposes three dimensional space into eight equal cubic octans until each octant meets some decomposition criteria.  Our octree stores cylspheres (cylinders with spheres on each end) and rectangular solids as primitives (other primitives can easily be added as required).  These primitives make up the two seven-degrees-of-freedom robot arms and environment modeled by the system.  Octree nodes containing more than a predetermined number N of primitives are decomposed.  This rule keeps the octree small, as the entire environment for our application can be modeled using a few dozen primitives.  As robot arms move, the octree is updated to reflect their changed positions.  During most update cycles, any given primitive does not change which octree node it is in.  Thus, modification to the octree is rarely required.  Incidents in which one robot arm comes too close to another arm or an object are reported.  Cycle time for interpreting current joint angles, updating the octree, and detecting/reporting imminent collisions averages 30 milliseconds on an Intel 80386 processor running at 20 MHz.",1990
Implementing a Global Termination Condition and Collecting Output Measures in Parallel Simulation,"This paper investigates how to implement arbitrary global termination conditions and collect statistics in a parallel simulation.  The problem is first discussed using Chandy and Sherman's space-time framework. Then termination conditions are categorized, and termination algorithms are given for several categories.  The chief problem is that if one evaluates the termination condition asynchronously with respect to the simulation, when termination is detected the simulator has already modified old attribute values needed to compute output measures.  The major conclusion is that minor modification of time warp permits use any termination condition.  In contrast, conservative protocols permit limitied termination conditions unless they are modified to incorporate mechanisms present in optimistic protocols.",1990
A Markov Model of Certain Structured Programs,"The paper is concerned with modeling the run time behavior  of a certain class of programs. Each program, represented by its flowgraph, is built up from one-in/one-out constructs (after the manner of Dijkstra). The programs have neither transient states nor absorbing states. Each program has one state which possesses two cycles of relatively prime length. A program which possesses these properties is called regular. Such a program may he modeled by a finite Markov chain. It is shown that if a program is regular then its Markov model has a regular transition matrix, that is, the sequence of powers of the transition matrix converges to a matrix all of whose rows are identical.  The experimental validity of the method is discussed, as are the implications of the method for program design.",1976
Building a Lexicon from Machine-Readable Dictionaries for Improved Information Retrieval,"Information retrieval systems have a tremendous potential for contributing to research in virtually all areas.  To date, this potential has not been fully realized, largely because of problems with controlling retrieval. One way to viewing these problems is that retrieval systems use keywords as indices to retrieve texts, as opposed to understanding the words in requests.  We describe a project for creating a lexicon from machine-readable dictionaries, which information retrieval systems can use to go beyond present indexing methods, bringing the actual performance of such systems closer to their potential.",1990
A Pedagogical Tool: Calculating Order Of Convergence,In this report we give a technique for computing the order of con¬vergence of a method which avoids the complexity of theory of difference equations. The technique may be easily perfected by any student in an undergraduate introductory numerical analysis class.,1975
Parallel ELLPACK for Shared Memory Multiprocessors,"This paper describes a parallel version of ELLPACK for shared memory multiprocessors. ELLPACK is a system for numerically solving elliptic PDEs. It consists of a very high level language for defining PDE problems and selecting methods of solution, and a library of approximately fifty problem solving modules. Earlier work considered three discretization modules (five point star, hodie, and hermite collocation), two linear system solution modules (linpack spd band and jacobi cg), and a triple module (hodie fft) which includes both discretization and solution, all for rectangular domains and simple boundary conditions. Here we describe parallel versions of six additional modules (hermite collocation, hodie helmholtz, five point star, band ge, sor, symmetric sor cg) for general boundary conditions and domains, and discuss modifications to the ELLPACK preprocessor, the tool that translates an ELLPACK ""program"" into FORTRAN.",1992
Virtual Programming Instrument Extended Hewlett-packard [vpi/ehp],No abstract available.,1975
Theory of Globally Convergent Probability-one Homotopies for Non-linear Programming,"For many years globally convergent probability-one homotopy methods have been remarkably successful on difficult realistic engineering optimization problems,most of which were attacked by homotopy methods because other optimization algorithms failed or were ineffective. Convergence theory has been derived for a few particular problems,and considerable fixed point theory exists,but generally convergence theory for the homotopy maps used in practice for nonlinear constrained optimization has been lacking.This paper derives some probability-one homotopy convergence theorems for unconstrained and inequality constrained optimization,for linear and nonlinear inequality constraints,and with and without convexity.Some insight is provided into why the homotopies used in engineering practice are so successful,and why this success is more than dumb luck.By presenting the theory as variations on a prototype probability-one homotopy convergence theorem,the essence of such convergence theory is elucidated.",2000
Unification of Problem Solving Environment Implementation Layers with XML,"This paper describes how XML is used to unify the implementation layers of a problem solving environment WBCSim. WBCSim is a Web based simulation system designed to increase the productivity of wood scientists conducting research on wood-based composites manufacturing processes. WBCSim serves as a prototypical example for the design, construction, and evaluation of small-scale problem solving environments. WBCSim supports five process models. A XML datasheet is tailored for each model. The WBCSim interface layer, server scripts and database management system all use this XML datasheet to improve the usability and maintainability of the three layers--client, server, developer--comprising the WBCSim system. A detailed description of the WBCSim system architecture is presented along with a typical scenario of usage.",2005
Optimization and Blending of Composite Laminates Using Genetic Algorithms with Migration,"Optimization of aircraft stiffened composite panel structures often results in manufacturing incompatibilities between adjacent panels. Using genetic algorithms to optimize local panel stacking sequences allows panel populations of stacking sequences to evolve in parallel and send migrants to adjacent panels, so as to blend the local panel designs globally. The blending process is accomplished using the edit distance between individuals of a population and the set of migrants from adjacent panels. The objective function evaluating the fitness of designs is modified according to the severity of mismatches detected between neighboring populations. This lays the ground work for natural evolution to a blended global solution without leaving the paradigm of genetic algorithms.",2002
Algorithm XXX: SHEPPACK: Modiﬁed Shepard Algorithm for Interpolation of Scattered Multivariate Data,"Scattered data interpolation problems arise in many applications. Shepard’s method for constructing a global interpolant by blending local interpolants using local-support weight functions usually creates reasonable approximations. SHEPPACK is a Fortran 95 package containing ﬁve versions of the modified Shepard algorithm: quadratic (Fortran 95 translations of Algorithms 660, 661, and 798), cubic (Fortran 95 translation of Algorithm 791), and linear variations of the original Shepard algorithm. An option to the linear Shepard code is a statistically robust ﬁt, intended to be used when the data is known to contain outliers. SHEPPACK also includes a hybrid robust piecewise linear estimation algorithm RIPPLE (residual initiated polynomial-time piecewise linear estimation) intended for data from piecewise linear functions in arbitrary dimension m. The main goal of SHEPPACK is to provide users with a single consistent package containing most existing polynomial variations of Shepard’s algorithm. The algorithms target data of different dimensions. The linear Shepard algorithm, robust linear Shepard algorithm, and RIPPLE are the only algorithms in the package that are applicable to arbitrary dimensional data.",2009
"You've Got Mail! Calendar, Weather and More: Customizable Phone Access to Personal Information","We present a design and a prototype of a system that provides access to calendar, email, weather, and news information over a phone using a VoiceXML interface. The system provides quick access to personal information, while enabling but not requiring interactivity. As one major application, we envision the system being used while commuting to work. At home, users define their preferences regarding content and order of presentation using a website. On the road, the personalized audio feed plays like a radio news show. Instead of commercials, the user is reminded of today's meetings, deadlines and listens to the email inbox.",2004
A Comparison of Formal Definitions of ADA Tasking,"In this paper, various formalisms (the operational approach, the denotational approach, the axiomatic approach, Petri nets and temporal logic) are briefly introduced through their applications in formally defining Ada tasking, and their coverages and relative merits are then",1989
"AI, Science, and Intellectual Processes: Preliminary Remarks andArguments","This paper argues that trying to answer questions like the precise relationship of AI to existing disciplines (psychology, philosophy, linguistics, etc.) is both premature and potentially harmful to all concerned.  This is not to say that we cannot say anything useful on the underlying questions which plague those who question its status; but the answers that can currently be given will probably fail to satisfy critics and proponents alike.  This dissatisfaction--and indeed much of the debate--results from a view of science which takes as its model mature, developed sciences, and ignores facts about necessary phases in their development.  This paper also argues that the question whether AI is a science is usually standing surrogate for concerns that have nothing whatever to do with science, and which should be addressed on their own grounds.  The fundamental thesis here is that understanding the current status of AI, and so understanding the relationship between the various proposed approaches, requires adopting a more sophisticated approach to the status and development of intellectual disciplines, and that such an approach can contribute substantially to a broad area of current disputes, including most notably the ""traditionalist/connectionist"" controversy.  This paper is divided into four questions: (1) ""What is AI?""; (2) ""Is AI science?""; (3) ""What does all this say about AI, cognitive science and art forms?""  The discussion of the fourth question will deal with consequences concerning the relationship between ""traditional"" and connectionist AI.  I outline my positions on these four questions.",1990
Metaphors for the Nature of Human-Computer Interaction in anEmpowering Environment: Interaction Styles Influences the Manner ofHuman Accomplishment,"Over the past four decades, the computer's role in helping humans solve information problems has increased steadily.  New genres of computer applications have arisen, leading to new ways of using computers.  Of these, perhaps the most advanced are what we call empowering environments for problem solving with computers.  We propose that a closed-loop model of interaction provides a more natural, more accurate description of problem solving with computers.  Further we suggest that human accomplishment is strongly enhanced by engagement of the user in such environments, and a key to engagement is the power of an interface as an illusion, representing the real-world problem-solving situation directly to the user.  In turn, the illusion is supported by directness, which depends on appropriate and timely feedback closely coupled to the user actions as part of inter-referential input/output.",1990
Computability Theory: An Introduction for Students of Computer Science,"The primary goal of this book is to introduce the basic concepts of  effective computability and to prepare the reader for the study of formal  language theory, recursive function theory, and the theory of computational  complexity.",1975
General Interface Description of Websites using CLICK and UIML,"This paper explores the domain of programming paradigm for Multi-Platform User Interfaces using XML based languages. The main focus of this work is User Interface Markup Language (UIML), an XML based language for describing user interfaces in a platform-independent manner. We have explored the capabilities of UIML as an interface description language for describing interactive websites. We have selected an end-user web programming tool called CLICK, which also uses an XML based interface description for the websites created through it. We have analyzed both the representations and devised a conversion process from CLICK XML to UIML. We have found that UIML is expressive enough to represent applications built using CLICK. UIML provides various benefits over the interface description generated by CLICK especially that of facilitating the development of web based interfaces for multiple platforms through CLICK.",2004
Preconditioned Iterative Methods for Homotopy Curve Tracking,"Homotopy algorithms are a class of methods for solving systems of nonlinear equations that are globally convergent with probability one. All homotopy algorithms are based on the construction of an appropriate homotopy map and then the tracking of a curve in the zero set of this homotopy map.  The curve-tracking algorithms used here require the solution of a series of very special systems.  In particular, each (n + 1) x (n + 1) system is in general nonsymmetric but has a leading symmetric indefinite n x n submatrix (typical of large structural mechanics problems, for example). Furthermore, the last row of each system may by chosen (almost) arbitrarily. The authors seek to take advantage of these special properties.  The iterative methods studied here include Craig's variant of the conjugate gradient algorithm and the SYMMLQ algorithm for symmetric indefinite problems. The effectiveness of various preconditioning strategies in this context are also investigated, and several choices for the last row of the systems to be solved are explored.",1991-05-01
Lpl: A Generalized List Processing Language,"The paper describes LFL, a generalized list processing language. LFL allows the user to define multiple cell structures and cell sizes at runtime, thereby allowing nonhomogeneous list structures. The paper examines the problems associated with list tracing in systems allowing multiple cell-types. Complex list tracing during garbage collection in LFL is avoided: (1) by creating a doubly-linked super list of all allocated cells and (2) by using a reference count scheme. No marking phase is required for garbage collection.   The problem of developing insertion and deletion procedures for lists with cells having multiple types of pointer structures is discussed and LFL solutions are given. LPL statements can handle singly-linked, doubly-linked, left-right-linked, and some multi-linked pointer structures automatically.   The design philosophy and the data organization for LPL are discussed in detail. Examples of the definition of cell structures are given, and all of the LPL list manipulation and creation statements arc examined and discussed.",1973
A Knowledge-Based System for Composite Document Analysis and Retrieval: Design Issues in the CODER Project,"The CODER (COmposite Document Expert/Extended/Effective Retrieval) Project aims at applying a variety of methods developed in the realm of artificial intelligence to improve the performance of information retrieval systems. A prototype CODER system is being developed and will serve as a testbed for future research in this area.  Initial experimentation will take place on a collection of more than three years of issues of the AIList ARPANET Digest CODER is being developed in MU-Prolog and C++ as a collection of experts communicating through central blackboards using UNIX&tm pipes and the TCP/IP protocol. This distributed system can be divided up across several machines, to best utilize special display devices, storage facilities, and processors. There is a central spine, including document text and document knowledge representations, and a large lexicon being constructed from two machine readable, English dictionaries. An entry/analysis subsystem carries out detailed analysis of composite, documents, determining the structure and type of the whole and of each part. An access/retrieval subsystem has models of each user, can accomodate a variety of query languages, and supports browsing, searching, and immediate feedback.  Many issues must be dealt with in the design of such a system, including issues of knowledge representation, natural language processing, storage management and support environments. This paper gives background, describes related work, explains the design principles and architecture, and closes with future plans.",1986-03-01
Iso-energy-efficiency: An approach to power-constrained parallel computation,"Future large scale high performance supercomputer systems require high energy efficiency to achieve exaflops computational power and beyond. Despite the need to understand energy efficiency in high-performance systems, there are few techniques to evaluate energy efficiency at scale. In this paper, we propose a system-level iso-energy-efficiency model to analyze, evaluate and predict energy-performance of data intensive parallel applications with various execution patterns running on large scale power-aware clusters. Our analytical model can help users explore the effects of machine and application dependent characteristics on system energy efficiency and isolate efficient ways to scale system parameters (e.g. processor count, CPU power/frequency, workload size and network bandwidth) to balance energy use and performance. We derive our iso-energy-efficiency model and apply it to the NAS Parallel Benchmarks on two power-aware clusters. Our results indicate that the model accurately predicts total system energy consumption within 5% error on average for parallel applications with various execution and communication patterns. We demonstrate effective use of the model for various application contexts and in scalability decision-making.",2010
Requirements Gathering and Modeling of Domain-Specific Digital Libraries with the 5S Framework: An Archaeological Case Study with ETANA,"Requirements gathering and conceptual modeling are essential for the customization of digital libraries (DLs), to help attend the needs of target com-munities. In this paper, we show how to apply the 5S (Streams, Structures, Spaces, Scenarios, and Societies) formal framework to support both tasks. The intuitive nature of the framework allows for easy and systematic requirements analysis, while its formal nature ensures the precision and correctness required for semi-automatic DL generation. More specifically, we show how 5S can help us define a domain-specific DL metamodel in the field of archaeology. Finally, an archaeological DL case study (from the ETANA project) yields informal and formal descriptions of two DL models (instances of the metamodel).",2005
Accounts from a Claims Reuse Experience: Design of an Airline Fares Tracker,"Previous research efforts have led to the establishment of a repository of claims as reusable knowledge entities. Through the analysis, design, and prototyping of a notification system aimed at monitoring airfares across time, airlines, and location, this paper presents the various work-products resulting from a scenario-based design approach coupled with the Claims Reuse Library to support reuse-centric claims analysis. Finally, we share our experience and findings using the Claims Reuse Library as a core to knowledge transfer.",2007-12-01
A Representation and Algorithm for Exact Computation of Cascaded Polygon Intersections with Fixed Storage Requirements,"Given a collection of polygons (or a collection of sets of polygons) with vertex points specified to some fixed-point precision, a technique is presented for accurately representing the intersection points formed by elements in the collection.  In particular, we recognize that in 2D, the result of a series of intersections between polygons must be composed of line segments that are part of the line segments making up the original polygons. While our techniques greatly improve the accuracy of floating point representations for intersection points, we also provide algorithms based on rational arithmetic that compute these intersection points exactly.  The storage required to represent an intersection vertex is slightly greater than four times the number of bits in the input resolution. Furthermore, no intermediate quantity requires resolution slightly greater than four times the resolution resolution of input vertex values, so implementation on existing computers at practical resolution can easily be done.  Cascaded intersection operations do not require ever greater amounts of storage for vertex points, as would normally be required by a rational arithmetic approach.  We also prove that a similar approach is not possible for any reasonable set of rotations.",1991
Inheritance in Actor Based Concurrent Object-Oriented Languages,"Inheritance is a valuable mechanism which enhances reusability and maintainability of software. A language design based on the actor model of concurrent computation faces a serious problem arising from the interference between concurrency and inheritance. A similar problem also occurs in other concurrent object-oriented languages. In this paper, we describe problems found in existing concurrent object-oriented languages. We present a solution which is based on a concept called behavior abstraction.",1988
User-Behavior Based Detection of Infection Onset,"A major vector of computer infection is through exploiting software or design flaws in networked applications such as the browser. Malicious code can be fetched and executed on a victim’s machine without the user’s permission, as in drive-by download (DBD) attacks. In this paper, we describe a new tool called DeWare for detecting the onset of infection delivered through vulnerable applications. DeWare explores and enforces causal  relationships between computer-related human behaviors and system properties, such as file-system access and process execution. Our tool can be used to provide real time protection of a personal computer, as well as for diagnosing and evaluating untrusted websites for forensic purposes. Besides the concrete DBD detection solution, we also formally define causal relationships between user actions and system events on a host. Identifying and enforcing correct causal relationships have important applications in realizing advanced and secure operating systems. We perform extensive experimental evaluation, including a user study with 21 participants, thousands of legitimate websites (for testing false alarms), as well as 84 malicious websites in the wild. Our results show that DeWare is able to correctly distinguish legitimate download events from unauthorized system events with a low false positive rate (< 1%).",2010
A Frame-Based Language in Information Retrieval,"With the advent of the information society, many researchers are turning to artificial intelligence techniques to provide effective retrieval over large bodies of textual information.  Yet any AI system requires a formalism for encoding its knowledge about the objects of its knowledge, the world, and the intelligence that it is designed to manifest. In the CODER system, the mission of which is to provide an environment for experiments in applying AI to information retrieval, that formalism is provided by a single well defined factual representation language. Designed as a flexible tool for retrieval research, the CODER factual representation language is a hybrid AI language involving a system of strong types for attribute values, a frame system, and a system of Prolog-like relational structures.  Inheritance is enforced throughout, and the semantics of type subsumption and object matching formally defined. A collection of type and object managers called the knowledge administration complex implements this common language for storing knowledge and communicating it within the system. Of the three types of knowledge structures in the language, the frame facility has proven most useful in the retrieval domain. The factual representation language is implemented in Prolog as a set of predicates accessible to all system modules. Each level of knowledge representation (elementary primitives, frames, and relations) has a type manager; the frame and relation levels also have object managers.  Storage of complete knowledge objects (statements in the factual representation language) is supported by a system or external knowledge bases. One paper discusses the frame construct itself, the implementation of the knowledge administration complex and external knowledge bases. and the use of the construct in retrieval research. The paper closes with a discussion of the utility of the language in experiments.",1988
Task Mapping Model (TMM) Analysis Manual,"This guide presents the task mapping model (TMM), a snythesis/analysis methodology for aiding the interface specialist in designing interfaces with better usability.  Briefly, TMM describes necessary knowledge for user task completion, and analyzes if this knowledge is supported by the user or interface.  This framework consists of various abstract levels of task decomposition and description.  TMM focuses on the mappings users make among domains (during task performance) to provide designers with specific information about task structure and user knowledge requirements.  This model allows descriptions of tasks using hierarchical decomposition coupled with abstraction of user knowledge requirements.",1993
Large Deformations of Rotating Polygonal Space Structures,"The effects of rotation on space structures are very important nowadays.  Because the structures are large and thus relatively flexible, unacceptable stresses or deformations may occur due to centrifugal forces.  In a recent paper we considered the effects of rotation on an unbalanced circular ring.  The present note studies a related problem, i.e., the free rotation of polygonal frames.  These shapes are basic in structural design.",1990
"Extending the 5S Framework of Digital Libraries to support Complex Objects, Superimposed Information, and Content-Based Image Retrieval Services","Advanced services in digital libraries (DLs) have been developed and widely used to address the required capabilities of an assortment of systems as DLs expand into diverse application domains. These systems may require support for images (e.g., Content-Based  Image Retrieval), Complex (information) Objects, and use of content at fine grain (e.g., Superimposed Information). Due to the lack of consensus on precise theoretical definitions for those services, implementation efforts often involve ad hoc development, leading to duplication and interoperability problems. This article presents a methodology to address those problems by extending a precisely specified minimal digital library (in the 5S framework) with formal definitions of aforementioned services. The theoretical extensions of digital library functionality presented here are reinforced with practical case studies as well as scenarios for the individual and integrative use of services to balance theory and practice.  This methodology has implications that other advanced  services can be continuously integrated into our current extended framework whenever they are identified. The theoretical definitions and case study we present may impact future development efforts and a wide range of digital library researchers, designers, and developers.",2010
Algorithm XXX: VTDIRECT95: Serial and Parallel Codes for the Global Optimization Algorithm DIRECT,"VTDIRECT95 is a Fortran 95 implementation of D.R. Jones' deterministic global optimization algorithm called DIRECT, which is widely used in multidisciplinary engineering design, biological science, and physical science applications.  The package includes both a serial code and a data-distributed massively parallel code for different problem scales and optimization (exploration vs. exploitation) goals.  Dynamic data structures are used to organize local data, handle unpredictable memory requirements, reduce the memory usage, and share the data across multiple processors.  The parallel code employs a multilevel functional and data parallelism to boost concurrency and mitigate the data dependency, thus improving the load balancing and scalability.  In addition, checkpointing features are integrated into both versions to provide fault tolerance and hot restarts.  Important alogrithm modifications and design considerations are discussed regarding data structures, parallel schemes, error handling, and portability.  Using several benchmark functions and real-world applications, the software is evaluated on different systems in terms of optimization effectiveness, data structure efficency, parallel performance, and checkpointing overhead.  The package organization and usage are also described in detail.",2007
GLS-SOD: A Generalized Local Statistical Approach for Spatial Outlier Detection,"Local based approach is a major category of methods for spatial outlier detection (SOD). Currently, there is a lack of systematic analysis on the statistical properties of this framework. For example, most methods assume identical and independent normal distributions (i.i.d. normal) for the calculated local differences, but no justifications for this critical assumption have been presented. The methods’ detection performance on geostatistic data with linear or nonlinear trend is also not well studied. In addition, there is a lack of theoretical connections and empirical comparisons between local and global based SOD approaches. This paper discusses all these fundamental issues under the proposed generalized local statistical (GLS) framework. Furthermore, robust estimation and outlier detection methods are designed for the new GLS model. Extensive simulations demonstrated that the SOD method based on the GLS model significantly outperformed all existing approaches when the spatial data exhibits a linear or nonlinear trend.",2010-03-01
Magnetohydrodynamic Flow Between a Solid Rotating Disk and a Porous Stationary Disk,"In this paper we examine the flow of a conducting fluid between a solid rotating disk and a stationary porous disk with uniform section of fluid through the porous disk in the presence of a magnetic fiels. The equations of motion are solved using least change secant update quasi-Newton and modern root finding techniques. The fluid motion depends on the cross-flow Reynolds number, rotation Reynolds number and Hartmann number. The effects of the parameters on the flow field are presented graphically.",1987
A Topological Model of the Data Space for a Block Structured Language,"A space X is defined, the points of which are trees which possess a unique name property. A topology is defined for X based upon partial order. The topology is shown to be reasonably natural relative to the rationals. The topological space is shown to be neither Hausdorff nor T_1. The implications of this for program convergence are discussed with examples.",1973
Optimal and Random Partitions of Random Graphs,"The behavior of random graphs with respect to graph partitioning is considered.  Conditions are identified under which random graphs cannot be partitioned well, i.e., a random partition is likely to be almost as good as an optimal partition.",1993-07-01
Steady Viscous Flow in a Trapezoidal Cavity,The flow in a trapezoidal cavity (including the rectangular and triangular cavities) with one moving wall is studied numerically by finite differences with special treatment in the corners. It is found that streamlines and vorticity distributions are sensitive to geometric changes. The mean square law for core vorticity is valid for the rectangle but ceases to be valid for the triangular cavity.,1992
PSE Research at Virginia Tech: An Overview,"The purpose of this report is to give an overview of the activities of the research group in problem solving environments (PSEs) at Virginia Tech.  Most of the report is devoted to an introduction to the area itself, with particular emphasis on our perspective and goals.  After a brief attempt at defining the term PSE, we consider individually the three key words: problem:     what problem domains are we addressing? solving:     what characterizes the solution process? environment: what should the computational environment include? The final section summarize our current specific interests.  For more information about PSE research at Virginia Tech, please visit our home page at www.cs.vt.edu/~pse.",1998-08-01
Implementation of B-Trees Using Dynamic Address Computation,"The B-tree is probably the most popular method in use today for indexes and inverted files in database management systems.  The traditional implementation for a B-tree uses many pointers (more than one per key), and which can directly affect the performance of the B-tree.  A general method of file organization and access (called Dynamic Address Computation) has been described by Cook that can be used to implement B-trees using no pointers.  A minimal amount of storage (in addition to the keys) is required.  This paper gives a detailed description of Direct Address Computation and the resulting system is analyzed, leading to the conclusion that, while the approach results in a simple implementation of B-trees, more work is required to achieve performance for large B-trees.",1990
Working-Set-Like Memory Management Algorithms,"This paper considers the design and evaluation of memory management  algorithms to be applied to multiprogramming computer systems with virtual  memory. The operation of the Denning working set algorithm is studied and  is recognized to be a replacement process of time indices based on a rule  closely related to the replacement rule of the First-In-First-Out replacement  algorithm. Basing on these analyses, a framework in the time domain is then  proposed. A duality rule capable of transforming a replacement algorithm  in the space domain into a working-set-like algorithm (retention algorithm)  in the time domain is designed. Some properties of these newly-designed  retention algorithms are derived. The performances of some retention  algorithms with respect to their space duals are experimentally studied by  simulation. Results show generally better performance for retention algorithms  than for their space dual replacement algorithms.",1974
Model Analysis in a Model Development Environment,"The Model Analyzer is a utility that renders automated and semi-automated support of the model development process. A prototype of this tool, demonstrating the capability for diagnostic analysis of non-executable model representations, is described from both a user and designer perspective. Key concepts affecting design decisions are discussed in the context of an underlying theory of model representation an analysis.  The importance of world-view-independent model representation is stressed as a precursor to the early employment of model diagnosis and analysis. An example serves to illustrate the capability of the current prototype and the importance of the design concepts and the UNIX, utilities yacc and lex in the Analyzer development.",1987
Interactive Tools: Making UIMS Usable,"The earliest UIMS provided primarily run-time facilities for interface management and a set of programming tools for the development of application from the implementation requirements with which many tool designers have approached UIMS design, there are also methodological requirements that have been seriously neglected. One reason is that interface design methodology is poorly understood and rarely axiomatic.  Nevertheless, it is important that we formulate methodological theories and provide UIMS with tools that support them. This paper proposes a storyboard metaphor for the conceptual design of human-computer interfaces.",1986
A fully discrete framework for the adaptive solution of inverse problems,"We investigate and contrast the differences between the discretize-then-differentiate and differentiate-then-discretize approaches to the numerical solution of parameter estimation problems. The former approach is attractive in practice due to the use of automatic differentiation for the generation of the dual and optimality equations in the first-order KKT system. The latter strategy is more versatile, in that it allows one to formulate efficient mesh-independent algorithms over suitably chosen function spaces. However, it is significantly more difficult to implement, since automatic code generation is no longer an option. The starting point is a classical elliptic inverse problem. An a priori error analysis for the discrete optimality equation shows consistency and stability are not inherited automatically from the primal discretization. Similar to the concept of dual consistency, We introduce the concept of optimality consistency. However, the convergence properties can be restored through suitable consistent modifications of the target functional. Numerical tests confirm the theoretical convergence order for the optimal solution. We then derive a posteriori error estimates for the infinite dimensional optimal solution error, through a suitably chosen error functional. This estimates are constructed using second order derivative information for the target functional. For computational efficiency, the Hessian is replaced by a low order BFGS approximation. The efficiency of the error estimator is confirmed by a numerical experiment with multigrid optimization.",2012-12-01
An Input Normal Form Homotopy for the L2 Optimal Model Order Reduction Problem,"In control system analysis and design, finding a reduced order model, optimal in the L-squared sense, to a given system model is a fundamental problem.  The problem is very difficult without the global convergence of homotopy methods, and a homotopy based approach has been proposed. The issues are the number of degrees of freedom, the well posedness of the finite dimensional optimization problem, and the numerical robustness of the resulting homotopy algorithm.  A homotopy algorithm based on the input normal form characterization of the reduced order model is developed here and is compared with the homotopy algorithms based on Hyland and Bernstein's optimal projection equations.  The main conclusions are that the input normal form algorithm can be very efficient, but can also be very ill conditioned or even fail.",1993-06-01
LEND and Faster Algorithms for Constructing Minimal Perfect Hash Functions,"The Large External object-oriented Network Database (LEND) system has been developed to provide efficient access to large collections of primitive or multimedia objects, semantic networks, thesauri, hypertexts, and information retrieval collections. An overview of LEND is given, emphasizing aspects that yield efficient operation. In particular, a new algorithm is described for quickly finding minimal perfect hash functions whose specification space is very close to the theoretical lower bound, i.e., around 2 bits per key. The various stages of processing are detailed, along with analytical and empirical results, including timing for a set of over 3.8 million keys that was processed on a NeXTstation in about 6 hours.",1992
Comparison of Unix Communication Facilities Used in Linda,"This report presents the results of an investigative effort that focuses on a first step toward providing a distributed framework for Linda system processes. In particular, we discuss the restructuring of the kernel ""process"" to support Tuple Space access through UNIX socket calls, rather than through shared memory primitives based on semaphore usage. A description of the restructured system and the rationale for such restructuring is presented first. Most intriguing, however, are the latter sections that discuss the ramifications and insights gained from our particular approach to system redesign, i.e., the unnecessary serialization of Tuple Space access, redundant memory copies, being victimized by the UNIX scheduler.",1992
Edge Coloring Planar Graphs with Two Outerplanar Subgraphs,The standard problem of edge coloring a graph with k colors is equivalent to partitioning the edge set of the graph into k matchings. Here edge coloring is generalized by replacing matchings with outerplanar graphs.  We give a polynomial-time algorithm that edge colors any planar graph with two outerplanar subgraphs.  Two is clearly minimal for the class of planar graphs.,1991-12-01
Explicit Parallel Programming: System Description,"The implementation of the Explicit Parallel Programming (EPP) system is described.  EPP is a prototype implementation of a language for writing parallel programs for shared memory multiprocessors.  EPP may be viewed as a coordination language, since it is used to define the sequencing or ordering of various tasks, while the tasks themselves are defined in some other compilable language. The two main components of the EPP system---a compiler and an executive---are described in this report.  An appendix is included which contains the grammar defining the EPP language as well as templates used by the compiler in code generation.",1991-05-01
Proceedings of the Resolve Workshop 2006,"The aim of the RESOLVE Workshop 2006 was to bring together researchers and educators interested in: Refining formal approaches to software engineering, especially component-based systems, and introducing them into the classroom.  The workshop served as a forum for participants to present and discuss recent advances, trends, and concerns in these areas, as well as formulate a common understanding of emerging research issues and possible solution paths.",2006-04-01
A Framework to Analyze the Performance of Load Balancing Schemes for Ensembles of Stochastic Simulations,"Ensembles of simulations are employed to estimate the statistics of possible future states of a system, and are widely used in important applications such as climate change and biological modeling. Ensembles of runs can naturally be executed in parallel. However, when the CPU times of individual simulations vary considerably, a simple strategy of assigning an equal number of tasks per processor can lead to serious work imbalances and low parallel efficiency. This paper presents a new probabilistic framework to analyze the performance of dynamic load balancing algorithms for ensembles of simulations where many tasks are mapped onto each processor, and where the individual compute times vary considerably among tasks. Four load balancing strategies are discussed: most-dividing, all-redistribution, random-polling, and neighbor-redistribution. Simulation results with a stochastic budding yeast cell cycle model is consistent with the theoretical analysis. It is especially significant that there is a provable global decrease in load imbalance for the local rebalancing algorithms due to scalability concerns for the global rebalancing algorithms. The overall simulation time is reduced by up to 25%, and the total processor idle time by 85%.",2012
Performance Evaluation of Navigation Approaches on High-resolution Displays,"We conducted a study to discover if the data navigation techniques suitable for high-resolution displays differed significantly from those traditionally used for single-screen desktop displays. The high-resolution capability of the former display makes it possible to show more data at once without having the user drill-down to get to the details. At the same time, the larger physical size makes it difficult for the user to interact with such a display using current day interaction techniques. Given these factors, we compare the performance of users on tasks that involve navigating into hierarchically-structured data. The specific visualization we use is a cushion treemap, displayed at multiple resolutions—on a 3x3, 17” tiled screen display; on a 2x2, 17” tiled screen display; on a single 17” screen display, and on a 66” SMART Board™. Through the performance evaluation of 24 users, we show that beyond a certain resolution and physical screen size, the drill-down technique fares relatively poorly, while the straightforward technique of displaying all the data at once results in better performance at the tasks we studied.",2007-12-01
The Model Generator: A Crucial Element of the Model Development Environment,"This report documents development of a prototype model  generator. The model generator is designed to assist a discrete event  simulation  modeler  in  converting  this  conceptual  model  to  a  representation which can be communicated to other persons (the  communicative model).  The current version uses English phrases to represent the  communicative model. The model generator is one of the tools of the  Model Development Environment, which is a toolset being developed at  VPI & SU.  The Model Development Environment is envisioned as a  complete set of tools designed to provide integrated support to the  modeler throughout the mode! life cycle.  The governing concepts of the model generator are based on the  Conical Methodology of Nance. 	With that underpinning, the prototype  model generator provides a tool of moderately general applicability for  the construction of hierarchical models.  The report includes a brief review of applicable previous efforts,  design criteria for the model generator, a description of the model  generator computer program, and a user manual.",1984
Simulation Model Development: System Specification Techniques,A review of software specification techniques and specification languages is described.  Special emphasis given to simulation model specification and the degree to which general software techniques are applicable in the modeling domain. The role of specification is examined in terms of existing techniques as well as abstraction permitting the identification of desirable properties unrelated to existing tools. A categorization of specification languages assists in understanding the similarities and differences among current approaches.,1986
Micropolar Flow Past a Stretching Sheet,"This paper studies the flow of an incompressible, constant density  micropolar fluid past a stretching sheet. The governing boundary layer  equations of the flow are solved numerically using a globally convergent  homotopy method in conjunction with a least change secant update quasi-  Newton algorithm. The flow pattern depends on three non-dimensional  parameters. Some interesting results are illustrated graphically and discussed.",1984
Second order adjoints for solving PDE-constrained optimization problems,"Inverse problems are of utmost importance in many fields of science and engineering. In the variational approach inverse problems are formulated as PDE-constrained optimization problems, where the optimal estimate of the uncertain parameters is the minimizer of a certain cost functional subject to the constraints posed by the model equations. The numerical solution of such optimization problems requires the computation of derivatives of the model output with respect to model parameters. The first order derivatives of a cost functional (defined on the model output) with respect to a large number of model parameters can be calculated efficiently through first order adjoint sensitivity analysis. Second order adjoint models give second derivative information in the form of matrix-vector products between the Hessian of the cost functional and user defined vectors. Traditionally, the construction of second order derivatives for large scale models has been considered too costly. Consequently, data assimilation applications employ optimization algorithms that use only first order derivative information, like nonlinear conjugate gradients and quasi-Newton methods. In this paper we discuss the mathematical foundations of second order adjoint sensitivity analysis and show that it provides an efficient approach to obtain Hessian-vector products. We study the benefits of using of second order information in the numerical optimization process for data assimilation applications. The numerical studies are performed in a twin experiment setting with a two-dimensional shallow water model. Different scenarios are considered with different discretization approaches, observation sets, and noise levels. Optimization algorithms that employ second order derivatives are tested against widely used methods that require only first order derivatives. Conclusions are drawn regarding the potential benefits and the limitations of using high-order information in large scale data assimilation problems.",2010
A Study of the Performance Simulation of a File Server Configured Micro-LAN,"This report describes the development of a simulation-based performance evaluation techniques for micro-LAN systems.  The objective of the simulation is to predict network performance characteristics such as response time and CPU utilization, under varying traffic conditions and different LAN configurations.  A model is developed which reliably predicts file-server network performance.  The general applicability of this model provides LAN users with a powerful tool for predicting and analyzing system behavior.",1987-02-01
Genetic Algorithm with Memory for Optimal Design of Laminated Sandwich Composite Site Panels,"This paper is concerned with augmenting genetic algorithms (GAs) to include memory for continuous variables, and applying this to stacking sequence design of laminated sandwich composite panels that involves both discrete variables and a continuous design variable. The term “memory” implies preserving data from previously analyzed designs. A balanced binary tree with nodes corresponding to discrete designs renders efficient access to the memory. For those discrete designs that occur frequently, an evolving database of continuous variable values is used to construct a spline approximation to the fitness as a function of the single continuous variable. The approximation is then used to decide when to retrieve the fitness function value from the spline and when to do an exact analysis to add a new data point for the spline. With the spline approximation in place, it is also possible to use the best solution of the approximation as a local improvement during the optimization process. The demonstration problem chosen is the stacking sequence optimization of a sandwich plate with composite face sheets for weight minimization subject to strength and buckling constraints. Comparisons are made between the cases with and without the binary tree and spline interpolation added to a standard genetic algorithm. Reduced computational cost and increased performance index of a genetic algorithm with these changes are demonstrated.",2002
Concerning Transforms for Three-Valued Systems,No abstract available.,1977
Revision of TR-09-25: A Hybrid Variational/Ensemble Filter Approach to Data Assimilation,"Two families of methods are widely used in data assimilation: the four dimensional variational (4D-Var) approach, and the ensemble Kalman filter (EnKF) approach. The two families have been developed largely through parallel research efforts. Each method has its advantages and disadvantages. It is of interest to develop hybrid data assimilation algorithms that can combine the relative strengths of the two approaches. This paper proposes a subspace approach to investigate the theoretical equivalence between the suboptimal 4D-Var method (where only a small number of optimization iterations are performed) and the practical EnKF method (where only a small number of ensemble members are used) in a linear Gaussian setting. The analysis motivates a new hybrid algorithm: the optimization directions obtained from a short window 4D-Var run are used to construct the EnKF initial ensemble.  The proposed hybrid method is computationally less expensive than a full 4D-Var, as only short assimilation windows are considered. The hybrid method has the potential to perform better than the regular EnKF due to its look-ahead property. Numerical results show that the proposed hybrid ensemble filter method performs better than the regular EnKF method for both linear and nonlinear test problems.",2010-03-01
Exploring the Impact of Context Sensitivity on Blended Analysis,"This paper explores the use of context sensitivity both intra- and interprocedurally in a blended (static/dynamic) program analysis for performance diagnosis of framework-intensive Web-based applications.  Empirical experiments with an existing blended analysis algorithm [9] compare combinations of (i) use of a context-insensitive call graph with a context-sensitive calling context tree, and (ii) use (or not) of context-sensitive code pruning within methods.  These experiments demonstrate achievable gains in scalability and performance in terms of several metrics designed for blended escape analysis, and report results in terms of object instances created, to allow more realistic conclusions from the data than were possible previously.",2010-04-01
Development of a Modern OPAC: From REVTOLC to MARIAN,"In the Retrieval Experiment -- Virginia Tech OnLine Catalog (REVTOLC) study we carried out a large pilot test in 1987 and a larger, controlled investigation in 1990, with 216 users and roughly 500,000 MARC records. Results indicated that a forms-based interface coupled with vector and relevance feedback retrieval methods would be well received.  Recent efforts developing the Multiple Access and Retrieval of Information with ANnotations (MARIAN) system have involved use of a specially developed object-oriented DBMS, construction of a client running under NeXTSTEP, programming of a distributed server with a thread assigned to each user session to increase concurrency on a small network of NeXTs, refinement of algorithms to use objects and stopping rules for greater efficiency, usability testing and iterative interface refinement.",1993-02-01
Building Bridges and Interfaces: Toward the Next Generation of UIMS,"User interface management systems (UIMS) have established themselves in both research and commercial arenas. We present several generations in UIMS evolution and discuss some problems of the early generations. In particular, we discuss the problems of a gap between methods used by behavioral scientists and computer scientists during the process of building interfaces. We present an empirical approach to begin  bridging this gap and results of our preliminary observations: a human-computer interface development life cycle and recording techniques for interface development, as well as UIMS needed to support them.  We conclude with future directions for the evolution of UIMS.",1987
MOON: MapReduce on Opportunistic eNvironments,"MapReduce offers a ﬂexible programming model for processing and generating large data sets on dedicated resources, where only a small fraction of such resources are every unavailable at any given time. In contrast, when MapReduce is run on volunteer computing systems, which opportunistically harness idle desktop computers via frameworks like Condor, it results in poor performance due to the volatility of the resources, in particular, the high rate of node unavailability. Specifically, the data and task replication scheme adopted by existing MapReduce implementations is woefully inadequate for resources with high unavailability. To address this, we propose MOON, short for MapReduce On Opportunistic eNvironments. MOON extends Hadoop, an open-source implementation of MapReduce, with adaptive task and data scheduling algorithms in order to offer reliable MapReduce services on a hybrid resource architecture, where volunteer computing systems are supplemented by a small set of dedicated nodes. The adaptive task and data scheduling algorithms in MOON distinguish between (1) different types of MapReduce data and (2) different types of node outages in order to strategically place tasks and data on both volatile and dedicated nodes. Our tests demonstrate that MOON can deliver a 3-fold performance improvement to Hadoop in volatile, volunteer computing environments.",2009
From Landscapes to Waterscapes: A PSE for Landuse Change Analysis,"We describe the design and implementation of L2W — a problem solving environment (PSE) for landuse change analysis. L2W organizes and unifies the diverse collection of software typically associated with ecosystem models (hydrological, economic, and biological). It provides a web-based interface for potential watershed managers and other users to explore meaningful alternative land development and management scenarios and view their hydrological, ecological, and economic impacts. A prototype implementation for the Upper Roanoke River Watershed in Southwest Virginia, USA is described.",2000
A Human Motor Behavior Model for Direct Pointing at a Distance,"Models of human motor behavior are well known as an aid in the design of user interfaces (UIs). Most current models apply primarily to desktop interaction, but with the development of non-desktop UIs, new types of motor behaviors need to be modeled. Direct Pointing at a Distance is such a motor behavior. A model of direct pointing at a distance would be particularly useful in the comparison of different interaction techniques, because the performance of such techniques is highly dependent on user strategy, making controlled studies difficult to perform. Inspired by Fitts’ law, we studied four possible models and concluded that movement time for a direct pointing task is best described as a function of the angular amplitude of movement and the angular size of the target. Contrary to Fitts’ law, our model shows that the angular size has a much larger effect on movement time than the angular amplitude and that the growth in the difficulty of the tasks is quadratic, rather then linear. We estimated the model’s parameters experimentally with a correlation coefficient of 96%.",2008-12-01
An Introduction to 3D User Interface Design,"3D user interface design is a critical component of any virtual environment (VE) application. In this paper, we present a broad overview of three-dimensional (3D) interaction and user interfaces. We discuss the effect of common VE hardware devices on user interaction, as well as interaction techniques for generic 3D tasks and the use of traditional two-dimensional interaction styles in 3D environments. We divide most user interaction tasks into three categories: navigation, selection/manipulation, and system control. Throughout the paper, our focus is on presenting not only the available techniques, but also practical guidelines for 3D interaction design and widely held myths. Finally, we briefly discuss two approaches to 3D interaction design, and some example applications with complex 3D interaction requirements. We also present an annotated online bibliography as a reference companion to this article.",2001
A Proposal for a Minilanguage for the Basis of Discussion Regarding Data Structures,"This report generalizes the concepts of data structures which were presented in mini-language 8 by Ledgard [1], and provides a basis for the more general discussion of data structures.",1975
The Effectiveness of Cache Coherence Implemented on the Web,"The popularity of the World Wide Web (Web) has generated so much network traffic that it has increased concerns as to how the Internet will scale to meet future demand. The increased population of users and the large size of files being transmitted have resulted in concerns for different types of Internet users. Server administrators want a manageable load on their servers. Network administrators need to eliminate unnecessary traffic, thereby allowing more bandwidth for useful information. End users desire faster document retrieval. Proxy caches decrease the number of messages that enter the network by satisfying requests before they reach the server. However, the use of proxies introduces a concern with how to maintain consistency among cached document versions. Existing consistency protocols used in the Web are proving to be insufficient to meet the growing needs of the World Wide Web population. For example, too many messages are due to caches guessing when their copy is inconsistent. One option is to apply the cache coherence strategies already in use for many other distributed systems, such as parallel computers. However, these methods are not satisfactory for the World Wide Web due to its larger size and range of users. This paper provides insight into the characteristics of document popularity and how often these popular documents change. The frequency of proxy accesses to documents is also studied to test the feasibility of providing coherence at the server. The main goal is to determine whether server invalidation is the most effective protocol to use on the Web today. We make recommendations based on how frequently documents change and are accessed.",2000-08-01
Priority-enabled Scheduling for Resizable Parallel Applications,"In this paper, we illustrate the impact of dynamic resizability on parallel scheduling. Our ReSHAPE framework includes an application scheduler that supports dynamic resizing of parallel applications. We propose and evaluate new scheduling policies made possible by our ReSHAPE framework. The framework also provides a platform to experiment with more interesting and sophisticated scheduling policies and scenarios for resizable parallel applications. The proposed policies support scheduling of parallel applications with and without user assigned priorities. Experimental results show that these scheduling policies significantly improve individual application turn around time as well as overall cluster utilization.",2009
A Domain Decomposition Preconditioner for Hermite Collocation Problems,"We propose a preconditioning method for linear systems of equations arising from piecewise Hermite bicubic collocation applied to two dimensional elliptic PDEs with mixed boundary conditions. We construct an efficient, parallel preconditioner for the GMRES method. The main contribution of the paper is a novel interface preconditioner derived in the framework of substructuring and employing a local Hermite collocation discretization for the interface subproblems based on a hybrid fine-coarse mesh. Interface equations based on this mesh depend only weakly on unknowns associated with subdomains. The effectiveness of the proposed method is highlighted by numerical experiments that cover a variety of problems.",2002
An SMP Soft Classification Algorithm for Remote Sensing,"This work introduces a symmetric multiprocessing (SMP) version of the continuous iterative  guided spectral class rejection (CIGSCR) algorithm, a semiautomated classiﬁcation algorithm for remote  sensing (multispectral) images. The algorithm uses soft data clusters to produce a soft classiﬁcation  containing inherently more information than a comparable hard classiﬁcation at an increased computational  cost. Previous work suggests that similar algorithms achieve good parallel scalability, motivating the parallel  algorithm development work here. Experimental results of applying parallel CIGSCR to an image with  approximately 10^8 pixels and six bands demonstrate superlinear speedup. A soft two class classiﬁcation is  generated in just over four minutes using 32 processors.",2012
Toward a Formal Specification of Menu-based Systems,"As software systems continue to increase in sophistication and complexity, so do the interface requirements that support the corresponding user interaction. To select the proper ' ' nd of ingredients that constitutes an adequate user interface, it is essential that the system designer have a firm understanding of the interaction process, i.e, how the selected dialogue format interacts with the user and with the underlying task software. To promote such an understanding, this paper presents a model that characterizes one particular dialogue for menu-based interaction. This model is actually a sequence of models, hierarchically structured, where each successive model builds on its predecessor by introducing additional characterization elements. The first model describes the minimal set of elements inherent to any menu-based interface; successive models extend this minimal set by introducing task actions, incremental history sequences, and frame-associated memory.  These principal model elements enable the characterization of fundamental, menu-based operations like computational and decision processes, user response reversal, and user directed movement. Moreover, because the principal model elements correspond directly to real world objects, an intuitive as well as formal understanding of menu-based interaction can be achieved. Effectively, the model elements and the hierarchical structure imposed by these elements provide and ideal environment for characterizing and classifying menu-based systems at various levels of sophistication.",1986
Experimental Comparison of Schemes for Interpreting Boolean Queries,"The standard interpretation of the logical operators in a Boolean retrieval system is in general too strict. A standard Boolean query rarely comes close to retrieving all and only those documents which are relevant to the user. An AND query is often too narrow and an OR query is often too broad. The choice of the AND results in retrieving on the left end of a typical average recall-precision graph, while the choice of the OR results in retrieving on the right end, implying a tradeoff between precision and recall. This study basically examines various proposed schemes, the P-norm, Classical Fuzzy-Set, MMM, Paice and TIRS, which provide means to soften the interpretation of the logical operators, and thus to attain both high precision and high recall search performance.    Each of the above schemes has shown great improvement over the standard Boolean scheme in terms of retrieval effectiveness. The differences in retrieval effectiveness between P-norm, Paice and MMM are shown to be relatively small.  However, related performance results obtained gives evidence of the ranking: P-norm, Paice, MMM and then TIRS.",1988-05-01
Instructional Footprinting and Semantic Preservation in Linda,"Linda is a coordination language designed to support process creation and inter-process communication within conventional computational languages.  Although the Linda paradigm touts architectural and language independence, it often suffers performance penalties, particularly on local area network platforms.  Instructional Footprinting is an optimization technique with the primary goal of enhancing the execution speed of Linda programs.  The two main aspects of Instructional Footprinting are instructional decomposition and code motion.  This paper addresses the semantic issues encountered when the Linda primitive, IN and RD, are decomposed and moved past other Linda operations.  Formal semantics are given as well as results showing significant speedup (as high as 64%) when Instructional Footprinting is used.",1993
"Taskmaster: An Interactive, Graphical Environment for Task Specification, Execution and Monitoring","Taskmaster is an interactive environment that employs a unique blend of graphic technologies and iconic images to support user task specification. In this environment, problem solving is based on the selection, specification, and composition of tools that correspond to natural sets of ordered operations. The Taskmaster environment is novel in that it:",1989
The Family Window: Perceived Usage and Privacy Concerns,"Families have a strong need to connect with their loved ones over distance. However, most technologies do not provide the same feelings of connectedness that one feels from seeing remote family members. Hence our goal was to understand if a video connection, in the form of a media space, could help families feel more connected. To answer this, we designed a video media space called the Family Window and deployed it within the homes of two families for eight months and four families for five weeks. We also interviewed 16 individuals to obtain additional feedback about the system and to learn about their privacy concerns.",2010
"Space for Two to Think: Large, High-Resolution Displays for Co-located Collaborative Sensemaking","Large, high-resolution displays carry the potential to enhance single display groupware collaborative sensemaking for intelligence analysis tasks by providing space for common ground to develop, but it is up to the visual analytics tools to utilize this space effectively. In an exploratory study, we compared two tools (Jigsaw and a document viewer), which were adapted to support multiple input devices, to observe how the large display space was used in establishing and maintaining common ground during an intelligence analysis scenario using 50 textual documents. We discuss the spatial strategies employed by the pairs of participants, which were largely dependent on tool type (data-centric or function-centric), as well as how different visual analytics tools used collaboratively on large, high-resolution displays impact common ground in both process and solution. Using these findings, we suggest design considerations to enable future co-located collaborative sensemaking tools to take advantage of the benefits of collaborating on large, high-resolution displays.",2011-06-01
Simultaneous Optimal Uncertainty Apportionment and Robust Design Optimization of Systems Governed by Ordinary Differential Equations,"The inclusion of uncertainty in design is of paramount practical importance because all real-life systems are affected by it. Designs that ignore uncertainty often lead to poor robustness, suboptimal performance, and higher build costs. Treatment of small geometric uncertainty in the context of manufacturing tolerances is a well studied topic. Traditional sequential design methodologies have recently been replaced by concurrent optimal design methodologies where optimal system parameters are simultaneously determined along with optimally allocated tolerances; this allows to reduce manufacturing costs while increasing performance. However, the state of the art approaches remain limited in that they can only treat geometric related uncertainties restricted to be small in magnitude.   This work proposes a novel framework to perform robust design optimization concurrently with optimal uncertainty apportionment for dynamical systems governed by ordinary differential equations. The proposed framework considerably expands the capabilities of contemporary methods by enabling the treatment of both geometric and non-geometric uncertainties in a unified manner. Additionally, uncertainties are allowed to be large in magnitude and the governing constitutive relations may be highly nonlinear.   In the proposed framework, uncertainties are modeled using Generalized Polynomial Chaos and are solved quantitatively using a least-square collocation method. The computational efficiency of this approach allows statistical moments of the uncertain system to be explicitly included in the optimization-based design process. The framework formulates design problems as constrained multi-objective optimization problems, thus enabling the characterization of a Pareto optimal trade-off curve that is off-set from the traditional deterministic optimal trade-off curve. The Pareto off-set is shown to be a result of the additional statistical moment information formulated in the objective and constraint relations that account for the system uncertainties. Therefore, the Pareto trade-off curve from the new framework characterizes the entire family of systems within the probability space; consequently, designers are able to produce robust and optimally performing systems at an optimal manufacturing cost.     A kinematic tolerance analysis case-study is presented first to illustrate how the proposed methodology can be applied to treat geometric tolerances. A nonlinear vehicle suspension design problem, subject to parametric uncertainty, illustrates the capability of the new framework to produce an optimal design at an optimal manufacturing cost, accounting for the entire family of systems within the associated probability space. This case-study highlights the general nature of the new framework which is capable of optimally allocating uncertainties of multiple types and with large magnitudes in a single calculation.",2011
An Evaluation of Change-Based Coverage Criteria,"Various coverage criteria are commonly used to assess the quality of test suites, but achieving full coverage according to these criteria is often impossible or impractical.  Our research starts from the popular assumption that a disproportionate number of faults is likely to reside in recently changed code.  Based on this assumption, we propose several change-based coverage criteria that reflect to what extent changes with respect to a previous program version are exercised by a test suite.  In a set of experiments on programs from the SIR repository, we found change-based criteria to reveal faults better than traditional criteria, and to enable the construction of much smaller test suites with similar fault detection effectiveness.  We also report on a case study that shows that achieving (near) 100% coverage according to a change-based criterion is both feasible and useful.",2011-03-01
Deterministic Global Optimization of Flapping Wing Motion for Micro Air Vehicles,"The kinematics of a flapping plate is optimized by combining the unsteady vortex lattice method with a deterministic global optimization algorithm. The design parameters are the amplitudes, the mean values, the frequencies, and the phase angles of the flapping motion. The results suggest that imposing a delay between the different oscillatory motions and controlling the way through which the wing rotates at the end of each half stroke would enhance the lift generation. The use of a general unsteady numerical aerodynamic model (UVLM) and the implementation of a deterministic global optimization algorithm provide guidance and a baseline for future efforts to identify optimal stroke trajectories for micro air vehicles with higher fidelity models.",2010-12-01
The Use of Complexity Metrics Throughout the Software Lifecycle,"Software metrics attempt to uncover difficult or complex components of a software system. The hypothesis is that complex components are more difficult to understand, hence they are hard to maintain and more prone to error. Discovery of these complex components can aid the software developer in selecting which components to redesign, direct the testing effort, and indicate the maintenance effort required. Previous studies have demonstrated two main concepts: (1) there exists a high correlation between design complexity and source code complexity, and (2) metrics applied to source code have a high correlation to the maintenance activity needed. This previous research motivates us to develop a methodology which uses complexity metrics throughout the software life cycle. Programmer productivity may be increased and software development cost may be reduced if error prone software is discovered early in the life cycle.",1992-05-01
An Evaluation Of Cpu Efficiency Under Dynamic Quantum Allocation,A model for a time-sharing operating system is developed in order to assess the effects of dynamic quantum allocation and overhead variability on central processing unit (CPU) efficiency. CPU efficiency is determined by the proportion of time devoted to user-oriented (problem state) tasks within a busy period. Computational results indicate that a dynamic quantum allocation strategy produces significant differences in CPU efficiency compared to a constant quantum. The differences are affected significantly  by the variability among allocated quantum values and the demand on the system. Overhead variability also has a pronounced effect. A function that depicts overhead as decreasing with demand produces more stable values of CPU efficiency. The interaction between demand and the amount of overhead is observed to be significant.,1975
Sframe: An Efficient System for Detailed DC Simulation of Bipolar Analog Integrated Circuits Using Continuation Methods,In this paper we describe an experimental system called sframe which is being incorporated into the design for manufacturability initiative at the Reading Works of AT&T Bell Laboratories. Our system is able to perform detailed and accurate DC analyses of integrated circuits containing several hundred transistors to be fabricated in a relatively complex junction isolated complementary technology.,1992
A Taxonomy for the Evaluation of Computer Documentation,"The evaluation of software documentation is a key issue in the more general framework of evaluating products of a software development process. The research described in this report focuses on (1) a general taxonomy of document characteristics that support an assessment of documentation adequacy, and (2) the refining of the general evaluation taxonomy to a specific application, namely, the Automated Design Description System (ADDS).",1988
The Methodology Roles in the Realization of a Model Development Environment,"The definition of ""methodology"" is followed by a very brief review of past work in modeling methodologies. The dual role of a methodologies is explained: (1) conceptual guidance in the modeling task, and (2) definition of needs for environment designers.  A model development based on the conical methodology serves for specific illustration of both roles.",1988
Structural Design using Cellular Automata,"Traditional parallel methods for structural design do not scale well. This paper discusses the application of massively scalable cellular automata (CA) techniques to structural design. There are two sets of CA rules, one used to propagate stresses and strains, and one to perform design analysis. These rules can be applied serially,periodically,or concurrently, and Jacobi or Gauss- Seidel style updating can be done. These options are compared with respect to convergence,speed, and stability.",2001
Implementation of a Perfect Hash Function Scheme,"This report surveys the recent development in computing perfect hash functions, and in particular, closely examines an algorithm proposed by Thomas Sager.  An implementation of that algorithm in C has been done to demonstrate and verify the behavior of the algorithm for various settings of parameters.",1989-03-01
Time Dependent Rate-Based Modeling for Trace Data,This paper introduces a novel technique to construct an empirical model fitting time-varying (transient) trace data.  The trace is a categorical or numerical time-series.  The modeling technique described in this paper first builds a Mass Evolution Graph (MEG) from the trace data.  Linear regression is then used to construct a time-dependent probability mass function (pmf) for each state in the trace data.,1995-10-01
NetEdit: A Collaborative Editor,"We present a collaborative text editor named NetEdit. NetEdit uses a replicated architecture with processing and data distributed across all clients. Due to replication, the response time for local edits is quite close to that of a single-user editor. Clients do not need explicit awareness of other clients since all communication is coordinated by a central server. As a result, NetEdit is quite scalable (linear growth relative to purely distributed systems (quadratic growth) in terms of number of communication paths required as the number of clients grow. NetEdit uses an n-way synchronization algorithm derived from the synchronization protocol of the Jupiter collaboration system. Along with describing the editor, its architecture and its synchronization algorithm, we present the results of a usability study that evaluated the collaboration awareness tools included in NetEdit.",2001
TMM Lecture Materials,TMM (Task Mapping Model) is a user-centered task analysis technique that uses situational evaluation to synthesize new user interface design requirements.  This is a sample set of lecture materials for teaching TMM.,1994
Algorithms for Storytelling,"We formulate a new data mining problem called ""storytelling"" as a generalization of redescription mining. In traditional redescription mining, we are given a set of objects and a collection of subsets defined over these objects. The goal is to view the set system as a vocabulary and identify two expressions in this vocabulary that induce the same set of objects. Storytelling, on the other hand, aims to explicitly relate object sets that are disjoint (and hence, maximally dissimilar) by finding a chain of (approximate) redescriptions between the sets. This problem finds applications in bioinformatics, for instance, where the biologist is trying to relate a set of genes expressed in one experiment to another set, implicated in a different pathway. We outline an efficient storytelling implementation that embeds the CARTwheels redescription mining algorithm in an A* search procedure, using the former to supply next move operators on search branches to the latter. This approach is practical and effective for mining large datasets and, at the same time, exploits the structure of partitions imposed by the given vocabulary. Three application case studies are presented: a study of word overlaps in large English dictionaries, exploring connections between genesets in a bioinformatics dataset, and relating publications in the PubMed index of abstracts.",2006
A More Cost Effective Algorithm for Finding Perfect Hash Functions,"As the use of knowledge-based systems increases, there will be a growing need for efficient artificial intelligence systems and methods to access large lexicons. In the Composite Document Expert/extended/effective Retrieval (CODER) system we have, in order to provide rapid access to data items on CD-ROM's and to terms in a lexicon built from machine readable dictionaries, investigated the construction of perfect hashing functions. We have considered algorithms reported earlier in the literature, have made numerous enhancements to them, have developed new algorithms, and here report on some of our results. This paper covers an $O(n^3)$ algorithm that has been applied to building hashing functions for a collection of 69806 words on a CD-ROM. Most recently we have developed a much better algorithm and have succeeded in finding a perfect hash function for a set of 5000 words taken from the Collins English Dictionary.",1988-09-01
Specification Languages: Understanding Their Role in Simulation Model Development,"Current software specification techniques and specification languages are reviewed, emphasizing research activities in software specification languages. Alternate software life cycle models are described and compared to a simulation life cycle model. The importance of constructing a model specification before creating a programmed model is emphasized. Disadvantages in using simulation programming languages as model specification languages are discussed. The multiple uses which are made of a model specification are presented; these uses correspond to the alternate uses made of a requirements specification for general software. To evaluate where specification tools for general software will be effective for simulation modeling, both areas where the simulation life cycle corresponds to a general software life cycle and areas in which they differ are characterized. Important",1987
The Automatic Digitizer in Computer Graphics,An operational hardware-software system is described which rapidly and economically converts hard-copy drawings into graphic data structures faithfully representing both their geometry and their topology. These facilities permit computer graphic console time to be concentrated on the interactive manipulation and analysis of readily accessible graphic data. They can be used to accomplish a large portion of the tedious and error-prone manual tasks historically used for entering such data into computers. The result is that the power of existing computer graphics systems can be enhanced considerably and their domains of applicability be extended into areas not previously considered economically feasible.,1974
First Report on Epos: Aspects of Generality And Efficiency in Programming Language Implementation,No abstract available.,1974
Development and Evaluation of a Model of Behavioral Representation Techniques,"A user-centered approach to interactive system development requires a way to represent the behavior of a user interacting with an interface. While a number of behavioral representation techniques exist, not all provide the capabilities necessary to support the interaction development process.  Based on observations of existing representation techniques and comments from users of the User Action Notation (UAN), a user- and task-centered behavioral representation technique, we have developed a model of behavioral representation techniques.  Our model is an epistemological framework for discussing, analyzing, extending, and comparing existing behavioral representation techniques, as well as being a springboard for developing and evaluating new techniques.  We present the model and results of our evaluation demonstrating the model's reusability and utility within the context of behavioral representation techniques.",1993
NMFS: Network Multimedia File System Protocol,"We describe an on-going project to develop a Network Multimedia File System (NMFS) protocol. The protocol allows ""transparent access of shared files across networks"" as Sun's NFS protocol does, but attempts to meet a real-time delivery schedule. NMFS is designed to provide ubiquitous service over networks both designed and not designed to carry  multimedia traffic.",1992
Temporal Aspects of Tasks in the User Action Notation,"High software usability stems from good user interface design.  Good designs invariably depend on an ability to understand and evaluate (and thereby improve) interface designs in the development process. Understanding and evaluating designs depends, in part, on the methods used to represent the designs.",1990
Edge-Packing Planar Graphs by Cyclic Graphs,"Maximum G Edge-Packing is the problem of finding the maximum number of edge-disjoint isomorphic copies of a fixed guest graph G in a host graph H.  This paper considers the cases where G and H are planar and G is cyclic.  Recent work on the general problem is surveyed, inadequacies and limitations in these results are identified, and NP-completeness proofs for key cases are presented.",1996-11-01
A New Adaptive GMRES Algorithm for Achieving High Accuracy,"GMRES(k) is widely used for solving nonsymmetric linear systems.  However, it is inadequate either when it converges only for k close to the problem size or when numerical error in the modified Gram-Schmidt process used in the GMRES orthogonalization phase dramatically affects the algorithm performance.  An adaptive version of GMRES(k) which tunes the restart value k based on criteria estimating the GMRES conversion rate for the given problem is proposed here.  This adaptive GMRES(k) procedure outperforms standard GMRES(k), several other GMRES-like methods, and QMR on actual large scale sparse structural mechanics postbuckling and analog circuit simulation problems.  There are some applications, such as homotopy methods for high Reynolds number viscous flows, solid mechanics postbuckling analysis, and analog circuit simulation, where very high accuracy in the linear system solutions is essential.  In this context, the modified Gram-Schmidt process in GMRES can fail causing the entire GMRES iteration to fail.  It is shown that the adaptive GMRES(k) with the orthogonalization performed by Householder transformations succeeds whenever GMRES(k) with the orthogonolization performed by the modified Gram-Schmidt process fails, and the extra cost of computing Householder transformations is justified for these applications.",1996-05-01
Concurrent Object-Oriented Real-Time Systems Research,This research investigates building real-time systems with a concurrent object oriented programming language. There are close parallels between the characteristics of concurrent object oriented languages and real-time systems.  The language's underlying model of concurrency reflects the distributed and concurrent nature of real-time systems while the language's object orientation and the reusability properties of class inheritance address the embedded and evolutionary aspects of real-time systems. Automatic reclamation of concurrent objects frees programmers from the error-prone task of explicit resource management of dynamically created objects. Reclamation of concurrent objects provides an integrated management of both processing and memory resources.,1988
Noisy Aerodynamic Response and Smooth Approximations in HSCT Design,Convergence difficulties were encountered in our recent efforts towards a combined aerodynamic-structural optimization of the High Speed Civil Transport (HSCT).  The underlying causes of the convergence problems were traced to numerical noise in the calculation of aerodynamic drag components for obstacles to convergence.  The first technique employed a sequential approximation optimization method which used large initial move limits on the design variables.  This helped dislodge the optimizer out of the local minima in the design space created by the noisy drag data.  The second method utilized the aircraft.  Two techniques were developed to circumvent the response surface methods to construct smooth approximations to the noisy data. The response surfaces were formed by analyzing several individual HSCT configuration and then fitting polynomial functions to selected objective function data.  A simplified example design problem was used to demonstrate the response surface technique and to investigate various other issues relating to the construction of the response surfaces.,1994
A Coarse Grained Parallel Variable-Complexity Multidisciplinary Optimization Paradigm,"Modern aerospace vehicle design requires the interaction of multiple discipines, traditionally processed in a sequential order. Multidisciplinary optimization (MDO), a formal methodology for the integration of these disciplines, is evolving towards methods capable of replacing the traditional sequential methodology of aerospace vehicle design by concurrent algorithms, with both an overall gain in product performance and a decrease in design time.  A parallel MDO paradigm using variable-complexity modeling and multipoint response surface approximations is presented here for the particular instance of the design of a high speed civil transport (HSCT).  This paradigm interleaves the disciplines at one level of complexity, and processes them hierarchically at another level of complexity, achieving parallelism within disciplines, rather than across disciplines.  A master-slave paradigm manages a coarse grained parallelism of the analysis and optimization codes required by the disciplines showing reasonable speedups and efficiencies on an Intel Paragon.",1995-10-01
Optimization in Aircraft Design,"A parallel variable-complexity modeling approach, permitting the efficient use of emerging parallel computing in multidisciplinary optimization (MDO) technology, is presented for the particular instance of High Speed Civil Transport (HSCT) design.  In this method simple analyses are used to limit the approximation domain based on the D-optimality criterion through the use of a genetic alogorithm.  At the D-optimal points, a refined analysis is performed.  The optimization code is composed of a sequence of analysis cycles between aerodynamic and structural calculations.  Results for coarse grained parallelization of the aerodynamic and structural codes on an Intel Paragon are presented for an example HSCT design problem involving only two variables.  The full HSCT design problem employs twenty-eight design variables.",1995
The Conical Methodology: A Framework for Simulation Model Development,"The Conical Methodology, intended for large discrete event simulation modeling, is reviewed from two perspectives. The designer perspectivebegins with the question: What is a methodology?  From an answer to that question is framed an inquiry based on an objective/principles/attributes linkage that has proved useful in evaluating software development methodologies.  The user perspective addresses the role of a methodology vis a vis the software utilities (the tools) that comprise the environment.  Principles of a methodology form the needs analysis by which the requirements for tool design can be derived.  A comparison with software development methodologies and some applications for the Conical Methodologies comprise the concluding summary.",1987
Free Rotation of a Circular Ring with an Unbalanced Mass,"Large space structures are much more flexible than their terrestrial counterparts. Rotation of large space structures may be desirable for stability, thermal or artificial gravity reasons. Previous literature includes the large deformations due to the free rotation of a slender rod and a ring about a diameter.    An important model of a space station is a ring rotating about its axis of symmetry. If the ring is balanced it will be stable and remain circular. The present note considers the case when the ring is unbalanced by a mass attached to a point on the ring.",1988-05-01
Trends in Programming Languages Standards,"This overview of standards activities covers programming languages per se, and omits intentionally the areas of addenda functions (such as data base management systems (DBMS) and graphics (GKS)), since these would better be the subject of a separate report. This survey, however, does address issues related to the binding of these functional facilities to the programming languages, and the interfaces between them.",1984
A Model Based on Software Quality Factors which Predicts Maintainability,"Computer scientists are continually attempting to improve software system development.  Systems are developed in a top-down fashion for better modularity and understandability.  Performance enhancements are implemented for more speed. One area in which a great deal of effort is being devoted is software maintenance. Brooks estimates that fifty percent of the development cost of a software system is for maintenance activities [BROF82]. Since a large portion of the effort of a system is devoted to maintenance, it is reasonable to assume that driving down maintenance costs would drive down the overall cost of the system. Measuring the complexity of a software system could aid in this attempt. By lowering the complexity of the system or of subsystems within the system, it may be possible to reduce the amount of maintenance necessary. Software quality metrics were developed to measure the complexity of software systems. This study relates the complexity of the system as measured by software metrics to the amount of maintenance necessary to that system. We have developed a model which uses several software quality metrics as parameters to predict maintenance activity.",1988-05-01
Symbol Manipulation in an Interactive Environment,"In an interactive environment the opportunity exists for the on-line execution of an algorithm represented as a sequence of ordered commands. In particular, when the sequence of commands defines a normal Markov algorithm, the on-line environment provides a practical testing ground for one of the basic theories of computation. In order to develop and  run a Markov algorithm a specification is required which will be capable of execution by a computer. One possibility is to represent the Markov algorithm and its data as a sequence of LISP S-expressions and to process the S-expressions using an extended LISP interpreter. In this paper the translation of Markov algorithms to LISP S-expressionsis discussed along with a minimum set of commands for a Markov processor. Some of the difficulties in constructing more complicated algorithms are also discussed and several enhancements are suggested which would make the resultant Markov processor more practical and easier to use.",1978
Map-Based Navigation in a Graphical MOO,"Traditional MUDs and MOOs lack support for global wareness and simple navigation. These problems can be addressed by the introduction of a map-based navigation tool. In this paper we report on the design and evaluation of such a tool for MOOsburg, a graphical 2D MOO based on the town of Blacksburg, Virginia. The tool supports exploration and place-based tasks in the MOO. It also allows navigation of a large-scale map and encourages users to develop survey knowledge of the town. An evaluation revealed some initial usability problems with our prototype and suggested new design ideas that may better support users. Using these results, the lessons learned about map-based navigation are presented.",2001
A Technique for Hiding Proprietary Details While Providing Sufficient Information for Researchers or Do You Recognize This Well-Known Algorithm?,"A major problem facing software engineering researchers is the difficulty of performing validation experiments requiring ""real-world"" data. This data may be composed of the requirements document, design descriptions, source code,test plans, and/or error histories. Most major software organizations object to giving this usually proprietary data to researchers. This objection is understandable since these organizations do not desire their proprietary source code or error history to be made public. This paper describes a technique to translate source code into a meta-language which will maintain the proprietary nature of the original source code by hiding the algorithms and data structures employed in the source code. This technique, however, preserves sufficient data for validation experiments to be performed by some software engineering researchers, particularly those in the software quality metrics community.",1986-05-01
An Example of Deriving Performance Properties from a VisualRepresentation of Program Execution,"This paper demonstrates that one can use a visual representation of the execution of a program to analyze performance properties.  The method starts with a visualization of the progress of processes, and then applies geometric properties of the visualization to analyze program execution behavior.  The visualization is based on progress graphs from the literature, which map the progress of each process to one Cartesian graph axis.  Line segments represent interprocess synchronization.  A directed, continuous path that does not cross a segment represents a particular execution of a program, and can be found using computational geometric algorithms.  A special case is also considered in which programs display periodic behavior.  The relation of the program class studied to Petri nets is also examined.",1995-06-01
Theory of Globally Convergent Probability-One Homotopies for Nonlinear Programming,"For many years, globally convergent probability-one homotopy methods have been remarkably successful on difficult realistic engineering optimization problems, most of which were attacked by homotopy methods because other optimization algorithms failed or were ineffective. Convergence theory has been derived for a few particular problems, and considerable fixed point theory exists, but generally convergence theory for the homotopy maps used in practice for nonlinear constrained optimization has been lacking. This paper derives some probability-one homotopy convergence theorems for unconstrained and constrained optimization, for linear and nonlinear constraints, and with and without convexity. Some insight is provided into why the homotopies used in engineering practice are so successful, and why this success is more than dumb luck. By presenting the theory as variations on a prototype probability-one homotopy convergence theorem, the essence of such convergence theory is elucidated.",1999-09-01
The Impact of Lookahead on Conservative Simulation,"This paper studies the impact on the performance of conservative simulation for both open and closed models.  For open models, we derive an upper bound on the performance improvement due to the lookahead.  We show that the benefit of lookahead diminishes as the simulation length increases.  For closed models, on the other hand, the performance improvement converges to a constant as the simulation length increases.",1995-02-01
Improving Genetic Algorithm Efficiency and Reliability in the Design and Optimization of Composite Structures,"Genetic algorithms (GAs) often require many iterations for convergence.  If the cost for the analysis of each laminate is high, then GA optimization becomes infeasible due to the large amount of CPU time required.  A genetic algorithm's ability to find optimal laminate designs that have complicated stacking sequence patterns in an efficient manner may be improved if the GA takes greater advantage of all the information generated throughout the search scheme.  In a standard GA procedure, an elitist method is typically implemented where the worst laminate from the chil population is replaced with the best laminate from the parent population.  Valuable information that may exist in other laminates of the parent population is no longer utilized once the child population has been created.  The present paper suggests new multiple elitist and variable elitist schemes where more than just the best laminate from the old population may be preserved in successive generations providing the GA with additional laminate designs with good performance.  These additional designs may contain pieces of the stacking sequence pattern that are vital for achieving the optimal laminate and help the GA converge more rapidly.  Results generated by utilizing the multiple elitist and variable elitist methods have shown to yield richer final populations and minor improvements in the computational cost, while maintaining a high level of reliability.",1996-10-01
Domain Engineering: An Empirical Study,"This paper presents a summary and analysis of data gathered from thirteen domain engineering projects, participant surveys, and demographic information. Taking a failure modes approach, project data is compared to an ideal model of the DARE methodology, revealing valuable insights into points of failure in the domain engineering process. This study suggests that success is a function of the domain analyst’s command of a specific set of domain engineering concepts and skills, the time invested in the process, and persistence in difficult areas. We conclude by presenting strategies to avoid points of failure in future domain engineering projects.",2006
Prediction-Based Prefetching for Remote Rendering Streaming in Mobile Virtual Environments,"Remote Image-based rendering (IBR) is the most suitable solution for rendering complex 3D scenes on mobile devices, where the server renders the 3D scene and streams the rendered images to the client. However, sending a large number of images is inefficient due to the possible limitations of wireless connections. In this paper, we propose a prefetching scheme at the server side that predicts client movements and hence prefetches the corresponding images. In addition, an event-driven simulator was designed and implemented to evaluate the performance of the proposed scheme. The simulator was used to compare between prediction-based prefetching and prefetching images based on spatial locality. Several experiments were conducted to study the performance with different movement patterns as well as with different virtual environments (VEs). The results have shown that the hit ratio of the prediction-based scheme is greater than the localization scheme in the case of random and circular walk movement patterns by approximately 35% and 17%, respectively. In addition, for a VE with high level of details, the proposed scheme outperforms the localization scheme by approximately 13%. However, for a VE with low level of details the localization based scheme outperforms the proposed scheme by only 5%.",2007-06-01
Dynamic Working Set Memory Allocation for Concurrent Processes,"The problem of allocating a given total amount of main memory page-frames M(t) available at time t, among a given set of concurrently active processes {P-sub i} is examined.  Memory allocation is managed under the working set policy, and it is assumed that M(t) is larger than the total sum of all working-set sizes but less than the sum of distinct pages in {P-sub i}.  It is required that each process receive at least an allocation equal to its working-set size and that the excess memory be additionally distributed among the concurrent processes to enhance their performance.  The upper and lower bounds on the individual memory allocations for each process are derived.  A fair policy for apportioning the excess memory among the concurrent processes is presented.  A procedure for rounding off fragmented page apportionment is described.  An application example, illustrating dynamically changing conditions in the concurrent active processes, is described.",1990
An Assessment Of Quasi-Newton Sparse Update Techniques For Nonlinear Structural Analysis,"In this paper an attempt is made to evaluate the performance of a  few algorithms for unconstrained minimization of nonlinear functions  that exploit sparsity of the Hessians of such functions. The evaluation  is centered around large scale, geometrically nonlinear problems of  structural analysis in general. In particular, the snap-through response of finite element models of a shallow elastic arch under a concentrated load at the crown is considered. The sensitivity of these algorithms to varying degrees of refinement of these finite element models as  well as to the sparsity pattern of the Hessian of the potential surface  in question are examined. The paper concludes by making recommendations  on the choice of an algorithm based on the scale of the problem and the  degree and type of nonlinearity.",1980
Validation of Expert System Performance,"Most definitions of an expert system include some reference to the ability of the system to perform at a level close to human expert performance. Yet the validation of expert systems, that is, the testing of systems so as to ascertain that they achieve an acceptable level of performance, has (with a few exceptions) been ad-hoc, informal, and in some cases of dubious value.  This paper attempts to establish validation as an important concern in expert systems research and development. The problems in validating an expert system are discussed, and a number of methods for validating expert systems, both qualitative and quantitative, are presented.",1986
Discrete Event Simulation with Pascal,"Pascal_SIM is a collection of Pascal constants, types, variables, functions and procedures for developing event, activity, three-phase of process oriented discrete-event simulation models.  Facilities are provided for queue processing, time advance and event list maintenance, control of entities and resources, random number generation and streams, sampling from parametric and empirical distributions, statistics collection, and visual displays.",1986-06-01
Covering a Set with Arithmetic Progressions is NP-Complete,"This paper defines a new class of set covering problems in which the subsets are implicitly derived from the properties of the set elements. In particular, the set elements are integers and the subsets are finite arithmetic progressions.  Both minimum cover and exact cover problems are defined.  Both problems are shown to be NP-Complete.",1989
An Operation Model Supporting the Generation of Requirements That Capture Customer Intent,"Product quality is a direct reflection of how well it meets the cutomer's needs. Hence, the ability to capture customer requirements correctly and succinctly is paramount. Unfortunately, within software development frameworks the gathering of requirements is one of the more ill-defined and least structured processes. To address this inadequacy, our paper (a) proposes a refinement (or decomposition) to the requirements generation phase that admits to detailed examination and promotes a better understanding of where requirements generation goes wrong,(b) using the results of a preliminary study, identifies where and how miscommunication and requirements omission occur, and (c) outlines a more structured process that can be applied early in the requirements generation process to minimize requirement faults.",1999-11-01
History in the Computer Science Curriculum,"IFIP  Working Group 9.7 (History of Computing) is charged with not only encouraging the preservation of computer artifacts, the recording of the memoirs of pioneers, and the analysis of the downstream impact of computer innovations, but also on the development of educational modules on the history of computing.  This paper presents an initial report on the study of the history of computing and informatics and preliminary proposals for the inclusion of aspects of the history of computing and informatics in the curriculum of university and college students.",1995-07-01
Multimedia Traffic Analysis Using CHITRA95,"We describe how to investigate collections of trace data representing network delivery of multimedia information with CHITRA95, a tool that allows a user to visualize, query, statistically analyze and test, transform, and model collections of trace data.  CHITRA95 is applied to characterize World Wide Web (WWW) traffic from three workloads: students in a classroom of network-connected workstations, graduate students browsing the Web, undergraduates browsing educational and other materials, as well as traffic on a courseware repository server.  We explore the inter-access time of files on a server (i.e., recency), the hit rate from a proxy server cache, and the distributions of file sizes and media types requested.  The traffic study also yields statistics on the effectiveness of caching to improve transfer rates.  In contrast to past WWW traffic studies, we analyze client as well as server traffic; we compare three workloads rather than drawing conclusions from one workload; and we analyze tcpdump logs to calculate the performance improvement in throughput that an end user sees due to caching.",1995-04-01
Suitability of Optimization Packages for an MDO Environment,"An examination of the performance of several optimization packages and their suitability for inclusion in a realistic multidisciplinary design optimization (MDO) environment is conducted. The packages are incorporated into a High-Speed Civil Transport (HSCT) aircraft design code and are used with both a response surface (RS) model and a more detailed, noisy model.  While most packages converge to similar designs, there are large variations in CPU times and usability. Results are reported for a SGI workstation.",1996-10-01
The Global Stability of a Rigid Solid Supported by Elastic Columns,"A new stability index, the global critical load, is advocated.  This index is useful for flexible structures prone to large disturbances such as earthquakes.  A symmetric rigid body supported by flexible legs is studied in detail.  The nonlinear equilibrium equations are solved and the results show that global stability depends heavily on the height of the mass center and the distance between the legs.",1994
Response Surface Models Combining Linear and Euler Aerodynamics for HSCT Design,"A method has been developed to efficiently implement supersonic aerodynamic predictions from Euler solutions into a highly constrained, multidisciplinary design optimization of a High-Speed Civil Transport.  The method alleviates the large computational burden associated with performing computational fluid dynamics analyses through the use of variable-complexity modeling techniques, response surface methodologies, and coarse grained parallel computing.  Using information gained from lower fidelity aerodynamic models, reduced term response surface models representing a correction to the linear theory response surface model predictions are constructed using Euler solutions.  Studies into five, ten, fifteen, and twenty variable design problems show that accurate results can be obtained with the reduced term models at a fraction of the cost of creating the full term quadratic response surface models.  Specifically, a savings of 255 CPU hours out of 392 CPU hours required to create the full term response surface model is obtained for the twenty variable problem on a single 75 MHz IP21 processor of a SGI Power Challenge.",1998-08-01
Performance Analysis of a Novel GPU Computation-to-core Mapping Scheme for Robust Facet Image Modeling,"Though the GPGPU concept is well-known in image processing, much more work remains to be done to fully exploit GPUs as an alternative computation engine. This paper investigates the computation-to-core mapping strategies to probe the efficiency and scalability of the robust facet image modeling algorithm on GPUs. Our fine-grained computation-to-core mapping scheme shows a significant performance gain over the standard pixel-wise mapping scheme. With in-depth performance comparisons across the two different mapping schemes, we analyze the impact of the level of parallelism on the GPU computation and suggest two principles for optimizing future image processing applications on the GPU platform.",2012
A Practical Blended Analysis for Dynamic Features in JavaScript,"JavaScript is widely used in Web applications; however, its dynamism renders static analysis ineffective. Our JavaScript Blended Analysis Framework is designed to handle JavaScript dynamic features. It performs a flexible combined static/dynamic analysis. The blended analysis focuses static analysis on a dynamic calling structure collected at runtime in a lightweight manner, and refines the static analysis using dynamic information. The framework is instantiated for points-to analysis with stmt-level MOD analysis and tainted input analysis. Using JavaScript codes from actual webpages as benchmarks, we show that blended points-to analysis for JavaScript obtains good coverage (86.6% on average per website) of the pure static analysis solution and finds additional points-to pairs (7.0% on average per website) contributed by dynamically generated/loaded code. Blended tainted input analysis reports all 6 true positives reported by static analysis, but without false alarms, and finds three additional true positives.",2012
Granularity Issues for Solving Polynomial Systems via Globally Convergent Algorithms on a Hypercube,"Polynomial systems of equations frequently arise in many applications such as solid modeling, robotics, computer vision, chemistry, chemical engineering, and mechanical engineering . Locally convergent iterative methods such as quasi-Newton methods may diverge or fail to find all meaningful solutions of a polynomial system. Recently a homotopy algorithm has been proposed for polynomial systems that is guaranteed globally convergent (always converges from an arbitrary starting point) with probability one, finds all solutions to the polynomial system, and has a large amount of inherent parallelism. There are several ways the homotopy algorithms can be decomposed to run on a hypercube. The granularity of a decomposition has a profound effect on the performance of the algorithm. The results of decompositions with two different granularities are presented. The experiments were conducted on an iPSC-16 hypercube using actual industrial problems.",1988-05-01
"Design Issues in General Purpose, Parallel Simulation Languages","This paper addresses the topic of designing general purpose, parallel simulation languages.  We discuss how parallel simulation languages differ from general purpose programming languages.  Our thesis is that the issues of distribution, performance, unusual implementation mechanism, and the desire for determinism are the dominant considerations in designing a simulation language today.  We then discuss the separate roles that special and general purpose simulation languages play.  Next we use the two languages, Sim++ and CPS, to illustrate these issues.  Then we discuss eight design considerations: process versus event oriented-view, basic program structure, I/O, making implementation cost explicit to the programmer, providing dynamic facilities, memory management, the semantics of false messages in time warp, and program development methodology considerations.  A number of conclusions are drawn from our experiences in language design.",1989
Implications of Using the Event Scheduling World View for Parallel Simulation,"Use of a particular world view, or conceptual framework, makes the programming of many simulations easier.  Parallel simulation implementations generally have not been based on a particular world view.  An open question is what impact a world view has on the execution of a simulation in parallel.",1990
Performance Evaluation of Three Algorithms to Generate Fundamental Cycles for Twelve Different Graphs,No abstract available.,1982
Personalization by Partial Evaluation.,"The central contribution of this paper is to model personalization by the programmatic notion of partial evaluation.Partial evaluation is a technique used to automatically specialize programs, given incomplete information about their input.The methodology presented here models a collection of information resources as a program (which abstracts the underlying schema of organization and ﬂow of information),partially evaluates the program with respect to user input,and recreates a personalized site from the specialized program.This enables a customizable methodology called PIPE that supports the automatic specialization of resources,without enumerating the interaction sequences beforehand .Issues relating to the scalability of PIPE,information integration,sessioniz-ling scenarios,and case studies are presented.",2001
A Study and Project-Based Evaluation of the Software Engineering Evaluation System (SEES),"The purpose of this document is to describe the design and execution of the quasi-experiment conducted in the Department of Computer Science, Virginia Tech, in accordance with the procedures described in the document ""An Experimental Design for Evaluating SEES"" prepared for NASA under contract NAS1-19610, Task 17. (Hereafter, that report is called the general design document.)  The quasi-experiment serves three important purposes.  First, the quasi-experiment is the proof of concept of the general experiment design.  It shows that the procedures defined in the general experiment design can be implemented. Second, provides details for setting up a true experiment: identifying the research hypothesis, designing the investigation, selecting various variables, procedures, and controls, measuring the variables, and evaluating the results.  Third, any insights revealed during the course of the quasi-experiment can be incorporated in the costly true experiment, thus providing a cost-effective experimental methodology. This report contains sufficient information so that the quasi-experiment can be replicated at an appropriate level of abstraction.  We also document and interpret all the results of the quasi-experiment.",1997-02-01
Designing a Testing Strategy for Expert Systems,"Testing programs with tractable algorithms is one area in which software engineers have made numerous advances over the past few decades.  Testing rule-based expert systems, however, is a new area in software engineering which requires new testing techniques.  For the most part, traditional software engineering testing strategies assume modular program development. This is assumption impractical to make for expert system development, for the knowledge base of an expert system is quite simply a huge non-modular program. It consists almost entirely of non-ordered, multi-branching decision statements. In traditional programming, the module interfaces are limited and well defined. For rule-based expert systems, the interaction among rules is combinatoric and highly data-driven. Thus, the testing of a completed expert system via traditional path analysis is impractical. The design of a testing strategy for expert systems focuses on the generic phases of expert system development.  Briefly, these phases include system definition, incremental system implementation, and system maintenance. Using this simplified breakdown of the expert system development process as a guide, certain testing techniques can be generalized enough to work for any expert system application.",1988
Report on Quasi-Experiment for Evaluating SEES,"The purpose of this document is to describe the design and execution of the quasi-experiment conducted in the Department of Computer Science, Virginia Tech, in accordance with the procedures described in the document, ""An Experimental Design for Evaluating SEES"" prepared for NASA under contract NASI-19610, Task 17. (Hereafter that report is called the general design document.) The quasi-experiment serves three important purposes. First, the quasi-experiment is the proof of concept of the general experiment design. It shows that the procedures defined in the general experiment design can be implemented. Second, provides details for setting up a true experiment: identifying the research hypothesis, designing the investigation, selecting various variables, procedures, and controls, measuring the variables, and evaluating the results. Third, any insights revealed during the course of the quasi-experiment can be incorporated in the costly true experiment, thus providing a cost-effective experimental methodology. This report contains sufficient information so that the quasi-experiment can be replicated at an appropriate level of abstraction. We also document and interpret all the results of the quasi-experiment.",1995-12-01
Software Reuse and Reusability Metrics and Models,"As organizations implement systematic software reuse programs to improve productivity and quality, they must be able to measure their progress and identify the most effective reuse strategies.  This is done with reuse metrics and models.  In this paper, we survey metrics and models of software reuse and reusability, and provide a classification structure that will help users select them.  Six types of metrics and models are reviewed: cost-benefit models, maturity assessment models, amount of reuse metrics, failure modes models, reusability assessment models, and reuse library metrics.",1995-05-01
ACT++ 2.0: A Class Library for Concurrent Programming in C++ Using Actors,"ACT++ 2.0 is the most recent version of a class library for concurrent programming in C++. The underlying model of concurrent computation is the Actor model. Programs in ACT++ consist of a collection of active objects called actors. Actors execute concurrently and cooperate by sending request and reply messages. An agent, termed the behavior of an actor, is responsible for processing a single request message and for specifying a replacement behavior which processes the next available request message. One of the salient features of ACT++ is its ability to handle the Inheritance Anomaly---the interference between the inheritance mechanism of object-oriented languages and the specification of synchronization constraints in the methods of a class---using the notion of behavior sets. ACT++ has been implemented on the Sequent Symmetry multiprocessor using the PRESTO threads package.",1992
Solving Finite Difference Approximations to Nonlinear Two-point Boundary Value Problems by a Homotopy Method,"The Chow-Yorke algorithm is a homotopy method that has been proved globally convergent for Brouwer fixed point problems, classes of zero finding, nonlinear programming, and two-point boundary value problems. The method is numerically stable, and has been successfully applied to several practical nonlinear optimization and fluid dynamics problems. Previous application of the homotopy method to two-point boundary value problems has been based  on shooting, which is inappropriate for fluid dynamics problems with sharp boundary layers. Here the Chow-Yorke algorithm is proved globally convergent for a class of finite difference approximations to nonlinear two-point boundary value problems. The numerical implementation of the algorithm is briefly sketched, and computational results are given for two fairly difficult fluid dynamics boundary value problems.",1979
A Query Language for Information Graphs,"In this paper we propose a database model and query language for information retrieval systems.  The information graph model and Graph Object Access Language (GOAL) allow integrated handling of data, information and knowledge along with a variety of specialized objects (e.g., for geographic or multimedia information systems).  There is flexible support for hyperbases, thesauri, lexicons, and both relational and object-oriented types of DBMS applications.  In this paper we give the first published account of our new, powerful model and language (GOAL), illustrate their use, and compare them with related work.",1993
Engineering Applications of the Chow-Yorke Algorithm,"The Chow-Yorke algorithm is a scheme for developing homotopy methods that are globally convergent with probability one. Homotopy maps leading to globally convergent algorithms have been created for Brouwer fixed point problems, certain classes of nonlinear systems of equations, the nonlinear complementarity problem, some nonlinear two-point boundary value problems, and convex optimization problems. The Chow-Yorke algorithm has been successfully applied to a wide range of engineering problems, particularly those for which quasi-Newton and locally convergent iterative techniques are inadequate. Some of those engineering applications are surveyed here.",1981
First Experiences with TMM,"The Task Mapping Model (TMM) is a human-computer interaction technique that supports situational analyses to derive new design requirements from formative evaluation findings.  While the TMM methodologies and formalisms are currently being developed and validated, this paper reports some informal first experiences and findings.  Two developers who are presently working on independent non-trivial interfaces were asked to use TMM and comment on it.  The developers found benefit in TM analyses, albeit in different ways, and their views were captured with an informal subjective post-analysis survey and reported here.",1993
Prediction-based Power-Performance Adaptation of Multithreaded Scientific Codes,"Computing is currently at an inflection point, with the degree of on-chip thread-level parallelism doubling every one to two years.  The number of cores has become one of the most important architectural parameters that characterize performance and power-efficiency of a modern microprocessor, and a computer system in general.  Concurrency lends itself naturally to allowing a program to trade some of its performance for power savings, by regulating the number of active cores. Unfortunately, in several computing domains, users are unwilling to sacrifice performance to save power.  Futhermore, the opportunities for saving power via other means, such as voltage and frequency scaling, may be limited in heavily optimized applications. In this paper, we present a prediction model for identifying energy-efficient operating points of concurrency in well-tuned multithreaded scientific applications, and a runtime system which uses live analysis of hardware event rates through the prediction model, to optimize applications dynamically. The runtime system throttles concurrency so that power consumption can be reduced and performance can be set at the knee of the scalability curve of each parallel execution phase.  We present a dynamic, phase-aware performance prediction model (DPAPP), which combines multivariate regression techniques with runtime analysis of data collected from hardware event counters, to locate optimal operating points of concurrency. DPAPP is hardware-aware, in the sense that it takes into account the dimensions of parallelism in the architecture, using distinct predictors and hardware events for each dimension. It is also phase-aware. Using DPAPP, we develop a prediction-driven runtime optimization scheme, which drastically reduces the overhead of searching the optimization space for power-performance efficiency, while achieving near-optimal performance and power savings in real parallel applications.",2007
Effect of Elevated Mass Center on the Global Stability of a Solid Supported by Elastica Columns,"Recently we advocated a new stability index, the global critical load, for very elastic structures.  This index is extremely useful for flexible structures under large disturbances such as earthquakes.  The present note determines this index for a two-dimensional rigid solid supported by two flexible columns.  Using the nonlinear elastica equations the buckling and postbuckling problem is solved by a homotopy nonlinear system solver.  The present results show the bifurcation curve is quite sensitive to the elevated mass center.  The global buckling load is drastically reduced although the critical buckling load of linear stability analysis is the same.  An explanation is given through the study of a solid supported by one column.",1996
CoreTSAR: Task Scheduling for Accelerator-aware Runtimes,"Heterogeneous supercomputers that incorporate computational accelerators such as GPUs are increasingly popular due to their high peak performance, energy efficiency and comparatively low cost. Unfortunately, the programming models and frameworks designed to extract performance from all computational units still lack the flexibility of their CPU-only counterparts. Accelerated OpenMP improves this situation by supporting natural migration of OpenMP code from CPUs to a GPU. However, these implementations currently lose one of OpenMP’s best features, its flexibility: typical OpenMP applications can run on any number of CPUs. GPU implementations do not transparently employ multiple GPUs on a node or a mix of GPUs and CPUs. To address these shortcomings, we present CoreTSAR, our runtime library for dynamically scheduling tasks across heterogeneous resources, and propose straightforward extensions that incorporate this functionality into Accelerated OpenMP. We show that our approach can provide nearly linear speedup to four GPUs over only using CPUs or one GPU while increasing the overall flexibility of Accelerated OpenMP.",2012
"Architecture of an Object-Oriented Expert System for Composite Document Analysis, Representation, and Retrieval","The CODER project is a multi-year effort to investigate how best to apply artificial intelligence methods to increase the effectiveness of information retrieval systems when handling collections of composite documents. In order to ensure system adaptability and to allow reconfiguration for controlled experimentation, the project has been designed as an expert system. The use of individually tailored specialist experts coupled with standardized blackboard modules for communication and internal and external knowledge bases for managing effective knowledge allows for quick prototyping, incremental development and flexibility under change. The system as a whole is structured as a set of communicating modules, designed under an object-oriented paradigm and implemented under UNIX&tm using pipes and the TCP/IP protocol. Inferential modules are being coded in  MU-Prolog; non-inferential modules are being prototyped in MU-Prolog and will be re-implemented as needed in C++.",1986-04-01
"Forward, Tangent Linear, and Adjoint Runge Kutta Methods in KPP–2.2 for Efficient Chemical Kinetic Simulations","The Kinetic PreProcessor (KPP) is a widely used software environment which generates Fortran90, Fortran77, Matlab, or C code for the simulation of chemical kinetic systems. High computational efficiency is attained by exploiting the sparsity pattern of the Jacobian and Hessian. In this paper we report on the implementation of two new families of stiff numerical integrators in the new version 2.2 of KPP. One family is the fully implicit three-stage Runge Kutta methods, and the second family are singly  diagonally-implicit Runge Kutta methods. For each family tangent linear models for direct decoupled sensitivity analysis, and adjoint models for adjoint sensitivity analysis of chemical kinetic systems are also implemented. To the best of our knowledge this work brings the first implementation of the direct decoupled sensitivity method and of the discrete adjoint sensitivity method with Runge Kutta methods. Numerical experiments with a chemical system used in atmospheric chemistry illustrate the power of the stiff Runge Kutta integrators and their tangent linear and discrete adjoint models. Through the integration with KPP–2.2. these numerical techniques become easily available to a wide community interested in the simulation of chemical kinetic systems.",2006-07-01
Multi Safe -- a Modular Multiprocessing Approach to Secure Database Management,"This paper describes the configuration and inter-module communication of a MULTIprocessor system for supporting Secure Authorization with Full Enforcement (MULTISAFE) for database management. A modular architecture is described which provides secure, controlled access to shared data in a multiuser environment--with low performance penalties, even for complex protection policies. The primary mechanisms are structured and verifiable. The entire approach is immediately extendible to distributed protection of distributed data. The system includes a user and applications module (UAM), a data storage and retrieval module (SRM), and a protection and security module (PSM). The control of intermodule communication is based on a data abstraction approach. It is initially described in terms of function invocations. An implementation within a formal message system is then described. The discussion of function invocations begins with the single terminal case and extends to the multiterminal case. Some physical implementation aspects are also discussed, and some examples of message sequences are given.",1980
Simulation Model Management: Resolving the Technological Gaps,"Model management poses requirements and responsibilities that extend throughout the life cycle of a simulation model. Recent publications have identified major problems in cost and time overruns, which are traceable to deficiencies in project and sponsor management. Beginning with the division of the simulation model life cycle into seven phases, we define ""model management"" and develop the requirements for a Model Management System (MMS). The functional description of a MMS focuses on those phases that jointly characterize the model development effort. Recent research in simulation model development is described, and particular emphasis is given to the approach taken with the Conical Methodology.",1981
Implementation of Fortran Random Number Generators on Computers with One's Complement Arithmetic,No abstract available.,1974
On Parallel ELLPACK for Shared Memory Machines,"This report outlines design goals and criteria for a new version of ELLPACK for shared memory multiprocessors.  We discuss several specific issues in detail, and suggest paths for further work.  An example is given which illustrates some of the tradeoffs necessary in parallelizing a large and complicated mathematical software package such as ELLPACK. We also raise several important questions that must be dealt with as packages such as ELLPACK evolve along with modern high-performance architectures.",1990
Contextual Boundary Formation by One-dimensional Edge Detection And Scan Line Matching,No abstract available.,1980
A Homotopy Approach for Solving Constrained Optimization Problems,"A homotopy approach for solving constrained parameter optimization problems is examined.  The first order necessary conditions, with the complementavity conditions represented using a technique due to Mangasarian, are solved. The equations are augmented to avoid singularities which occur when the active constraint changes. The Chow -Torke algorithm is used to track the homotopy path leading to the solution to the desired problem at the terminal point. A simple example which illustrates the technique, and an application to a fuel optimal orbital transfer problem are presented.",1988
The Equilibrium States of a Heavy Rotating Column,"A heavy, rotating vertical column is clamped at one end and free at the other end. The stability boundaries are found by both analytical approximations and numerical integration. The problem depends on two non-dimensional parameters: beta representing the importance of gravity to rigidity and alpha representing the importance of rotation to rigidity. Buckled shapes for the different modes are also obtained.",1982
Analysis of Future Event Set Algorithms for Discrete Event Simulation,"This work reports on new analytical and empirical results on the performance of algorithms for handling the  future event set in discrete event simulation.  These  results provide a clear insight to the factors affecting algorithm performance; evaluate the ""hold"" model, often used to study future event set algorithms; and determine the best algorithm(s) to use.",1980
The Design and Implementation of Concurrent Input/Output Facilities in ACT++ 2.0,"ACT++ 2.0 is the most recent version of a class library for concurrent programming in C++. Programs in ACT++ consist of a collection of active objects called actors. Actors execute concurrently and cooperate by sending request and reply messages. An agent, termed the behavior of an actor, is responsible for processing a single request message and for specifying a replacement behavior which processes the next available request message. One of the salient features of ACT++ is its realization of I/O as an actor operation. A special type of actor, called an interface actor, provides a high level interface for a file. Interface actors are sent request messages whenever I/O is necessary and can also transparently perform asynchronous I/O. ACT++ has been implemented on the Sequent Symmetry multiprocessor using the PRESTO threads package.",1992
Scientists in the MIST: Simplifying Interface Design for End Users,"We are building a Malleable Interactive Software Toolkit (MIST), a tool set and infrastructure to simplify the design and construction of dynamically-reconfigurable (malleable) interactive software. Malleable software offers the end-user powerful tools to reshape their interactive environment on the fly. We aim to make the construction of such software straightforward, and to make reconfiguration of the resulting systems approachable and    manageable to an educated, but non-specialist, user. To do so, we draw on a diverse body of existing research on alternative approaches to user interface (UI) and interactive software    construction, including declarative UI languages, constraint-based programming and UI management, reflection and data-driven programming, and visual programming techniques.",2006-05-01
A Methodology For Validating Multivariate Response Simulation Models By Using Simultaneous Confidence Intervals,"This paper deals with the substantiation that a multivariate response self- or trace-driven simulation model, within its domain of applicability, possesses a satisfactory range of accuracy consistent with the intended application of the model.  A methodology is developed by using simultaneous confidence intervals to do this substantiation with respect to the mean behavior of a simulation model that represents an observable system. A trade off analysis can be performed and judgement decisions can be made as to what data collection budget to allocate, what data collection method to use, how many observations to collect on each of the model and system response variables, and what confidence level to choose for producing the range of accuracy with satisfactory lengths. The methodology is illustrated for self-driven steady-state and trace-driven terminating simulations.",1981
The Comparison and Improvement of Effort Estimates from Three Software Cost Models,"This paper presents an empirical investigation of effort estimation techniques using three software cost models: Boehm's basic COCOMO model, Thebaut's COPMO model, and Putnam's model. The results, based on proprietary historical project data provided by a major computer manufacturer, are in three parts: (1) a comparison of the three mdels showing that more accurate estimates are obtained from the COPMO model; (2) improvements in the COPMO estimates by means of two techniques termed ""submodeling"" and ""constructive modeling""; and (3) the definition and evaluation of a promising new technique, termed ""adjustment multipliers"", for improving a model's estimates. Finally, a second set of industrial data is used to confirm the general applicability of these improvement techniques.",1987
Generalized Structured Programming,"In an effort to eliminate some inconveniences connected with Dijkstra's method of Structured Programming, a generalized set of basic flow graphs for structuring programs is suggested. These structures generate the set of all flow graphs that can be fully decomposed by Allen and Cocke's method of interval reduction. It will be shown that programs Composed of the proposed basic structures have most, if not all, of the Positive characteristics claimed for programs written with the classic rules of Structured Programming. Further, by extending Wirth's programming language PASCAL a set of new control constructs has been suggested that support the proposed set of flow structures.",1973
The Development of Software Engineers: A View From a User,"Like many organizations, the Naval Surface Weapons Center (NSWC) has recognized a tremendous growth in the use of computing resources. How NSWC focused attention on the crucial role of software development technology and devised a plan for dealing with the scarcity of competent software development  personnel is the subject of this paper.  The necessary knowledge areas for software engineering are identified in discussing the academic requirements and some conclusions arrived at during the deliberations are mentioned.",1981
Strategies for Introducing Formal Methods into the ADA Life Cycle,This is the final report of a short study of the applicability of formal definition techniques to program development activities with specific emphasis on using the programming language Ada.  The portion of the study here encompasses three elements: - A review of the various formal defition techniques; - A study of the existing and planned tools in programming development environments; - An examination of life cycle methodologies with the objective of inserting formalized techniques.,1988
A Protection Model Incorporating Both Authorization and Constraints,"This paper presents a powerful and flexible protection model which includes both authorizations of open systems and constraints of closed systems. In this model, rules of 'inheritance"" determine the authorizations which are created for new data derived by authorized computations from existing data. These rules create a middle-ground between purely discretionary and purely non-discretionary systems. Although the proposed protection model is quite general, it is presented in this paper in the context of a distributed relational database system. The core mechanisms of the model control access to all databases including the authorization and constraint data bases themselves. It is, therefore, a self-regulating and integrated system. The power and flexibility of the model derive from its use of authorizations and constraints as two complementary and interrelated types of control. The tight protection provided by closed systems is maintained since constraints are defined only as a complement to authorizations and not as a substitute. An enforcement algorithm is given which shows how the effects of the authorizations and constraints can be efficiently realized. Among other applications, it is shown how this model provides a useful, partial answer to the question of safety decidability.",1985
Dynamic Quantum Allocation and Swap-Time Variability in Time-Sharing Operating Systems,No abstract available.,1973
Rotation of Polygonal Space Structures,"The free rotation in space of flexible polygonal structures about their axes is studied.  The large deformation equations are formulated, resulting in a set of sixth order nonlinear differential equations.  The governing parameter B represents the relative importance of centrifugal forces to flexural rigidity.  Perturbation solutions are obtained for small B.  The equations are also integrated numerically by quasi-Newton and homotopy methods.  Forces, moments, and maximum deformations are determined for polygons of three to six sides and B up to 100.  At very large B, the polygons deform into a circle.",1990
Design and Evaluation of Techniques to Utilize Implicit Rating Data in Complex Information Systems.,"Research in personalization, including recommender systems, focuses on applications such as in online shopping malls and simple information systems. These systems consider user profile and item information obtained from data explicitly entered by users - where it is possible to classify items involved and to make personalization based on a direct mapping from user or user group to item or item group. However, in complex, dynamic, and professional information systems, such as Digital Libraries, additional capabilities are needed to achieve personalization to support their distinctive features: large numbers of digital objects, dynamic updates, sparse rating data, biased rating data on specific items, and challenges in getting explicit rating data from users. In this report, we present techniques for collecting, storing, processing, and utilizing implicit rating data of Digital Libraries for analysis and decision support. We present our pilot study to find virtual user groups using implicit rating data. We demonstrate the effectiveness of implicit rating data for characterizing users and finding virtual user communities, through statistical hypothesis testing. Further, we describe a visual data mining tool named VUDM (Visual User model Data Mining tool) that utilizes implicit rating data. We provide the results of formative evaluation of VUDM and discuss the problems raised and plans for further studies.",2007-05-01
The Uses of Finite Fields,"The paper is tutorial in nature, although some of the results  are new. It reviews some of the elementary facts about the structure  and construction of finite fields and hypothesizes a computer whose fundamental instruction set consists of the Galois field operations.  Each total function is shown to be defined by a unique polynomial and this normal representation is also the minimal polynomial representation.  A method is presented, due to Newton, for constructing the coefficients  of the defining polynomial using divided differences. It is shown that  under certain circumstances a total function may be more efficiently  evaluated by a rational form with non-zero denominator. Finally a  rational form representation is shown to be a natural representation  for each partial function. In the light of these considerations the process of producing code for the hypothetical machine is almost entirely  automated.",1976
"Stylitism, Synergism And Syncretism: The Interface of Computer Science And Operations Research","The interface of computer science and operations research is described from a stylitic perspective. This position enables an honest appraisal of the disciplinary synergism often claimed for the two. Operations research emphasizes the development of algorithms and the implications therein; while computer science gives primary emphasis to the representation of algorithms and the implications of execution on  a digital computer. The realization of a syncretic state can be claimed only for discrete event simulation and, to a lesser extent, for scheduling theory. To the extent that the ""algorithm"" can serve as common ground for both disciplines, a broader, more fundamental form  of syncretism might be achieved.",1978
Vectorization Experiment on the IBM 3090 Vector Facility,"A comparison between sequential code and vectorization code for matrix multiplication is made and the possible ways to vectorize matrix multiplication are examined.  The result is that vectorization of the correct loop results in a speedup of almost 3 for vectors that are long enough, i.e., longer than or equal to half the vector length of the machine.  The number and order of memory references is a very important factor.",1990
Considerations for Future Programming Language Standards Activities,"This paper reviews the current state of programming language standards activities with respect to the anomalies which exist between the various published and proposed standards for FORTRAN, COBOL, PL/I and BASIC. Proposals are made for the inclusion of formalisms within future standards and the extension of the standards to include additional items such as error conditions and documentation.",1976
No Abelian Semigroup Operation Is Complete,This paper shows that there does not exist a finite abelian semigroup   with order > 3 such that the semigroup operation is complete over S. Neither is {*} complete with constants over S. J. C. Muzio has shown that over any finite space there exists a set of two abelian semigroup operations which is complete with constants over the space. Hence Muzio's result is best possible.,1974
Integrated Model Pluralism: An Alternative to a Universal Model Description Language,This paper elaborates an integrated hierarchy of model description formalisms previously proposed. We discuss the advantages of the hierarchical approach vis a vis a standardized universal model description language. Among the principal advantages are the opportunities provided for concise model specification while  at the same time enabling equivalence testing of models expressed in diverse formalisms.,1979
Supersonic Flows of Dense Gases in Cascade Configurations,"We examine the steady, inviscid, supersonic flow of Bethe-Zel'dovich-Thompson (BZT) fluids in two-dimensional cascade configurations.  Bethe-Zel'dovich-Thompson fluids are single-phase gases having specific heats so large that the fundamental derivative of gasdynamics is negative over a finite range of pressures and temperatures.  The equation of state is the well-known Martin-Hou equation, and the numerical scheme is the explicit predictor-corrector method of MacCormack.  Numerical comparisons between BZT fluids and lighter fluids such as steam are presented.  It was found that the natural dynamics of BZT fluids can result in significant reductions in the adverse pressure gradients associated with the collision of compression waves with neighboring turbine blades.  A numerical example of an entirely isentropic supersonic cascade flow is also presented.",1995-11-01
Dynamic Data Structures for a Direct Search Algorithm,"The DIRECT (DIviding RECTangles) algorithm of Jones, Perttunen, and Stuckman (1993), a variant of Lipschitzian methods for bound constrained global optimization, has proved effective even in higher dimensions. However, the performance of a DIRECT implementation in real applications depends on the characteristics of the objective function, the problem dimension, and the desired solution accuracy. Implementations with static data structures often fail in practice, since it is difficult to predict memory resource requirements in advance. This is especially critical in multidisciplinary engineering design applications, where the DIRECT optimization is just one small component of a much larger computation, and any component failure aborts the entire design process. To make the DIRECT global optimization algorithm efficient and robust on large-scale, multidisciplinary engineering problems, a set of dynamic data structures is proposed here to balance the memory requirements with execution time, while simultaneously adapting to arbitrary problem size. The focus of this paper is on design issues of the dynamic data structures, and related memory management strategies. Numerical computing techniques and modiﬁcations of Jones’ original DIRECT algorithm in terms of stopping rules and box selection rules are also explored. Performance studies are done for synthetic test problems with multiple local optima. Results for application to a site-specific system simulator for wireless communications systems (S4W) are also presented to demonstrate the effectiveness of the proposed dynamic data structures for an implementation of DIRECT.",2001
"The GeoSim Interface Library (GIL): Programmer's Manual, Version1.0.1","The GeoSim Interface Library (GIL) is a small, easy to use set of functions for building a graphical user interface.  It was originally designed to support the user interface needs of Project GeoSim, a computer-aided education project for introductory geography classes. The original design for GIL was driven by the need to develop easy to use software that was portable to a variety of computing environments. It was used by a team of programmers engaged in rapid prototyping and frequent change to their user interfaces.  GIL is freely available via anonymous FTP, gopher or Mosaic (or any other World Wide Web client) from geosim.cs.vt.edu, url http://geosim.cs.vt.edu.",1994
Crawling on the World Wide Web,"As the World Wide Web grows rapidly, a web search engine is needed for people to search through the Web. The crawler is an important module of a web search engine. The quality of a crawler directly affects the searching quality of such web search engines. Given some seed URLs, the crawler should retrieve the web pages of those URLs, parse the HTML files, add new URLs into its buffer and go back to the first phase of this cycle. The crawler also can retrieve some other information from the HTML files as it is parsing them to get the new URLs. This paper describes the design, implementation, and some considerations of a new crawler programmed as an learning exercise and for possible use for experimental studies.",2002
Nonreciprocating Sharing Methods in Cooperative Q-Learning Environments,"Past research on multiagent simulation with cooperative reinforcement learning (RL) focuses on developing sharing strategies that are adopted and used by all agents in the environment. In this paper, we target situations where this assumption of a single sharing strategy that is employed by all agents is not valid. We seek to address how agents with no predetermined sharing partners can exploit groups of cooperatively learning agents to improve learning performance when compared to independent learning. Specifically, we propose three intra-agent methods that do not assume a reciprocating sharing relationship and leverage the pre-existing agent interface associated with Q-Learning to expedite learning.",2012-06-01
A Methodology for Test Selection,"Software creation requires not only testing during the development cycle by the development staff, but also independent testing following the completion of the implementation. However in the latter case, the amount of testing that can be carried out is often limited by time and resources. At the very most, independent testing can be expected to provide 100% test coverage of the test requirements (or specifications) associated with the software element with the minimum of effort.  This paper describes a methodology employing integer programming by which the amount of testing required to provide the maximum possible test coverage of the test requirements (for the given test set) is assured while at the same time minimizing the total number of tests to be included in a test suite. A collateral procedure provides recommendations on which tests might be eliminated if less than 100% test coverage of the test requirements is permitted. This latter procedure will be useful in determining the risk of not running the minimum set of tests for 100% test coverage. A third process selects from the test matrix the set of tests to be applied to the system following maintenance modification of any test requirements-- that is, to provide a submatrix for regression testing. The potential benefits for applying the integer programming technique in test data selection is also discussed.",1988
Determining Initial States for Time-Parallel Simulations,"In this paper, we propose a time-parallel simulation method which uses a pre-simulation to identify recurrent states. Also, an approximation technique is suggested for approximate Markovian modeling for queueing networks to extend the class of simulation models which can be simulated efficiently using our time-parallel simulation. A central server system and a virtual circuit of a packet-switched data communication network modeled by closed queueing networks are experimented with the proposed time-parallel simulation. Experiment results suggest that the proposed approach can exploit massive parallelism while yielding accurate results.",1992
Iterative Algorithms for the Linear Complementarity Problem,"Direct complementary pivot algorithms for the  linear complementarity problem with P-matrices are known to  have exponential computational complexity.  The analog of  Gauss-Seidel and SOR iteration for linear complementarity  problems with P-matrices has not been extensively developed.  This paper extends some work of van Bokhoven to a class of  nonsymmetric P-matrices, and develops and compares several  new iterative algorithms for the linear complementarity  problem.  Numerical results for several hundred test  problems are presented. Such indirect iterative algorithms  may prove useful for large sparse complementarity problems.",1984
Mathematical Modeling of Annular Reactors,"A generalized mathematical model for describing the whole-cell-hollow-fiber reactor and the annular bed reactor is presented. The annular reactor model consists of a mixed-type problem for which a novel numerical procedure is developed. The procedure is demonstrated for a number of examples, and it is proved that the model and solution technique are  well suited for the simulation of annular reactors.",1984
Multirate timestepping methods for hyperbolic conservation laws,This paper constructs multirate time discretizations for hyperbolic conservation laws that allow different time-steps to be used in different parts of the spatial domain. The discretization is second order accurate in time and preserves the conservation and stability properties under local CFL conditions. Multirate timestepping avoids the necessity to take small global time-steps (restricted by the largest value of the Courant number on the grid) and therefore results in more efficient algorithms.,2006
CU2CL: A CUDA-to-OpenCL Translator for Multi- and Many-core Architectures,"The use of graphics processing units (GPUs) in high-performance parallel computing continues to become more prevalent, often as part of a heterogeneous system. For years, CUDA has been the de facto programming environment for nearly all general-purpose GPU (GPGPU) applications. In spite of this, the framework is available only on NVIDIA GPUs, traditionally requiring reimplementation in other frameworks in order to utilize additional multi- or many-core devices. On the other hand, OpenCL provides an open and vendorneutral programming environment and runtime system. With implementations available for CPUs, GPUs, and other types of accelerators, OpenCL therefore holds the promise of a “write once, run anywhere” ecosystem for heterogeneous computing. Given the many similarities between CUDA and OpenCL, manually porting a CUDA application to OpenCL is typically straightforward, albeit tedious and error-prone. In response to this issue, we created CU2CL, an automated CUDA-to- OpenCL source-to-source translator that possesses a novel design and clever reuse of the Clang compiler framework. Currently, the CU2CL translator covers the primary constructs found in CUDA runtime API, and we have successfully translated many applications from the CUDA SDK and Rodinia benchmark suite. The performance of our automatically translated applications via CU2CL is on par with their manually ported countparts.",2011
Clustering for Data Reduction: A Divide and Conquer Approach,"We consider the problem of reducing a potentially very large dataset to a subset of representative prototypes.  Rather than searching over the entire space of prototypes, we first roughly divide the data into balanced clusters using bisecting k-means and spectral cuts, and then find the prototypes for each cluster by affinity propagation.  We apply our algorithm to text data, where we perform an order of magnitude faster than simply looking for prototypes on the entire dataset.  Furthermore, our ""divide and conquer"" approach actually performs more accurately on datasets which are well bisected, as the greedy decisions of affinity propagation are confined to classes of already similar items.",2007-10-01
A Gaussian Derivative Based Version of JPEG for Image Compression and Decompression,"The compression and decompression of continuous-tone images is important in document management and transmission systems.  This paper considers an alternative image representation scheme, based on Gaussian derivatives, to the standard discrete cosine transformation (DCT), within a JPEG framework.  Depending on the computer arithmetic hardware used, the approach developed here might yield a compression/decompression technique twice as fast as the DCT and of (essentially) equal quality.",1995-11-01
CHITRA93: A System to Model Ensembles of Trace Data,"CHITRA, a sanskrit word meaning a beautiful or pleasing picture, represents a new approach to understanding trace data generated by computer and communication systems.  CHITRA can analyze any time series data, although its features are specifically tailored to trace data from computer and communication systems that we have studied over the years. Because a single trace file represents just a single observation may or may not be representative of the range of behavior that the system can manifest, CHITRA93 analyzes multiple trace files representing multiple observations.  A collection of trace files is called an ensemble. CHITRA couples methods to visualize and statistically analyze ensembles, culminating in the construction of one or more models that fit the data in an ensemble.  CHITRA addresses the potential state space explosion problem in a model by providing a set of transforms that aggregate or delete information from a trace file.",1994
Probability-One Homotopy Algorithms for Full and Reduced Order H-squared/H-to Infinity Controller Synthesis,"Homotopy algorithms for both full- and reduced-order LQG controller design problems with an H-to infinity constraint on disturbance attenuation are developed.  The H-to infinity constraint is enforced by replacing the covariance Lyapunov equation by a Riccati equation whose solution gives an upper boundary on H-squared performance.  The numerical algorithm, based on homotopy theory, solves the necessary conditions for a minimum of the upper bound on H-squared performance. The algorithms are based on two minimal parameter formulations: Ly, Bryson, and Cannon's 2X2 block parametrization and the input normal Riccati form parametrization.  An over-parametrization formulation is also proposed.  Numerical experiments suggest that the combination of a globally convergent homotopy method and a minimal parameter formulation applied to the upper bound minimization gives excellent results for mixed-norm H-squared/H-to infinity synthesis.  The nonmonocity of homotopy zero curves is demonstrated, proving that algorithms more sophisticated that standard continuation are necessary.",1994
Some Experiments on the Sorting by Reversals Method,Sorting by reversals is the problem of finding the minimum number of reversals required to sort a permutation pi.  The problem is significant with respect to the study of genome rearrangements and phylogeny reconstruction.  This paper presents a programming framework for performing experiments on the problem.  Several conjectures concerning optimal sorting sequences are tested using this framework.,1995-09-01
Confirming the Effectiveness of the Requirements Generation Model,"Product quality is directly related to how well that product meets the customer’s needs and intents. It is paramount, therefore, to capture customer requirements correctly and succinctly. Unfortunately, most development models tend to avoid, or only vaguely define the process by which requirements are generated. Other models rely on formalistic characterizations that require specialized training to understand. To address such drawbacks we introduce the Requirements Generation Model (RGM) that (a) decomposes the conventional “requirements analysis” phase into sub-phases which focus and refine requirements generation activities, (b) constrains and structures those activities, and (c) incorporates a monitoring methodology to assist in detecting and resolving deviations from process activities defined by the RGM. The results of an industry-based study are also presented and substantiate the effectiveness of the RGM in producing a better set of requirements.",2002
Development of the CODER System: A Test-bed for Artificial Intelligence Methods in Information Retrieval,"The CODER (COmposite Document Expert/Extended/Effective Retrieval) system is a test-bed for investigating the application of artificial intelligence methods to increase the effectiveness of information retrieval systems. Particular attention is being given to analysis and representation of heterogeneous documents, such as electronic mail digests or messages, which vary widely in style, length, topic, and structure. Since handling passages of various types in these collections is difficult even for experimental systems like SMART, it is necessary to turn to other techniques being explored by information retrieval and artificial intelligence researchers. The CODER system architecture involves communities of experts around active blackboards, accessing knowledge bases that describe users, documents, or lexical items of various types. Most of the lexical knowledge base construction work is now complete, and experts for search and temporal reasoning can perform a variety of processing tasks.  User information and queries are being gathered, and the first prototype is nearly complete.  It appears that a number of artificial intelligence techniques are needed to best handle such common, but complex, document analysis and retrieval tasks.",1986-12-01
Personalizing the GAMS Cross-Index,"The NIST Guide to Available Mathematical Software (GAMS)system at http://gams.nist.gov serves as the gateway to thousands of scientific codes and modules for numerical com-putation.We describe the PIPE personalization facility for GAMS,whereby content from the cross-index is specialized for a user desiring software recommendations for a specific problem instance.The key idea is to (i)mine structure,and (ii)exploit it in a programmatic manner to generate personalized web pages.Our approach supports both content based and collaborative personalization and enables information integration from multiple (and complementary)web resources.We present case studies for the domain of linear,second-order,elliptic partial differential equations that indicate strong empirical evidence for the usefulness of our semi-automatic approach.",2000-03-01
Toward a Machine Assisted Software Performance Diagnosis Methodology,"This paper discusses a methodology for diagnosing performance problems for parallel and distributed programs.  The methodology is based on the formation and testing of hypotheses about the cause of performance bottlenecks.  The process is illustrated with a case study of an actual problem arising in a parallel discrete event simulation program in which granularity is a primary bottleneck and barrier implementation is a secondary bottleneck.  The paper also describes the evolution of Chitra, a software performance measurement and analysis tool whose objective is to automate certain steps in software performance diagnosis.",1993-04-01
Genetic Algorithms with Local Improvement for Composite Laminate Design,"This paper describes the application of a genetic algorithm to the stacking sequence optimization of a composite laminate plate for buckling load maximization.  Two approaches for reducing the number of analyses are required by the genetic algorithm are described.  First, a binary tree is used to store designs, affording an efficient way to retrieve them and thereby avoid repeated analyses of designs that appeared in previous generations.  Second, a local improvements scheme based on approximations in terms of lamination parameters is introduced. Two lamination parameters are sufficient to define the flexural stiffness and hence the buckling load of a balanced, symmetrically laminated plate.  Results were obtained for rectangular graphite-epoxy plates under biaxial in-plane loading.  The proposed improvements are shown to reduce significantly the number of analyses required for the genetic optimization.",1993
Probability-One Homotopy Algorithms for Robust Controller Analysis and Synthesis with Fixed-Structure Multipliers,"To enable the development of M-K (i.e., multiplier-controller) iteration schemes that do not require (suboptimal) curve fitting, mixed structured singular value analysis tests that allow the structure of the multipliers to a priori be specified, have been developed.  These tests have recently been formulated as linear matrix inequality (LMI) feasibility problems.  The least conservative of these tests always results in unstable multipliers and hence requires a stable coprime factorization of the multiplier before the control synthesis phase of the M-K iteration.  This paper first reviews the LMI formulations of robustness analysis.  It then develops alternative formulations that directly synthesize the stable factorizations and are based on the existence of positive definite solutions to certain Riccati equations.  These problems, unlike the LMI problems, are not convex. The feasibility problem is approached by posing an associated optimization problem that cannot be solved using standard descent methods.  Hence, we develop probability-one homotopy algorithms to find a solution.  These results easily extend to provide computationally tractable algorithms for fixed-architecture, robust control design, which appear to have some advantages over the bilinear matrix inequality (BMI) approaches resulting from extensions of the LMI framework for robustness analysis.",1996
"Virginia Disc 2: Preparing, Presenting and Retrieving MARC StandardLibrary Data on a CDROM","Most of today's commercial information storage systems are stilled based on Boolean queries and inverted files despite the recent advances in information retrieval research.  This report discusses the creation on the CDROM publication, named Virginia Disc 2, with the intention of comparing two retrieval systems: Micro-VTLS and Personal Librarian. Micro-VTLS belongs to the class of conventional Boolean systems with inverted index files.  Personal Librarian uses newer concepts learned from the Syracuse Information Retrieval Experiment (SIRE).  The motivation for comparing the two retrieval approaches is to demonstrate the superiority of newer concepts like term weighting, ranking, and similar measurements employed by Personal Librarian.  Virginia Disc 2 uses MARC, a widely accepted standard format for communicating library information.  The automation of bibliographic information is a prevalent application in libraries and a CDROM implementation using new information retrieval concepts could generate interest in adopting these new approaches.  This report discusses a comparison of Micro-VTLS and Personal Librarian in the areas of retrieval speed, recall and precision.  This report also reviews the process of preparing the Virginia Disc 2 CDROM.  It discusses the considerations, the preparation effort and the final product of sorting MARC library data on a CDROM.",1990
Validation of Multivariate Response Simulation Models by Using Hotelling's Two-sample T^2 Test,"A procedure is developed by using Hotelling's two-sample T^2 test to test the validity of a multivariate response simulation model that represents an observable system. The validity of the simulation model is tested with respect to the mean behavior under a given experimental frame. A trade-off analysis can be performed and judgement decisions can be made as to what data collection budget to allocate, what data collection method to use, how many observations to collect on each of the model and system response variables, and what model builder's risk to choose for testing the validity under a satisfactory model user's risk. The procedure for validation is illustrated for a simulation model that represents an M/M/l queueing system with two performance measures of interest.",1981
Garbage Collection of Actors,"This paper considers the garbage collection of concurrent objects for which it is necessary to know not only ""reachability"", the usual criterion for reclaiming data, but also the ""state"" (active or blocked) of the object.  For the actor model, a more comprehensive definition than previously available is given for reclaimable actors.  Two garbage collection algorithms, implementing a set of ""coloring"" rules, are presented and their computational complexity is analyzed.  Extensions are briefly described to allow incremental, concurrent, distributed and real-time collection.  It is argued that the techniques used for the actor model applies to other object-oriented concurrent models.",1990
A Task Scheduling Algorithm for Minimum Busiest Procesor Idle Time,"This paper provides a heuristic to minimize the idle time of the busiest processor in a system in which the number of modules to be executed by every processor has been predetermined for a given precedence graph.  Except for the busiest processor, the assignment of modules to the other processors is done as evenly as possible.  Each of the modules involved is of unit size.  An exhaustive enumeration solution of the problem is of NP-complete complexity.  The heuristic presented in this paper is of polynomial time.",1993
A Structural Model of Shape,"Shape description and recognition is an important and interesting problem in scene analysis. Our approach to shape description is a formal model of a shape consisting of a set of primitives, their properties, and their interrelationships. The primitives are the simple parts and intrusions of the shape which can be derived through the graph-theoretic clustering procedure described in [31]. The interrelationships are two ternary relations on the primitives: the intrusion relation which relates two simple parts that join to the intrusion they surround and the protrusion relation which relates two intrusions to the protrusion between them. Using this model, a shape matching procedure that uses a tree search with look-ahead to find mappings from a prototype shape to a candidate shape has been developed. An experimental SNOBOL4 implementation has been used to test the program on hand-printed character data with favorable results.",1979
Real-Time Garbage Collection of Actors,"This paper shows how to perform real-time automatic garbage collection of objects possessing their own thread of control.  Beyond its interest as a novel real-time problem, the relevance of automatic management and concurrent objects to real-time applications is briefly discussed.  The specific model of concurrent objects used in the paper is explained.  A definition of real-time garbage collection is given and an algorithm satisfying this definition is described.  An analysis of the relationship between latency, memory overhead and system size shows that this approach is immediately feasible for low-performance real-time systems or multi-processor real-time systems with dedicated processor functions.  Future improvements in the collector's performance are outlined.",1990
Performance Modeling and Analysis of a Massively Parallel DIRECT— Part 2,"Modeling and analysis techniques are used to investigate the performance of a massively parallel version of DIRECT, a global search algorithm widely used in multidisciplinary design optimization applications. Several highdimensional benchmark functions and real world problems are used to test the design effectiveness under various problem structures. In this second part of a twopart work, theoretical and experimental results are compared for two parallel clusters with different system scale and network connectivity. The first part studied performance sensitivity to important parameters for problem configurations and parallel schemes, using performance metrics such as memory usage, load balancing, and parallel efficiency. Here linear regression models are used to characterize two major overhead sources—interprocessor communication and processor idleness—and also applied to the isoefficiency functions in scalability analysis. For a variety of highdimensional problems and large scale systems, the massively parallel design has achieved reasonable performance. The results of the performance study provide guidance for efficient problem and scheme configuration. More importantly, the design considerations and analysis techniques generalize to the transformation of other global search algorithms into effective large scale parallel optimization tools.",2007
Assessing the Utility of a Personal Desktop Cluster,"The computer workstation, introduced by Sun Microsystems in 1982, was the tool of choice for scientists and engineers as an interactive computing environment for the development of scientific codes. However, by the mid-1990s, the performance of workstations began to lag behind high-end commodity PCs. This, coupled with the disappearance of BSD-based operating systems in workstations and the emergence of Linux as an opensource operating system for PCs, arguably led to the demise of the workstation as we knew it.  Around the same time, computational scientists started to leverage PCs running Linux to create a commodity-based (Beowulf) cluster that provided dedicated compute cycles, i.e., supercomputing for the rest of us, as a cost-effective alternative to large supercomputers, i.e., supercomputing for the few. However, as the cluster movement has matured, with respect to cluster hardware and open-source software, these clusters have become much more like their large-scale supercomputing brethren — a shared datacenter resource that resides in a machine room.  Consequently, the above observations, when coupled with the ever-increasing performance gap between the PC and cluster supercomputer, provide the motivation for a personal desktop cluster workstation — a turnkey solution that provides an interactive and parallel computing environment with the approximate form factor of a Sun SPARCstation 1 “pizza box” workstation. In this paper, we present the hardware and software architecture of such a solution as well as its prowess as a developmental platform for parallel codes. In short, imagine a 12-node personal desktop cluster that achieves 14 Gflops on Linpack but sips only 150-180 watts of power, resulting in a performance-power ratio that is over 300% better than our test SMP platform.",2007
Matching Three-Dimensional Objects Using a Relational Paradigm,"A relational model for describing three-dimensional objects has been designed and implemented as part of a database system. The models which provide rough descriptions to be used at the top level of a hierarchy for describing objects, were designed for initial matching attempts on an unknown object. The descriptions are in terms of the set of simple parts of the objects. Simple parts can be sticks (long, thin parts), plates (flat, wide parts) , and blobs (parts that have three significant dimensions). The relations include an attribute-value table for global properties of the object, the properties of the simple parts, binary connection and support relationships, ternary connection relationships, parallel relationships, perpendicular relationships, and binary constraints. An important use of the system is to characterize the similarity and differences between three-dimensional objects. Toward this end, we have defined a measure of relational similarity between three-dimensional object models and a measure of feature similarity, based only on Euclidean distance between attribute-value tables. In a series of experiments, we compare the results of using the two different similarity measures and conclude that the relational similarity is much more powerful than the feature similarity and should be used when grouping the objects in the database for fast access.",1980
Long-Haul TCP vs. Cascaded TCP,"In this work, we investigate the bandwidth and transfer time of long-haul TCP versus cascaded TCP [5]. First, we discuss the models for TCP throughput. For TCP flows in support of bulk data transfer (i.e., long-lived TCP flows), the TCP throughput models have been derived [2, 3]. These models rely on the congestion-avoidance algorithm of TCP. Though these models cannot be applied with short-lived TCP connections, our interest relative to logistical networking is in longer-lived TCP flows anyway, specifically TCP flows that spend significantly more time in the steady-state congestion-avoidance phase rather than the transient slow-start phase. However, in the case where short-lived TCP connections must be modeled, several TCP latency models have been proposed [1, 4] and based on these latency models, the throughput and transfer time of short-lived TCP connections are obtainable. Using the above models, the transfer times for a data file of size S packets can be computed for both long-haul TCP and cascaded TCP. The performance of both systems is compared via their transfer times. One system is said to be preferred if its tranfer time is lower than that of the other. Based on these performance comparisons, we develop a decision model that decides whether to use the cascaded TCP or long-haul TCP.",2006
Design Metrics Which Predict Source Code Quality,"Since the inception of software engineering, the major goal has been to control the development and maintenance of reliable software. To this end, many different design methodologies have been presented as a means to improve software quality through semantic clarity and syntactic accuracy during the specification and design phases of the software life cycle. On the other end of the life cycle, software quality metrics have been proposed to supply quantitative measures of the resultant software. This study is an attempt to unify the concepts of design methodologies and software quantity metrics by providing a means to determine the quality of a design before its implementation. By knowing (quantitatively) the quality of the design, a considerable amount of time and money can be saved by realizing design problems and being able to correct these problems at design time. All of this can be accomplished before any effort has been expended on the implementation of the software. This paper provides a means of allowing a software designer to predict the quality of the source code at design time. Actual equations for predicting source code quality from design metric values are given.",1987
Graph-based Diagnosis of Discrete Event Model Specifications,Several diagnostics which assist in the construction of specifications for discrete event simulation models are defined. The model specifications to be analyzed must be in a particular form called a Condition Specification. The diagnostics are based on analysis of graphs easily derived from a Condition Specification. Most of the diagnostics are intended to be applied as a Condition Specification is being developed. Th ree categories for the classification of diagnostics of model specifications are defined. Two examples illustrate the graphical forms derivable from a Condition Specification.,1983
Set Operations for Unaligned Linear Quadtrees,"An algorithm is presented that performs set operations (e.g., union or intersection) on two unaligned images represented by linear quadtrees. This algorithm seeks to minimize the number of nodes that must be searched for or inserted into the disk-based node lists that represent the trees. Windowing and matching operations can also be cast as unaligned set functions; these operations can then be solved by similar algorithms.",1988
Fol: A Language for Implementing File Organizations for Information Storage And Retrieval Systems,"The language FOL is described. FOL facilitates the implementation of file organizations for IS & R systems. FOL is implemented in a list processing language LPL. Files in FOL are interepreted as a list of records, where each record is equivalent to a node structure. A description of LPL is also included.",1973
"An Interactive Environment for Tool Selection, Specification and Composition","This paper describes a high-level, screen oriented programming environment that supports problem solving by tool selection and tool composition. Each tool is a powerful parameterized program that performs a single high-level operation (e.g., sort a file). To solve a given problem, the user first interacts with the system to compose a task overview consisting of a sequence of generic operations. Such sequences are called compositions.  Once an overview is established, a second part of the environment interacts with the user to help expand the generic operations into a corresponding sequence of parameterized tool calls. When a composition is expanded to include details such as parameterization and punctuation it is called a script. This script, when executed by the underlying runtime system, computes a solution to the specified user task.  The current environment runs under the Unix operating system on a VAX 11/785, and uses a Bitgraph terminal with a 640x720 bitmap display and standard keyboard as the principal interface device.",1986
Parallel Load Balancing Strategies for Ensembles of Stochastic Biochemical Simulations,"The evolution of biochemical systems where some chemical species are present with only a small number of molecules, is strongly inﬂuenced by discrete and stochastic effects that cannot be accurately captured by continuous and deterministic models. The budding yeast cell cycle provides an excellent example of the need to account for stochastic effects in biochemical reactions. To obtain statistics of the cell cycle progression, a stochastic simulation algorithm must be run thousands of times with different initial conditions and parameter values. In order to manage the computational expense involved, the large ensemble of runs needs to be executed in parallel. The CPU time for each individual task is unknown before execution, so a simple strategy of assigning an equal number of tasks per processor can lead to considerable work imbalances and loss of parallel efficiency. Moreover, deterministic analysis approaches are ill suited for assessing the effectiveness of load balancing algorithms in this context. Biological models often require stochastic simulation. Since generating an ensemble of simulation results is computationally intensive, it is important to make efficient use of computer resources. This paper presents a new probabilistic framework to analyze the performance of dynamic load balancing algorithms when applied to large ensembles of stochastic biochemical simulations. Two particular load balancing strategies (point-to-point and all-redistribution) are discussed in detail. Simulation results with a stochastic budding yeast cell cycle model conﬁrm the theoretical analysis. While this work is motivated by cell cycle modeling, the proposed analysis framework is general and can be directly applied to any ensemble simulation of biological systems where many tasks are mapped onto each processor, and where the individual compute times vary considerably among tasks.",2010-12-01
Data Leak Detection As a Service: Challenges and Solutions,"We describe a network-based data-leak detection (DLD) technique, the main feature of which is that the detection does not require the data owner to reveal the content of the sensitive data. Instead, only a small amount of specialized digests are needed. Our technique – referred to as the fuzzy fingerprint – can be used to detect accidental data leaks due to human errors or application flaws. The privacy-preserving feature of our algorithms minimizes the exposure of sensitive data and enables the data owner to safely delegate the detection to others.We describe how cloud providers can offer their customers data-leak detection as an add-on service with strong privacy guarantees. We perform extensive experimental evaluation on the privacy, efficiency, accuracy and noise tolerance of our techniques. Our evaluation results under various data-leak scenarios and setups show that our method can support accurate detection with very small number of false alarms, even when the presentation of the data has been transformed. It also indicates that the detection accuracy does not degrade when partial digests are used. We further provide a quantifiable method to measure the privacy guarantee offered by our fuzzy fingerprint framework.",2012
Programming Environments for Multidisciplinary Grid Communities,"Rapid advances in technological infrastructure as well as the emphasis on application support systems have signaled the maturity of grid computing. Today’s grid computing environments (GCEs) extend the notion of a programming environment beyond the compile-schedule-execute paradigm to include functionality such as networked access, information services, data management, and collaborative application composition. In this article, we present GCEs in the context of supporting multidisciplinary communities of scientists and engineers. We present a high-level design framework for building GCEs and a space of characteristics that help identify requirements for GCEs for multidisciplinary communities. By describing integrated systems for five different multidisciplinary communities, we outline the unique responsibility (and opportunity) for GCEs to exploit the larger context of the scientific or engineering application, defined by the ongoing activities of the pertinent community. Finally, we describe several core systems support technologies that we have developed to support multidisciplinary GCE applications.",2001-07-01
Software Reuse Metrics and Models: A Survey,This paper is no longer available.  It has been replaced by TR-95-07.,1994
Seeing Things Your Way: Information Visualization for a User-Centered Database of Computer Science Literature,"Project Envision is a user-centered multimedia database of computer science literature.  Envision features powerful information visualization by displaying search results as a matrix of icons, with layout semantics under user control.  Its Graphic View interacts with Item Summary and Preview Item windows to give users access to bibliographic information and abstracts.  The concepts underlying these windows are being extended to a Graphical Browser for the full database and for hierarchical structures.  This paper describes the development process and information visualization facilities in Envision search results and browsing displays.",1994
Parallel Global Aircraft Configuration Design Space Exploration,"The preliminary design space exploration for large,interdisciplinary engineering problems is often a difficult and time-consuming task. General techniques are needed that efficiently and methodically search the design space. This work focuses on the use of parallel load balancing techniques integrated with a global optimizer to reduce the computational time of the design space exploration. The method is applied to the multidisciplinary design of a High Speed Civil Transport (HSCT). A modified Lipschitzian optimization algorithm generates large sets of design points that are evaluated concurrently using a variety of load balancing schemes.The load balancing schemes implemented in this study are: static load balancing, dynamic load balancing with a master-slave organization, fully distributed dynamic load balancing, an fully distributed dynamic load balancing via threads. All of the parallel computing schemes have high parallel efficiencies. When the variation in the design evaluation times is small, the computational overhead needed for fully distributed dynamic load balancing is substantial enough so that it is more efficient to use a master-slave paradigm. However, when the variation in evaluation times is increased, fully distributed load balancing is the most efficient.",2000
A Homotopy Algorithm for the Combined H2/H∞ Model Reduction Problem,"The problem of finding a reduced order model, optimal in the H2 sense, to a given system model is a fundamental one in control system analysis and design. The addition of an H∞ constraint to the H2 optimal model reduction problem results in a more practical yet computationally more difficult problem. Without the global convergence of probability-one homotopy methods the combined H2 /H∞ model reduction problem is difficult to solve. Several approaches based on homotopy methods have been proposed. The issues are the number of degrees of freedom, the well posedness of the finite dimensional optimization problem, and the numerical robustness of the resulting homotopy algorithm. Homotopy algorithms based on two formulations---input normal form; Ly, Bryson, and Cannon's 2x2 block parametrization are developed and compared.",1992
Strategies for Parallelizing PDE Software,"Three strategies for parallelizing components of the mathematical software package ELLPACK are considered: an explicit approach using compiler directives available only on the target machine, an automatic approach using an optimizing and parallelizing precompiler, and a two-level approach based on extensive use of a set of low level computational kernels. Each approach to parallelization is described in detail, along with a discussion of the effort involved. In connection with the third strategy, a set of computational kernels useful for PDE solving is proposed. We describe our experience in parallelizing six problem solving components of ELLPACK using each of the three strategies and give performance results for a shared memory multiprocessor. Our results suggest that the two-level strategy allows the best balance among programmer effort, portability, and parallel performance.",1992
An Introduction to the Concepts and Techniques of Automated Theorem-Proving,"This paper summarizes the theoretical foundations and basic methodology of automated theorem-proving. The material presented includes a review of the predicate calculus, the transformation of a first-order wff into clause normal form, J. A. Robinson's resolution principle, semantic resolution, significant resolution heuristics (search strategies), and paramodulation. The application of theorem-proving techniques to the problem of program correctness is also briefly discussed.",1975
A Taxonomy for the Syntax and Semantics of Programs,"A comparison is made between the competence and performance models of Chomsky and the two elements of syntax proposed by McCarthy. These concepts are then expanded over utterances in natural languages and programs from programming languages, to cover the syntax of a program, the underlying abstract elements of the program, the meaning of the program, the algorithm which the program represents, the program itself and the statement of the problem being solved.",1975
Empirical Comparisons of Virtual Environment Displays,"There are many different visual display devices used in virtual environment (VE) systems. These displays vary along many dimensions, such as resolution, field of view, level of immersion, quality of stereo, and so on. In general, no guidelines exist to choose an appropriate display for a particular VE application. Our goal in this work is to develop such guidelines on the basis of empirical results. We present two initial experiments comparing head-mounted displays with a workbench display and a foursided spatially immersive display. The results indicate that the physical characteristics of the displays, users' prior experiences, and even the order in which the displays are presented can have significant effects on performance.",2001
Using Pinch Gloves(TM) for both Natural and Abstract Interaction Techniques in Virtual Environments,"Usable three-dimensional (3D) interaction techniques are difficult to design, implement, and evaluate. One reason for this is a poor understanding of the advantages and disadvantages of the wide range of 3D input devices, and of the mapping between input devices and interaction techniques. We present an analysis of Pinch Gloves™ and their use as input devices for virtual environments (VEs). We have developed a number of novel and usable interaction techniques for VEs using the gloves, including a menu system, a technique for text input, and a two-handed navigation technique. User studies have indicated the usability and utility of these techniques.",2001
Solving Galerkin Approximations to Nonlinear Two-Point Boundary Value Problems by a Globally Convergent Homotopy Method,"The Chow-Yorke algorithm, i.e., a homotopy method that has been proved globally convergent j problems, certain classes of zero finding and nonlinear programming problems, and two-point, boundary based on shooting, finite differences, and spline collocation. The method is numerically stable and has been applied to a wide range of  practical engineering problems. Here the Chow-Yorke algorithm is proved globally c Galerkin approximations to nonlinear two-point boundary value problems. Several numerical implement are briefly described, and computational results are presented for a fairly difficult, magnet-hydrodynamic problem.  Key words. homotopy method, Chow-Yorke algorithm, globally convergent,, two-point boundary value method, finite element method, nonlinear equations",1986
Static and Dynamic Software Quality Metric Tools,"The ability to detect and predict poor software quality is of major importance to software engineers, managers, and quality assurance organizations.  Poor software quality leads to increased development costs and expensive maintenance.  With so much attention on exacerbated budgetary constraints, a viable alternative is necessary.  Software quality metrics are designed for this purpose.  Metrics measure aspects of code or PDL representations, and can be collected and used throughout the life cycle [RAMC85].",1990
Multidisciplinary Design Optimization with Quasiseparable Subsystems,"Numerous hierarchical and nonhierarchical decomposition strategies for the optimization of large scale systems, comprised of interacting subsystems, have been proposed. With a few exceptions, all of these strategies have proven theoretically unsound. This paper focuses on a class of quasiseparable optimization problems narrow enough for a rigorous decomposition theory, yet general enough to encompass many large scale engineering design problems. The subsystems for these problems involve local design variables and global system variables, but no variables from other subsystems. The objective function is a sum of a global system criterion and the subsystems’ criteria. The essential idea is to give each subsystem a budget and global system variable values, and then ask the subsystems to independently maximize their constraint margins. Using these constraint margins, a system optimization then adjusts the values of the system variables and subsystem budgets. The subsystem margin problems are totally independent, always feasible, and could even be done asynchronously in a parallel computing context. An important detail is that the subsystem tasks, in practice, would be to construct response surface approximations to the constraint margin functions, and the system level optimization would use these margin surrogate functions. The purpose of the present paper is to present a decomposition strategy in a general context, provide rigorous theory justifying the decomposition, and give some simple illustrative examples.",2002
Adding Value to the Software Development Process: A Study in Independent Verification and Validation,"Independent Verification and Validation (IV&V) is best viewed as an overlay process supporting a software development effort.  While the touted benefits of a properly managed IV&V activity are many, they specifically emphasize: (a) early fault detection, (b) reduced time to remove faults, and (c) a more robust end-product.  This paper outlines a study funded by NASA-Langley Research Center to examine an existing IV&V methodology, and to confirm (or refute) the touted beneficial claims.  In the study two distinct development groups are established, with only one having an IV&V contingent.  Both groups are tasked to produce a software product using the same set of requirements.  Within each phase of the development effort, fault detection and fault removal data are recorded.  An analysis of that data reveals that the group having the IV&V contingent: (a) detected errors earlier in the software development process, and (b) on the average, required significantly less time to remove those faults.  Moreover, a test for operational correctness further reveals that the system developed by the group having the IV&V component was substantially more robust than the one produced by the other development group.  A statistical analysis of our results is also provided to establish significance.",1998-08-01
Quantum Computing Applied to Optimization,"Optimization problems represent a class of problems that can be time consuming to solve and very complex. In this paper, a quantum algorithm for solving optimization problems is proposed. The algorithm utilizes the encoding scheme from genetic algorithms to encode the problem and then uses Grover's unitary transformation to seek out a solution. The efficiency of the algorithm depends on the length of the chromosome or the coded variable. As a simple example the satisfiability problem, an NP-complete problem, is examined using the algorithm and the time complexity of solving this problem is greatly improved. The traveling salesman and minimum spanning tree problems are also briefly discussed.",1999-09-01
Globally Convergent Homotopy Algorithms for the Combined H-squared/ H-to Infinity Model Reduction Problem,"The problem of finding a reduced order model, optimal in the H-squared sense, to a given system model is a fundamental one in control system analysis and design.  The addition of a H-to infinity constraint to the H-squared optimal model reduction problem results in a more practical yet computationally more difficult problem.  Without the global convergence of probablity-one homotopy methods the combined H-squared/H-to infinity model reduction problem is difficult to solve. Several approaches based on homotopy methods have been proposed.  The issues are the number of degrees of freedom, the well posedness of the finite dimensional optimization problem, and the numerical robustness of the resulting homotopy algorithm.  Homotopy algorithms based on several formulations -- input normal, Ly, Bryson, and Cannon's 2 x 2 block parametrization -- are developed and compared here.",1993-06-01
A Comparison of Two Methods for Soft Boolean Operator Interpretation in Information Retrieval,"Information retrieval systems generally are given Boolean logic queries by users or search intermediaries, in order that an efficient and effective search for relevant documents can be automatically carried out. Previous work with an extended interpretation of Boolean queries has shown that a dramatic improvement in search effectiveness results.  Using the P-norm to compute distance from the ideal points in a multi-dimensional space of truth values leads to best results .1 values are on the order of 1 to 4.  Other schemes besides the P-norm approach have been proposed in recent years.  This paper describes experimental studies aimed at evaluating one family of such methods.  In particular, a parameterized fuzzy-logic approach is contrasted with the norm interpretation. Regression analysis supports expected results of parameter settings and gives further insight into why the P-norm scheme is superior.",1986
Parallel Solution of Generalized Symmetric Tridiagonal Eigenvalue Problems on Shared Memory Multiprocessors,"This paper describes and compares two methods for solving a generalized eigenvalue problem , where T and S are both real symmetric and tridiagonal, and S is positive definite, and the target architecture is a shared memory multiprocessor. One method can be viewed as a generalization of the treeql algorithm of Dongarra and Sorensen [1987]. The second algorithm is a straightforward parallel extension of the bisection/inverse iteration algorithm treeps of Lo, Philippe, and Sameh [1987]. The two methods are representative of families of algorithms of quite different character. We illustrate and compare sequential and parallel performance of the two approaches with numerical examples.",1992
A Genetic Algorithm Approach to Cluster Analysis,"A common problem in the social and agricultural sciences is to find clusters in experi- mental data; the standard attack is a deterministic search terminating in a locally optimal clustering. We propose here a genetic algorithm (GA) for performing cluster analysis. GAs have been used profitably in a variety of contexts in which it is either impractical or impossible to directly solve for a globally optimal solution to complex numerical problems. In the present case, our GA clustering tech- nique attempted to maximize a variance-ratio (VR) based goodness-of-fit criterion defined in terms of external cluster isolation and internal cluster homogeneity. Although our GA-based clustering algorithm cannot guarantee to recover the cluster solution that exhibits the global maximum of this fitness function, it does explicitly work toward this goal (in marked contrast to existing clustering al- gorithms, especially hierarchical agglomerative ones such as Ward’s method). Using both constrained and unconstrained simulated datasets, Monte Carlo results showed that in some conditions the ge- netic clustering algorithm did indeed surpass the performance of conventional clustering techniques (Ward’s and K-means) in terms of an internal (VR) criterion. Suggestions for future refinement and study are offered.",1998-08-01
A New View on What Limits TCP/IP Throughput in Local Area Networks,"This paper presents experimental results on what limits local area network throughput at the application program level for two popular transport protocols, TCP and UDP, using two application program interfaces, Berkeley sockets and System V transport layer interface.  The sensitivity of application-level performance to the choice of host computer speed and background load on the host are also studied.  Two sets of measurements are discussed.  The first contains macroscopic measurement of throughput on 68020, 68030, 68040, 80386, SPARC, and MIPS R2000 and R3000 based computers over a single Ethernet subnet.  The second presents a detailed timing analysis using a hardware monitor of a TCP/IP implementation for PC architecture computers.  Previous studies implicate memory copying, checksumming, and the operating system interface as the major overheads in TCP/IP, rather than the time required to execute the protocol itself.  This study indicates that these factors are secondary when the sender and receiver are closely matched in speed; rather the primary bottleneck is the TCP flow control mechanism.  TCP flow control becomes closer to optimal as the degree of speed mismatch increases.  We draw conclusions on why window mechanisms should be augmented by rate based flow control in the new generation of high data rate networks.",1991
Hodiex: A Sixth Order Accurate Method for Solving Elliptical PDEs,"This paper describes a method for discretizing general linear two dimensional elliptical PDEs with variable coefficients, Lu=g, which achieves high orders of accuracy on an extended range of problems.  The method can be viewed as an extension of the ELLPACK6 discretization module HODIE (""High Order Difference Approximation with Identity Expansion""), which achieves high orders of accuracy on a more limited class of problems.  We thus call this method HODIEX.  An advantage of HODIEX methods, including the one described here, is that they are based on a compact 9-point stencil which yields linear systems with a smaller bandwidth than if a larger stencil were used to achieve higher accuracy.",1993
A Framework for Assessing the Adequacy and Effectiveness of software development methodologies,"A framework for evaluating software development methodologies is described.  Beginning with the issue of what constitutes a methodology, an evaluation procedure is presented that exploits defined linkages among objectives, principles, and attributes intrinsic to the software development process.  The evaluation of software development methodologies reflects an assessment structured by the needs, process, and product sequence for system development.  Linkages are defined from recognized software engineering sources, providing a foundation for both objective and subjective ""analysis"" of a given methodology.  Application of the evaluation procedure to two software development methodologies currently used by the United States Navy establishes feasibility of the framework and reveals the inherent power of the evaluation procedure.",1991
Software Quality Measurement: Validation of a Foundational Approach,"This report discusses the first year findings of a proposed three year investigation effort that focuses on the assessment and prediction of software quality.  The research exploits fundamental linkages among software engineering Objectives, Principles and Attributes (the OPA framework). Process, code and document quality indicators are presented relative to the OPA framework, with elaboration on their individual roles in assessing and predicting software quality.  The synthesis of an Ada code analyzer is discussed as well as proposed complementary tools comprising an automated data collection and report generation system.  Key Words and Phrases: Simulation quality assessment and prediction; software engineering objectives, principles and attributes; software quality indicators, process indicators, code indicators, document quality indicators.",1991
A Robust Variable Order Facet Model for Image Data,"It is a common practice in computer vision and image processing to convolve rectangular constant coefficient windows with digital images to perform local smoothing and derivative estimation for edge detection and other purposes.  If all data points in each image window belong to the same statistical population, this practice is reasonable and fast. But, as is well known, constant coefficient window operators produce incorrect results if more than one statistical population is present within a window, e.g., if a gray level or gradient discontinuity is present.  This paper shows one way to apply the theory of robust statistics to the data smoothing and derivative estimation problem.  A robust window operator is demonstrated that preserves gray level and gradient discontinuities in digital images as it smooths and estimates derivatives.",1991
UNITY Algorithms for Detecting Stable and Non-Stable Termination Conditions in Time Warp Parallel Simulations,"This paper extends work done by Abrams and Richardson on the topic of implementing global termination conditions and collecting output measures in parallel simulation.  Concentrating on the Time Warp method for parallel simulation, an improved categorization scheme for termination conditions is presented, as well as algorithms written in UNITY notation to implement each category.",1990
Email-Set Visualization: Facilitating Re-Finding in Email Archives,"In this paper we describe ESVT – EmailSet Visualization Tool, an email archive tool that provides users a visualization to re-find and discover information in their email archive. ESVT is an end-to-end email archive tool that can be used from archiving a user’s email messages to visualizing queries on the email archive. We address email archiving by allowing import of email messages from an email server or from a standard existing email client. The central idea in ESVT’s visualization, an “email-set”, is the set of emails that are the result of a query on a user’s email archive. ESVT provides a multiple email-set view - visualization of multiple email-sets on a time axis. In addition, each email set can be individually visualized based on person and time axis, using the single email-set view. Query logs, individual email visualization, multiple email set visualization provide rich contextual cues, thus enabling end users to deal with email overload and re-find past email which otherwise wouldn’t be discovered easily.",2007-05-01
Update on Multirate Timestepping Methods for Hyperbolic Conservation Laws,This paper constructs multirate time discretizations for hyperbolic conservation laws that allow different timesteps to be used in different parts of the spatial domain. The proposed family of discretizations is second order accurate in time and has conservation and linear and nonlinear stability properties under local CFL conditions. Multirate timestepping avoids the necessity to take small global timesteps (restricted by the largest value of the Courant number on the grid) and therefore results in more efficient algorithms. Numerical results obtained for the advection and Burgers equations confirm the theoretical findings.,2007-03-01
Representing Polyhedra: Faces are Better than Vertices,"In this paper, we investigate the reconstruction of planar-faced polyhedra given their spherical dual representation.  We prove that the spherical dual representation is unambiguous for all genus 0 polyhedra and that a genus 0 polyhedra can be uniquely reconstructed in polynomial time.  We also prove that when the degree of the spherical dual representation is at most four, the representation is unambiguous for polyhedra at any genus.  The first result extends the well known result that a vertex or face connectivity graph represents a polyhedron unambiguously when the graph is triconnected and planar in the case of planar-faced polyhedra.  The second result shows that when each face of a polyhedron of arbitrary genus has at most four edges, the polyhedron can be recontructed uniquely.  This extends the result that a polyhedron can be uniquely reconstructed when each face of the polyhedron is triangular.  To obtain this result, we prove that the 4-dimension hypercube, a classic example of ambiguity in the wire frame representation scheme, is unambiguous when the same connectivity graph is viewed as the spherical dual representation of a polyhedron and thus that faces are a better representation than vertices.  A result of the reconstruction algorithm is that high level features of the polyhedron are naturally extracted.  Both of our results explicitly use the fact that the faces of the polyhedron are planar.  We conjecture that the spherical dual representation is unambiguous for polyhedra of any genus.",1990
High Dimensional Homotopy Curve Tracking on a Shared Memory Multiprocessor,"Results are reported for a series of experiments involving numerical curve tracking on a shared memory parallel computer.  Several algorithms exist for finding zeros or fixed points of nonlinear systems of equations that are globally convergent for almost all starting points, i.e., with probability one.  The essence of all such algorithms is the construction of an appropriate homotopy map and then the tracking of some smooth curve in the zero set of this homotopy map.  HOMPACK is a mathematical software package implementing globally convergent homotopy algorithms with three different techniques for tracking a homotopy zero curve, and has separate routines for dense and sparse Jacobian matrices.  A parallel version of HOMPACK is implemented on a shared memory parallel computer with various levels and degrees of parallelism (e.g., linear algebra, function and Jacobian matrix evaluation), and a detailed study is presented for each of these levels with respect to the speedup in execution time obtained with the parallelism, the time spent implementing the parallel code and the extra memory allocated by the parallel algorithm.",1991
Homotopy Approaches to the H2 Reduced Order Model Problem,"The optimal projection approach to solving the H2 reduced order model problem produces two coupled, highly nonlinear matrix equations with rank conditions as constraints. The algorithms proposed herein utilize probability-one homotopy theory as the main tool. It is shown that there is a family of systems (the homotopy) that make a continuous transformation from some initial system to the final system.  With a carefully chosen initial system all the systems along the homotopy path will be asymptotically stable, controllable and observable. One method, which solves the matrix equations in their original form, requires a decomposition of the projection matrix using the Drazin inverse of a matrix.  An effective algorithm for computing the derivative of the projection matrix that involves solving a set of Sylvester equations is given.  Several strategies for choosing the homotopy maps and the starting points (initial systems) are discussed and compared, in the context of some reduced order model problems from the literature.  Numerical results are included for ten test problems, of sizes 2 through 17.",1991
Analysis of Function Component Complexity for Hypercube Homotopy Algorithms,"Probability-one homotopy algorithms are a class of methods for solving nonlinear systems of equations that are globally convergent from an arbitrary starting point with probability one. The essence of these homotopy algorithms is the construction of a homotopy map ra and the subsequent tracking of a smooth curve g in the zero set  ra-1 (0) of ra . Tracking the zero curve g requires repeated evaluation of the map ra  its n x (n + 1) Jacobian matrix Dra , and numerical linear algebra for calculating the kernel of Dra . This paper analyzes parallel homotopy algorithms on a hypercube, briefly reviewing the numerical linear algebra, several communication topologies and problem decomposition strategies, and concentrating on function component complexity, problem size, and the effect of different component complexity distributions. These parameters interact in complicated ways, but some general principles can be inferred based on empirical results. Implications for developing reliable and efficient parallel mathematical software packages for this problem area are also discussed.",1991
Representing Polyhedra: Faces Are Better than Vertices,"In this paper, we investigate the reconstruction of planar-faced polyhedra given their spherical dual representation. We prove that the spherical dual representation is unambiguous for all genus 0 polyhedra and that a genus 0 polyhedron can be uniquely reconstructed in polynomial time. We also prove that when the degree of spherical dual representation is at most four, the representation is unambiguous for polyhedra of any genus.",1992
Terminating Parallel Simulations,"This paper presents two different general solutions to the simulation termination problem for various simulation protocols and parallel architectures. Implementation of both solutions with conservative-synchronous, optimistic, and conservative-asynchronous simulation protocols as well as synchronous shared-memory, distributed, and asynchronous shared memory architectures is discussed.",1992
FAST-INV: A Fast Algorithm for building large inverted files,"Inverted files are widely used in building bibliographic and other types of retrieval systems.  In order to investigate the utility of advance information retrieval methods for improving access to large online library catalogs, it was necessary to extend the SMART system in a variety of ways.  One particular problem was to develop a fast method to produce an inverted file from hundreds of thousands of (partial) MARC records.  The FAST-INV software was developed in 1986, taking advantage of the large primary memories available on modern computers and the order inherent in the input data.  Using the new algorithm, processing in primary memory for N basic data elements has time complexity O(N), and processing of files that will not fit in primary memory can be accomplished in a fixed number of passes.  Performance studies show this approach to be (at least) an order of magnitude faster than commonly used techniques.  It is hoped that these findings will be of interest to database providers and will help them reduce costs relating to the building of inverted files, as we have been doing for the last five years.",1991
The Application of Concurrent Object-Oriented Techniques to Reactive Systems,"A language and system model combining concurrency, abstract communication and an object orientation offers several advantages in the design and implementation of large-scale reactive systems. An object-orientation captures the abstraction and variety of entities inhabiting the environment while the autonomy of actual entities is clearly reflected by expressions of concurrency in the program of the reactive system. Abstract communication is necessary to achieve data sharing among heterogeneous systems. However, attempts to design and implement a paradigm unifying these three features have encountered unexpected difficulties. These difficulties include the interference between concurrency control (synchronization) and inheritance, inadequate application-oriented communication abstractions, the absence of a useful model of exception handling for concurrent object-oriented applications, and the lack of a powerful and useful theory of computation based on asynchrony.",1992
Toward Parallel Mathematical Software for Elliptic Partial Differential Equations,"Three approaches to parallelizing important components of the mathematical software package ELLPACK are considered: an explicit approach using compiler directives available only on the target machine, an automatic approach using an optimizing and parallelizing precompiler, and a two-level approach based on extensive use of a set of low level computational kernels. The focus is on shared memory architectures.  Each approach to parallelization is described in detail, along with a discussion of the effort involved. Performance on a test problem, using up to sixteen processors of a Sequent Symmetry S81, is reported and discussed.  Implications for the parallelization of a broad class of mathematical software are drawn.",1991
Diffusion and Wave Propagation in Cellular Automaton Models of Excitable Media,"In this paper, the authors (1) examine general ""masks"" as discrete approximations to the diffusion equation, showing how to calculate the diffusion coefficient from the elements of the mask; (2) combine the mask with a thresholding operation to simulate the propagation of waves in excitable media, showing that (for well-chosen masks) the waves obey a linear ""speed-curvature"" relation with slope given by the predicted diffusion coefficient; and (3) assess the utility of different masks in terms of computational efficiency and adherence to a linear speed-curvature relation.",1991
An Artificial Intelligence Environment for Information Retrieval Research,"The CODER (COmposite Document Expert/Extended/Effective Retrieval) project is a multi-year effort to investigate how best to apply artificial intelligence methods to increase the effectiveness of information retrieval systems. Particular attention is being given to analysis and representation of heterogeneous documents, such as electronic mail digests or messages, which vary widely in style, length, topic,and structure. In order to ensure system adaptability and to allow reconfiguration for controlled experimentation, the project has been designed as a moderated expert system. This thesis covers the design problems involved in providing a unified architecture and knowledge representation scheme for such a system, and the solutions chosen for CODER. An overall object-oriented environment is constructed using a set of message-passing primitives based on a modified Prolog call paradigm. Within this environment is embedded the skeleton of a flexible expert system, where task decomposition is performed in a knowledge-oriented fashion and where subtask managers are implemented as members of a community of experts. A three-level knowledge representation formalism of elementary data types, frames, and relations is provided, and can be used to construct knowledge structures such as terms, meaning structures, and document interpretations. The use of individually tailored specialist experts coupled with standardized blackboard modules for communication and control and external knowledge bases for maintenance of factual world knowledge allows for quick prototyping, incremental development and flexibility under change. The system as a whole is structured as a set of communicating modules, defined functionally and implemented under UNIX^TM using sockets and the TCP/IP protocol for communication. Inferential modules are being coded in MU-Prolog; non-inferential modules are being prototyped in MU-Prolog and will be re-implemented as needed in C++.",1988
ReSHAPE: A Framework for Dynamic Resizing and Scheduling of Homogeneous Applications in a Parallel Environment,"Applications in science and engineering often require huge computational resources for solving problems within a reasonable time frame. Parallel supercomputers provide the computational infrastructure for solving such problems. A traditional application scheduler running on a parallel cluster only supports static scheduling where the number of processors allocated to an application remains fixed throughout the lifetime of execution of the job. Due to the unpredictability in job arrival times and varying resource requirements, static scheduling can result in idle system resources thereby decreasing the overall system throughput. In this paper we present a prototype framework called ReSHAPE, which supports dynamic resizing of parallel MPI applications executed on distributed memory platforms. The framework includes a scheduler that supports resizing of applications, an API to enable applications to interact with the scheduler, and a library that makes resizing viable. Applications executed using the ReSHAPE scheduler framework can expand to take advantage of additional free processors or can shrink to accommodate a high priority application, without getting suspended. In our research, we have mainly focused on structured applications that have two-dimensional data arrays distributed across a two-dimensional processor grid. The resize library includes algorithms for processor selection and processor mapping. Experimental results show that the ReSHAPE framework can improve individual job turn-around time and overall system throughput.",2007
Tracing the Efficient Curve for Multi-objective Control-Structure Optimization,"A recently developed active set algorithm for tracing parametrized optima is adapted to multi-objective optimization.  The algorithm traces a path of Kuhn-Tucker points using homotopy curve tracking techniques, and is based on identifying and maintaining the set of active constraints. Second order necessary optimality conditions are used to determine nonoptimal stationary points on the path.  In the bi-objective optimization case the algorithm is used to trace the curve of efficient solutions (Pareto optima). As an example, the algorithm is applied to the simultaneous minimization of the weight and control force of a ten-bar truss with two collocated sensors and actuators, with some interesting results.",1991
Trusting Remote Users… Can They Identify Problems Without Involving Usability Experts?,"Based on our belief that critical incident data, observed during usage and associated closely with specific task performance are the most useful kind of formative evaluation data for finding and fixing usability problems, we developed a Remote Usability Evaluation Method (RUEM) that involves real users self-reporting critical incidents encountered in real tasks performed in their normal working environments without the intervention of evaluators. In our exploratory study we observed that users were able to identify, report, and rate the severity level of their own critical incidents with only brief training.",2007
A Comparison of Three Cursor Control Devices on a Cursor Control Benchmark Task,"A number of past studies have compared performance of cursor control devices.  Conclusions regarding the ""best"" device for a particular application are difficult to draw because the tasks used in previous comparisons have differed from one another and have not included all of the factors affecting performance with the devices.  The present study used a target acquisition task whose components were derived from analyzing the cursor control device movements of users while performing actual tasks.  The components of the task were target size, target distance, direction, and mode (i.e., point-and-click vs. drag).  Three cursor control devices, cursor keys, mouse, and trackball, were each used by 12 subjects to perform the task. Results indicate the mouse and trackball perform similarly in most cases. The cursor keys always performed more poorly than the other devices. Hence, the mouse and trackball are preferable to the cursor keys based on objective measure of user performance.  In conditions where cursor keys are necessary, careful attention should be paid to aspects of the interface listed above in order to attain the highest level of performance possible.",1991
Globally Convergent Homotopy Methods for the DC Operating PointProblem,"Accurate and efficient computer simulation of a proposed design for an integrated circuit (""chip"") is essential because of the difficulty and expense of building prototypes for such devices.  The transistors and diodes in such circuits are modeled with nonlinear equations, hence simulation of circuits requires the solution of systems of nonlinear equations involving hundreds or even thousands of variables.  This paper discusses the application of probability-one homotopy methods to various systems of nonlinear equations which arise in circuit simulation.  The so-called ""coercivity conditions"" which are required for such methods are proved using concepts from circuit theory.  The theoretical claims of global convergence for such methods are substantiated by experiments with a collection of examples which have proved difficult for commercial simulation packages which do not use homotopy methods.  Moreover, by careful design of the homotopy equations, the performance of the homotopy methods can be made quite reasonable.",1990
Device Independent Perspective Volume Rendering Using Octrees,"This paper describes a reasonably fast, space efficient algorithm for volume rendering. The algorithm is device independent since it is written as an X Windows client. It makes no graphics calls to dedicated graphics hardware, but allows the X server to take advantage of such hardware when it exists. It can be run on any machine that supports X windows, from an IBM-PC to a high-end graphics workstation. It produces a perspective projection of the volume, since perspective projections are generally easier to interpret than parallel projections. The algorithm uses progressive refinement to give the user a quick view of the dataset and how it is oriented. If a different orientation or dataset is desired, the user may interrupt the rendering process. Once the desired dataset and position have been determined, the progressive refinement process continues and the image improves in quality until the greatest level of detail is displayed.",1992-05-01
Adapting Protocols to Massively Interconnected Systems,"This paper describes ongoing research focused on two critical problems posed by the interconnection of a massive number of computer systems. The interconnection may be achieved through wide area or local area networks. The two problems considered in this research are as follows: (1) performance analysis of the protocols used in an internetwork connecting thousands to millions of nodes, and (2) application development in a massively distributed, heterogeneous environment  where components implemented in different programming languages must be integrated and/or reused.  The performance analysis problem is addressed by employing large-scale parallel simulation, extended finite state machines and objected-oriented simulation techniques.  The approach to solving the application development problem is based on an environment which exploits the synergism between object-oriented programming and layered communication protocols (specifically, OSI).",1991-05-01
Edge-Packing by Isomorphic Subgraphs,"Maximum G Edge-Packing (EPackG) is the problem of finding the maximum number of edge-disjoint isomorphic copies of a fixed guest graph G in a host graph H.  This paper investigates the computational complexity of edge-packing for planar guests and planar hosts.  Edge-packing is solvable in linear time when G is a 2-path and H is arbitrary, or when H is outerplanar and G is either a 3-cycle or a k-star.  Edge-packing is solvable in polynomial time when both G and H are trees.  Edge-packing is NP-complete when H is planar and G is either a cycle or a tree with ≥3 edges.  The approximability of EPackG is considered.  A strategy for developing polynomial-time approximation algorithms for planar hosts is exemplified by a linear-time approximation algorithm for EPack k-star that finds an edge-packing of size at least one-half optimal.  Finally, EPackG is shown not to be in the complexity class Max SNP, though it is Max SNP-hard for G a k-star, ≥3.",1991
Multi-Objective Control-Structures Optimization Via Homotopy Methods,"A recently developed active set algorithm for tracking parametrized optima is adapted to multi-objective optimization.  The algorithm traces a path of Kuhn-Tucker points using homotopy curve tracking techniques, and is based on identifying and maintaining the set of active constraints.  Second order necessary optimality conditions are used to determine nonoptimal stationary points on the path.  In the bi-objective optimization case the algoritm is used to trace the curve efficient solutions (Pareto optima).  As an example, the algorithm is applied to the simultaneous minimization of the weight and control force of a ten-bar truss with two collocated sensors and actuators, with some interesting results.",1990
Geometric Performance Analysis of Mutual Exclusion: The Model Solution,"This paper presents an analytic solution to progress graphs used for performance analysis.  It derives the exact sequence of blocking and running times experienced by two processes sharing mutually exclusive, reusable resources.  A novel application of Dijkstra's progress graphs yields the complex relationship between the waiting times at each synchronization point.  The problem of solving progress graphs is formulated in terms of finding the minimum solution of each of a set of Diophantine equations.  An algorithm is presented to find all steady state behaviors involving blocking that emerge from any initial condition.",1990
Third Generation Cellular Automaton for Modeling Excitable Media,"This paper introduces a new cellular automaton model of excitable media with improved treatments of (1) diffusion and wave propagation, and (2) slow dynamics of the recovery variable.  The automaton is both computationally efficient and faithful to the underlying partial differential equations.",1991
Intelligent Fusion of Structural and Citation-Based Evidence for Text Classification,"This paper investigates how citation-based information and structural content (e.g., title, abstract) can be combined to improve classification of text documents into predefined categories. We evaluate different measures of similarity, five derived from the citation structure of the collection, and three measures derived from the structural content, and determine how they can be fused to improve classification effectiveness. To discover the best fusion framework, we apply Genetic Programming (GP) techniques. Our empirical experiments using documents from the ACM digital library and the ACM classification scheme show that we can discover similarity functions that work better than any evidence in isolation and whose combined performance through a simple majority voting is comparable to that of Support Vector Machine classifiers.",2004
Capturing Truthiness: Mining Truth Tables in Binary Datasets,"We introduce a new data mining problem: mining truth tables in binary datasets. Given a matrix of objects and the properties they satisfy, a truth table identifies a subset of properties that exhibit maximal variability (and hence, complete independence) in occurrence patterns over the underlying objects. This problem is relevant in many domains, e.g., bioinformatics where we seek to identify and model independent components of combinatorial regulatory pathways, and in social/economic demographics where we desire to determine independent behavioral attributes of populations. Besides intrinsic interest in such patterns, we show how the problem of mining truth tables is dual to the problem of mining redescriptions, in that a set of properties involved in a truth table cannot participate in any possible redescription. This allows us to adapt our algorithm to the problem of mining redescriptions as well, by first identifying regions where redescriptions cannot happen, and then pursuing a divide and conquer strategy around these regions. Furthermore, our work suggests dual mining strategies where both classes of algorithms can be brought to bear upon either data mining task. We outline a family of levelwise approaches adapted to mining truth tables, algorithmic optimizations, and applications to bioinformatics and political datasets.",2007-02-01
Remote Usability Testing Methods a la Carte,"Although existing lab-based formative usability testing is frequently and effectively applied to improving usability of software user interfaces, it has limitations that have led developers to turn to remote usability evaluation methods (RUEMs) to collect formative usability data from daily usage by real users in their own real-world task environments.                                The enormous increase in Web usage, where users can be isolated and the network and remote work settingbecome intrinsic parts of usage patterns, is strong motivation for supplementing lab-based testing with remote usability evaluation methods. Another significant impetus for remote evaluation is the fact that the iterative development cycle for any software, Web application or not, does not end with initial deployment. We review and informally compare several approaches to remote usability evaluation with respect to quantity and quality of data collected and the effort to collect the data.",2007
Temporal Aspects of Tasks in the User Action Notation,"The need for communication among a multiplicity of cooperating roles in user interface development translates into the need for a common set of interface design representation techniques.  The important difference between design of the interaction part of the interface and design of the interface software calls for representation techniques with a behavioral view---a view that focuses on user interaction rather than on the software. The User Action Notation (UAN) is a user- and task-oriented notation that describes physical (and other) behavior of the user and interface as they perform a task together.  The primary abstraction of the UAN is a user task.  The work reported here addresses the need to identify temporal relationships within user task descriptions and to express explicitly and precisely how designers view temporal relationships among those tasks.  Drawing on simple temporal concepts such as events in time and preceding and overlapping of time intervals, we identify basic temporal relationships among tasks: sequence, waiting, repeated disjunction, order independence, interruptibility, one-way interleavability, mutual interleavability, and concurrency.  The UAN temporal relations, through the notion of modal logic, offer an explicit and precise representation of the specific kinds of temporal behavior that can occur in asynchronous user interaction, without the need to detail all cases that might result.",1991
Document Quality Indicators: A Framework for Assessing Documentation Adequacy,"This paper presents case study results of a research effort funded by the Naval Surface Warfare Systems (NSWC) at Dahlgren, Virginia.  The investigation focuses on assessing the adequacy of project documentation based on an identified taxonomic structure relating documentation characteristics.  Previous research in this area has been limited to the study of isolated characteristics of documentation and English prose, without considering the collective contributions of such characteristics.  The research described in this paper takes those characteristics, adds others, and establishes a well-defined approach to assessing the ""adequacy"" of software documentation.  The identification of Document Quality Idicators (DQIs) provide the basis for the assessment procedure.  DQIs are hierarchically defined in terms of document Qualities, Factors that refine Qualities, and Quantifiers that provide for the measurement of Factors.",1990
A Workload Emulator Architecture for Distributed Systems,"This report presents the initial design of a general purpose workload emulator that emulates workstations, terminals, and communication equipment attached to host computers and servers in a distributed computing environment.  The workload emulator can be programmed to model the time dependent behavior of a workload for performance evaluation of distributed systems.",1991
"The Changing Relationships between Computing Centers, Other Campus Units and University Administrations","This is the text of the presentation to the opening session of the 1986 ACM/SIGUCCS annual meeting held in St. Louis, MO.  The author presents some concerns regarding the widening gap between university computing centers, as service organizations administered by one particular subgroup of their clients. and those they are committed to serve.",1986-03-01
Designing Explicit Numeric Input Interfaces for Immersive Virtual Environments,"User interfaces involving explicit control of numeric values in immersive virtual environments have not been well studied. In the context of designing three-dimensional interaction techniques for the creation of multiple objects, called cloning, we have developed and tested a dynamic slider interface (D-Slider) and a virtual numeric keypad (VKey). Our cloning interface requires precise number input because it allows users to place objects at any location in the environment with a precision of 1/10 unit. The design of the interface focuses on feedback, constraints, and expressiveness. Comparative usability studies have shown that the newly designed user interfaces were easy to use, effective, and had a good quality of interaction. We describe a working prototype of our cloning interface, the iterative design process for D-Slider and V-Key, and lessons learned. Our interfaces can be re-used for any virtual environment interaction tasks requiring explicit numeric input.",2004
Design and Display of Enhancing Information in Desktop Information-Rich Virtual Environments: Challenges and Techniques,"Information-Rich Virtual Environments (IRVEs) have been described as environments in which perceptual information is enhanced with abstract (or symbolic) information such as text, numbers, images, audio, video, or hyperlinked resources. Desktop VE applications present the same information design and layout challenges as immersive VEs, but in addition, they may also be integrated with external windows or frames commonly used in desktop interfaces. This paper enumerates design approaches for the display of enhancing information both internal and external to the virtual world?s render volume. Using standard web-based software frameworks, we explore a number of implicit and explicit spatial layout methods for the display and linking of abstract information, especially text. Within the virtual environment view, we demonstrate both Heads-Up-Displays and encapsulated scenegraph behaviors we call Semantic Objects. For desktop displays, which support information display venues external to the scene, we demonstrate the linking and integration of the scene with web browsers and the Snap-Together visualization a system. Finally, we describe the application of these techniques in the PathSim Visualizer, an IRVE interface for the biomedical domain. These design techniques are relevant for instructional and informative interfaces for a wide variety of desktop VE applications.",2003
Linda-LAN: A Controlled Parallel Processing Environment,"An investigation is performed on a controlled parallel processing environment based upon the Linda paradigm, a conceptually simple programming and operational framework. The environment efficiently and effectively manages system resources via a centralized control sub-system. A detailed overview of the environment executing on a local area network is presented along with analysis on its control sub-system. The control sub-system's execution characteristics of stability and scalability are also substantiated.",1992
Documentation Production under Next Generation Technologies,"This report describes the development of the Abstraction Refinement Model as a basis for linking the development and maintenance tasks in software systems. Documentation is critical in both efforts, and the reliance on development documentation during maintenance is characterized by the model and through a characterization of the development documentation requirement stipulated under DoD-STD2167A. The Abstraction Refinement Model enables a coherent characterization of the reverse engineering requirements generally caused by a faulty or inadequately documented development process. Within the context of the model, the Automated Documentation Design System (ADDS) is characterized, and the system is evaluated with regard to its current capabilities versus future potential. A set of recommendations regarding ADDS concludes the report.",1989
Integrated Access to a Large Medical Literature Database,"Project INCARD (INtegrated CARdiology Database) has adapted the CODER (COmposite Document Expert/effective/extended Retrieval) system and LEND (Large External Network object oriented Database) to provide integrated access to a large collection of bibliographic citations, a full text document in cardiology, and a large thesaurus of medical terms.  CODER is a distributed expert-based information system that incorporates techniques from artificial intelligence, information retrieval, and human-computer interaction to support effective access to information and knowledge bases.  LEND is an object-oriented database which incorporates techniques from information retrieval and database systems to support complex objects, hypertext/hypermedia and semantic network operations efficiently with very large sets of data. LEND stores the CED lexicon, MeSH thesaurus, MEDLARS bibliographics records on cardiology, and the syllabus for the topic Abnormal Human Biology (Cardiology Section) taught at Columbia University.  Together, CODER/LEND allow efficient and flexible access to all of this information while supporting rapid ""intelligent"" searching and hypertext-style browsing by both novice and expert users. This report gives statistics on the collections, illustrations of the system's use, and details on the overall architecture and design for Project INCARD.",1991
Designing Attention-Centric Notification Systems: Five HCI Challenges,"Through an examination of the emerging domain of cognitive systems, with a focus on attention-centric cognitive systems used for notification, this document explores the human-computer interaction challenges that must be addressed for successful interface design. This document asserts that with compatible tools and methods, user notification requirements and interface usability can be abstracted, expressed, and compared with critical parameter ratings; that is, even novice designers can assess attention cost factors to determine target parameter levels for new system development. With a general understanding of the user tasks supported by the notification system, a designer can access the repository of design knowledge for appropriate information and interaction design techniques (e.g., use of color, audio features, animation, screen size, transition of states, etc), which have analytically and empirically derived ratings. Furthermore, usability evaluation methods, provided to designers as part of the integrated system, are adaptable to specific combinations of targeted parameter levels. User testing results can be conveniently added back into the design knowledge repository and compared to target parameter levels to determine design success and build reusable HCI knowledge. This approach is discussed in greater detail as we describe five HCI challenges relating to cognitive system development: (1) convenient access to basic research and guidelines, (2) requirements engineering methods for notification interfaces, (3) better and more usable predictive modeling for pre-attentive and dual-task interfaces, (4) standard empirical evaluation procedures for notification systems, and (5) conceptual frameworks for organizing reusable design and software components. This document also describes our initial work toward building infrastructure to overcome these five challenges, focused on notification system development. We described LINK-UP, a design environment grounded on years of theory and method development within HCI, providing a mechanism to integrate interdisciplinary expertise from the cognitive systems research community. Claims allow convenient access to basic research and guidelines, while modules parallel a lifecycle development iteration and provide a process for requirements engineering guided by this basic research. The activities carried out through LINK-UP provide access to and interaction with reusable design components organized based on our framework. We think that this approach may provide the scientific basis necessary for exciting interdisciplinary advancement through many fields of design, with notification systems serving as an initial model. A version of this document will appear as chapter 3 in the book Cognitive Systems: Human Cognitive Models in Systems Design edited by Chris Forsythe, Michael Bernard, and Timothy Goldsmith resulting from a workshop led by the editors in summer 2003. The authors are grateful for the input of the workshop organizers and conference attendees in the preparation of this document.",2005
"FATODE: A Library for Forward, Adjoint, and Tangent Linear Integration of ODEs","FATODE is a FORTRAN library for the integration of ordinary differential equations with direct and adjoint sensitivity analysis capabilities.  The paper describes the capabilities, implementation, code organization, and usage of this package.  FATODE implements four families of methods -- explicit Runge-Kutta for nonstiff problems and fully implicit Runge-Kutta, singly diagonally implicit Runge-Kutta, and Rosenbrock for stiff problems.  Each family contains several methods with different orders of accuracy; users can add new methods by simply providing their coefficients. For each family the forward, adjoint, and tangent linear models are implemented. General purpose solvers for dense and sparse linear algebra are used; users can easily incorporate problem-tailored linear algebra routines.  The performance of the package is demonstrated on several test problems.  To the best of our knowledge FATODE is the first publicly available general purpose package that offers forward and adjoint sensitivity analysis capabilities in the context of Runge Kutta methods. A wide range of applications are expected to benefit from its use; examples include parameter estimation,  data assimilation, optimal control, and uncertainty quantification.",2011-11-01
A Framework for the Expansion of Spatial Features Based on Semantic Footprints,"Geographic feature expansion is a common task in Geographic Information Systems (GIS). Identifying and integrating geographic features is a challenging task since many of their spatial and non-spatial properties are described in different sources. We tackle this expansion problem by defining semantic footprints as a measure of similarity among features. Furthermore, we propose three quantifiers of semantic similarity: spatial, dimensional, and ontological affinity. We show how these measures dilute, concentrate, harden, or concede the feature space, and provide useful insights into the semantic relationships of the spatial entities. Experiments demonstrate the effectiveness of our approach in semantically associating the most appropriate spatial features.",2011
Digital Library Education in Computer Science Programs,"In an effort to identify the “state of the art” in digital library education in computer science (CS) programs, we analyzed CS courses on digital libraries and digital library-related topics. Fifteen courses that mention digital libraries in the title or short description were identified; of these, five are concerned with digital libraries as the primary topic of the course. The readings from these five courses were analyzed further, in terms of their authors and the journals in which they were published.",2007
New Results for the Minimum Weight Triangulation Problem,"The current best polynomial time approximation algorithm produces a triangulation that can be O(log n) times the weight of the optimal triangulation. We propose an algorithm that triangulates a set P of n points in a plane in O(n3) time and that never does worse than the greedy triangulation. We investigate issues of local optimality pertaining to known triangulation algorithms and suggest an interesting new approach to studying triangulation algorithms. We restate the minimum weight triangulation problem as a graph problem and show the NP-hardness of a closely related graph problem. Finally, we show that the constrained problem of computing the minimum weight triangulation, given a set of points in a plane and enough edges to form a triangulation, is NP-hard. These results are an advance towards a proof that the minimum weight triangulation problem is NP-hard.",1992
The Cost of Terminating Optimistic Parallel Discrete-Event Simulations,"In a previous paper, we proposed seven algorithms for mechanically adding an arbitrary termination condition to a conservative-synchronous, non-terminating parallel simulation. Informal arguments about the performance of each algorithm were made, and the arguments were confirmed through measurement for four of these algorithms. The benchmark used was the simulation of a Torus network using the Bounded Lag protocol on a shared memory multiprocessor. In this paper, we report on the performance of the simulation of the same Torus network benchmark with the same four termination conditions using an optimistic protocol on a message-passing multiprocessor. We also report on the performance of a colliding pucks simulation with three additional termination conditions.",1992
Optimization and Blending of Composite Laminates Using Guide based Genetic Algorithms,"Composite panel structure optimization is commonly decomposed into panel optimization subproblems, with specified local loads, resulting in manufacturing incompatibilities between adjacent panel designs. A new method proposed here for constructing globally blended panel designs uses a parallel decomposition antithetical to that of earlier work. Rather than performing concurrent panel genetic optimizations, a single genetic optimization is conducted for the entire structure with the parallelism solely within the fitness evaluations. A guide based genetic algorithm approach is introduced to exclusively generate and evaluate valid globally blended designs, utilizing a simple master-slave parallel implementation, implicitly reducing the size of the problem design space and increasing the quality of discovered local optima.",2003
A Practical Method to Estimate Information Content in the Context of 4D-Var Data Assimilation. I: Methodology,"Data assimilation obtains improved estimates of the state of a physical system by combining imperfect model results with sparse and noisy observations of reality. all observations used in data assimilation are equally valuable. The ability to characterize the usefulness of different data points is important for analyzing the effectiveness of the assimilation system, for data pruning, and for the design of  future sensor systems.  This paper focuses on the four dimensional variational (4D-Var) data assimilation framework. Metrics from information theory are used to quantify the contribution of observations to decreasing the uncertainty with which the system state is known. We establish an interesting relationship between different information-theoretic metrics and the variational cost function/gradient under Gaussian linear assumptions. Based on this insight we derive an ensemble-based computational procedure to estimate the information content of various observations in the context of 4D-Var. The approach is illustrated on a nonlinear test problem. In the companion paper (Singh et al., 2012a) the methodology is applied to a global chemical data assimilation experiment.",2012-03-01
Algebraic Methods in Prolog Programming,"We discuss some difficulties of coding and invoking functions implemented by predicates and we propose a number of conceptual tools for overcoming these difficulties.  We use an algebraic model which supports strategies for designing complete, parsimonious, and terminating functions. We describe a translation scheme for converting these functions into predicates and we prove a number of properties of this transformation. Our investigation provides an algebraic interpretation of the cut and raises two issues of lack of orthogonality in the Prolog programming languages. We outline practical tools, based on our ideas, which simplify substantially some significant steps of the design and use of certain Prolog predicates. We extend current work on the translation from algebraic specifications and term rewriting systems to logic programs along two",1989-09-01
The Simulation Model Development Environment: An Overview,"The purpose of this paper is to provide an overview of the Simulation Model Development Environment (SMDE) that has been under development since 1983. The SMDE architecture is composed of four layers: (0) Hardware and Operating System, (1) Kernel SMDE, (2) Minimal SMDE, and (3) SMDEs. Following the incremental development software engineering life cycle, SMDE software components are identified. Guided by the principles enunciated by the Conical Methodology, evolutionary prototyping approaches have been used to develop the following minimal SMDE tools: Premodels Manager, Assistance Manager, Model Generator, Model Analyzer, Model Translator, and Model Verifier. The Model Generator has been the most critically important tools, and five prototypes have been developed.",1992
An OAI-based Digital Library Framework for Biodiversity Information Systems,"Biodiversity information systems (BISs) involve all kinds of heterogeneous data, which include ecological and geographical features. However, available information systems offer very limited support for managing such data in an integrated fashion, and integration is often based on geographic coordinates alone. Furthermore, such systems do not fully support image content management (e.g., photos of landscapes or living organisms), a requirement of many BIS end-users. In order to meet their needs, these users - e.g., biologists, environmental experts - often have to alternate between distinct biodiversity and image information systems to combine information extracted from them. This cumbersome operational procedure is forced on users by lack of interoperability among these systems. This hampers the addition of new data sources, as well as cooperation among scientists. The approach provided in this paper to meet these issues is based on taking advantage of advances in Digital Library (DL) innovations to integrate networked collections of heterogeneous data. It focuses on creating the basis for a biodiversity information system under the digital library perspective, combining new techniques of content-based image retrieval and database query processing mechanisms. This approach solves the problem of system switching, and provides users with a flexible platform from which to tailor a BIS to their needs.",2004
Usability of Tablet PC as a Remote Control Device for Biomedical Data Visualization,"Interaction through multi-platform user interfaces (MPUIs), is increasingly being used in battlefield applications, telemedicine, classroom education, and engineering applications. Some of the uniqueness of these non-traditional user interfaces lies in the division of information between multiple displays and the remote control of information (e.g., using one computer to control a remote display). We preformed an exploratory study to compare three different setups: a Tablet PC with a traditional desktop, a Tablet PC with a large screen display (LSD) combination, and a desktop computer. The results showed that many users preferred the familiar Microsoft Windows widgets available on the Tablet PC; users often had difficulty generalizing their experiences' when using the Tablet PC; and the form factor of the Tablet PC worked in favor and against the user in different conditions. Our results indicate that while there are yet problems to overcome, generic handheld devices can make highly effective remote controls for virtual environments.",2004
Instructional Footprinting: A Model for Exploiting Concurrency through Instructional Decomposition and Code Motion,"In many languages, the programmer is provided the capability of communicating, through the use of function calls, with other separate, independent processes. This capability can be as simple as a service request made to the operating system, or more advanced as Tuple Space operations specific to a Linda programming system. The problem with such calls, however, is that they block while waiting for data or information to be returned. This synchronous nature and lack of concurrency can be avoided by initiating the request for data earlier in the code and retrieving the returned data later when it is needed. In order to facilitate this concurrency of processing, an instructional footprint model is developed which formally describes movement of instructions. This paper presents research findings that entail the development of the instructional footprint model, an algorithmic framework in which to exploit concurrency in programming languages, and some preliminary results from applying the instructional footprint model.",1992
"An Interactive Environment for Dialogue Development: Its Design, Use and Evaluation","The Author's Interactive Didogue Environment (AIDE) of the Dialogue Management System is an integrated set of direct manipulation tools used by a dialogue author to design and implement human-computer interfaces without writing source code. This paper presents the conceptua! dialogue transaction model upon which AIDE is based, describes AIDE, and illustrates how a dialope author develops an interface using AIDE. A preliminary empirical evaluation of the use of AIDE versus the use of a programming language to implement an interface shows very encouraging results.",1985
Integrating TQM and SEI Process Assessment,"This paper describes a methodology for assessing the software process (both development and maintenance) used by an organization. The assessment methodology integrates the principles of Total Quality Management and the work of the Software Engineering Institute. Assessment results in a well-understood, well-documented, quantitatively evaluated software process. The methodology utilizes four steps: investigation, modeling, data collection, and analysis of both process content and process output. Process improvements are determined by analysis results.",1992
Storytelling Security: User-Intention Based Traffic Sanitization,"Malicious software (malware) with decentralized communication infrastructure, such as peer-to-peer botnets, is difficult to detect. In this paper, we describe a traffic-sanitization method for identifying malware-triggered outbound connections from a personal computer. Our solution correlates user activities with the content of outbound traffic. Our key observation is that user-initiated outbound traffic typically has corresponding human inputs, i.e., keystroke or mouse clicks. Our analysis on the causal relations between user inputs and packet payload enables the efficient enforcement of the inter-packet dependency at the application level. We formalize our approach within the framework of protocol-state machine. We define new application-level traffic-sanitization policies that enforce the inter-packet dependencies. The dependency is derived from the transitions among protocol states that involve both user actions and network events. We refer to our methodology as storytelling security. We demonstrate a concrete realization of our methodology in the context of peer-to-peer file-sharing application, describe its use in blocking traffic of P2P bots on a host. We implement and evaluate our prototype in Windows operating system in both online and offline deployment settings. Our experimental evaluation along with case studies of real-world P2P applications demonstrates the feasibility of verifying the inter-packet dependencies. Our deep packet inspection incurs overhead on the outbound network flow. Our solution can also be used as an offline collect-and-analyze tool.",2010-12-01
Integration of VT ETD-db with Banner,"The Electronic Thesis and Dissertation database (ETD-db) was developed at Virginia Tech by Digital Library and Archives for the VT Graduate School and the Networked Digital Library of Theses and Dissertations (NDLTD).  The software is freely available and over 100 universities worldwide have implemented the ETD-db system.  One drawback of the system is the dependence on user keyed data.  At Virginia Tech, like most other universities, there is an administrative database that could provide much of this information.  The Banner Administrative System is the central administration system at Virginia Tech.   Banner’s underlying database software is from Oracle.  This paper will demonstrate how the ETD-db can be seamlessly integrated with an Oracle database or more specifically the Banner Administrative System, to improve the integrity of the data for ETDs.",2008
Generic Environment for Interactive Experiments,No abstract available.,1983
Shortening Time-to-Discovery with Dynamic Software Updates for Parallel High Performance Applications,"Despite using multiple concurrent processors, a typical high performance parallel application is long-running, taking hours, even days to arrive at a solution. To modify a running high performance parallel application, the programmer has to stop the computation, change the code, redeploy, and enqueue the updated version to be scheduled to run, thus wasting not only the programmer’s time, but also expensive computing resources. To address these inefficiencies, this article describes how dynamic software updates can be used to modify a parallel application on the ﬂy, thus saving the programmer’s time and using expensive computing resources more productively. The net effect of updating parallel applications dynamically reduces their time-to-discovery metrics, the total time it takes from posing a problem to arriving at a solution. To explore the benefits of dynamic updates for high performance applications, this article takes a two-pronged approach. First, we describe our experience in building and evaluating a system for dynamically updating applications running on a parallel cluster. We then review a large body of literature describing the existing state of the art in dynamic software updates and point out how this research can be applied to high performance applications. Our experimental results indicate that dynamic software updates have the potential to become a powerful tool in reducing the time-to-discovery metrics for high performance parallel applications.",2009
On Consistency Properties of Discrete Adjoint Linear Multistep Methods,"In this paper we analyze the consistency properties of discrete adjoints of linear multistep methods. Discrete adjoints are very popular in optimization and control since they can be constructed automatically by reverse mode automatic differentiation. The consistency analysis reveals that the discrete linear multistep adjoints are, in general, inconsistent approximations of the adjoint ODE solution along the trajectory. However, the discrete adjoints at the initial time (and therefore the discrete adjoint gradients) converge to the adjoint ODE solution with the same order as the original linear multistep method. Discrete adjoints inherit the zero-stability properties of the forward method. Numerical results confirm the theoretical findings.",2007
On the discrete adjoints of adaptive time stepping algorithms,"We investigate the behavior of adaptive time stepping numerical algorithms under the reverse mode of automatic differentiation (AD). By differentiating the time step controller and the error estimator of the original algorithm, reverse mode AD generates spurious adjoint derivatives of the time steps. The resulting discrete adjoint models become inconsistent with the adjoint ODE, and yield incorrect derivatives. To regain consistency, one has to cancel out the contributions of the non-physical derivatives in the discrete adjoint model. We demonstrate that the discrete adjoint models of one-step, explicit adaptive algorithms, such as the Runge--Kutta schemes, can be made consistent with their continuous analogs using simple code modifications. Furthermore, we extend the analysis to cover second order adjoint models derived through an extra forward-mode differentiation of the discrete adjoint code. Two numerical examples support the mathematical derivations.",2008-04-01
Collaborating on Affinity Diagrams Using Large Displays,"Gathering and understanding user requirements is an  essential part of design. Techniques like affinity  diagramming are useful for gathering and  understanding user data but have shortcomings such as  the difficulty to preserve the diagram after its creation,  problems during the process such as searching for  notes, and loss of shared awareness. We propose an  early prototype that solves problems in the process of  creating an affinity diagram and enhances it using a  large screen display in combination with individual  PDAs.",2008
DNESYS--An Expert System for Automatic Extraction of Drainage Networks from Digital Elevation,"The determination of drainage networks and drainage basins is one of the more tedious yet important uses of topographic maps, and geographic information systems are now used extensively as a manual aid to facilitate that task. However, the wide availability of digital elevation maps has stimulated attempts to automate the process even further.  In this paper, the problems that arose in earlier programs to map drainage systems are analyzed in detail. An expert system called the Drainage Network Extraction System (DNESYS) is described which uses both local operators and global reasoning to extract drainage networks and ridge lines. A stream representation called a parameterized directed graph (PDG) is constructed to model a drainage system. The construction of a model begins with an initial pixel labeling procedure. Then a network tracing and property measurement procedure converts the 2-D low-level labeling information into a symbolic database for high -level processing. By applying Dempster- Shafer evidence theory, evidence collection and uncertain reasoning are performed against the DNESYS knowledge-base that contains the drainage system model and the organized expert knowledge. By discarding erroneous information and supplying missing information, DNESYS produces a complete PDG which can be converted into the final drainage system.",1988
Making a Case for Hci: Comparing Materials for Case-based Teaching,"This paper investigates case-based methods for bridging the conflicting goals of providing both topic coverage and practical experience in teaching humancomputer interaction (HCI). Case-based methods rely on design, development, and testing material for existing or on-going projects to provide details on difficult decisions and solutions in the interface creation process. Cases are touted as alternatives to real experience (immensely important in HCI courses) and provide rich environments for computer science instruction. We evaluate benefits and limitations of five types of case materials--contemporary articles, professionally prepared cases, familiar interfaces, ongoing development projects, and incomplete information (jigsaw)--to probe how they should be structured and approached by an HCI instructor. Through an experience that assessed case-based activities in an undergraduate HCI course, we determined tradeoffs relating to student participation, preparation characteristics, and short- and long-term learning outcomes. Professionally prepared case materials provided a sense of comfort in preparation, but were surpassed by familiar interfaces in terms of performance and post-use feedback, leading to recommendations for case materials development and adoption of case-based learning for HCI. Based on our results, we can make several conclusions that should influence selection and development of materials for casebased pedagogy, and we illustrate the need for structured case creation processes that can be performed conjointly with system development efforts.",2004
Termination and Output Measure Generation in Optimistic and Conservative-Synchronous Parallel Simulations,"This paper proposes algorithms to stop parallel discrete event simulations using arbitrary termination conditions and to collect output measures. We show that the time complexity of termination can be higher than that of the underlying simulation; therefore termination can reduce or even preclude speed-up. We propose simulation protocol independent algorithms solving the termination and generation problems. Implementations of the algorithms for conservative-synchronous and optimistic protocols are presented. The predicted and measured increase in real time required to execute a time warp and a bounded lag simulation equipped with our termination algorithms is presented. The chief conclusion is that when the time required for each evaluation of the termination condition exceeds the mean time to execute an event, termination can dominate the simulation running time.",1992
An Engineering Model of Subcognition,"Symbolic and neural modes are discussed from the point of view of implementing the ""intuitive"" processes which mediate real-time control skills. The engineering preference is to transcend rather than emulate those limitations which are inherent in the brain's subsymbolic modes of operation. A main payoff lies in recovering, from performance data, symbolic representations of intuitive real-time skills. Practical illustrations are reviewed using recent case studies.",1992
Robust Nonlinear Least Squares Estimation Using the Chow- Yorke Homotopy Method,No abstract available.,1983
Analysis of a Nonhierarchical Decomposition Algorithm,"Large scale optimization problems are tractable only if they are somehow decomposed. Hierarchical decompositions are inappropriate for some types of problems and do not parallelize well. Sobieszczanski-Sobieski has proposed a nonhierarchical decomposition strategy for nonlinear constrained optimization that is naturally parallel. Despite some successes on engineering problems, the algorithm as originally proposed fails on simple two dimensional quadratic programs. This paper carefully analyzes the algorithm for quadratic programs, and suggests a number of modifications to improve its robustness.",1992
Modeling the Goodput of TCP NewReno in Cellular Environments,"In this paper, we present an analytical model that characterizes TCP NewReno's goodput as a function of round-trip time, average time duration between handoffs, average number of packets reordered during a handoff, and the congestion window threshold.  In cellular networks, the effective packet-loss probability for a flow experiencing handoffs is not exactly equal to the physical-layer packet-loss probability; it also depends on the frequency of handoffs and the number of packets that may arrive out of order at the receiver due to handoffs.   With the emergence of technologies such as WiMax and Ultra Mobile Broadband (UMB), understanding the effect of handoffs and packet reordering on the goodput of TCP becomes very important for the designers of next generation cellular networks.  Existing TCP throughput models cannot be used to understand the precise effect of handoffs and the resultant packed reordering on TCP's goodput.  In this paper, we present a model of TCP NewReno goodput that captures the effect of handoffs.  We validate the model by performing actual file transfers between different hosts that are connected by a router which emulates the wireless environment with handoff events and packet reordering.",2007
Accelerating Data-Serial Applications on Data-Parallel GPGPUs: A Systems Approach,"The general-purpose graphics processing unit (GPGPU) continues to make significant strides in high-end computing by delivering unprecedented performance at a commodity price. However, the many-core architecture of the GPGPU currently allows only data-parallel applications to extract the full potential out of the hardware. Applications that require frequent synchronization during their execution do not experience much performance gain out of the GPGPU. This is mainly due to the lack of explicit hardware or software support for inter thread communication across the entire GPGPU chip.  In this paper, we design, implement, and evaluate a highly-efficient software barrier that synchronizes all the thread blocks running on an ofﬂoaded kernel on the GPGPU without having to transfer execution control back to the host processor. We show that our custom software barrier achieves a three-fold performance improvement over the existing approach, i.e., synchronization via the host processor.  To illustrate the aforementioned performance benefit, we parallelize a data-serial application, specifically an optimal sequence-search algorithm called Smith-Waterman (SWat), that requires frequent barrier synchronization across the many cores of the nVIDIA GeForce GTX 280 GPGPU. Our parallelization consists of a suite of optimization techniques — optimal data layout, coalesced memory accesses, and blocked data decomposition. Then, when coupled with our custom software-barrier implementation, we achieve nearly a nine-fold speed-up over the serial implementation of SWat. We also show that our solution delivers 25 faster on-chip execution than the na¨ıve implementation.",2008
A Special Topics Course on Personal Information Management,"Personal Information Management (PIM) is an important emerg- ing area of study in Computer Science and Information Systems. During the Spring of 2006, we offered a special topics course in PIM at Virginia Tech. This paper presents some motivation of why studying PIM is important, the goals for the course, some sam- ple material from the course, and a few student evaluations. The paper presents in detail an activity called “Day in the Life of My Information” that resulted in an interesting experience from both, educational and research points of view.",2006-12-01
The Implementation of Four Conceptual Frameworks for Simulation Modeling in High-Level Languages,"This is a tutorial paper on how to implement a simulation model in a high-level programming language (e.g ., C, Pascal, FORTRAN) by using the following conceptual frameworks (also called world views, simulation strategies, and formalisms): (1) event scheduling, (2) activity scanning, (3) three-phase approach, and (4) process interaction. Implementation details under each conceptual framework are covered in a high level without being concerned about execution efficiency. The purpose is to reveal the characteristics of the four conceptual frameworks so that the programmer can select and implement one to achieve certain model quality characteristics such as maintainability, reusability, and execution efficiency. A problem is defined for use as an example for illustrating the concepts throughout the paper.",1988
Modeling Multigrain Parallelism on Heterogeneous Multi-core Processors,"Heterogeneous multi-core processors integrate conventional processing cores with computational accelerators. To maximize performance on these systems, programs must exploit multiple dimensions of parallelism simultaneously, including task-level and data-level parallelism. Unfortunately, parallel program designs with multiple dimensions of parallelism today are ad hoc, resulting in performance that depends heavily on the intuition and skill of the programmer. Formal techniques are needed to optimize parallel program designs. We propose a parallel computational model for steering multi-grain parallelization in heterogeneous multi-core processors. Our model accurately predicts the execution time and scalability of a program using multiple conventional processors and accelerators.  The model reveals optimal degrees of multi-dimensional, task-level and data-level concurrency in parallel programs. We use the model to derive mappings of two full computational phylogenetics applications on multi-processors featuring the IBM Cell Broadband Engine.",2007
Finding Computer Science Syllabi on the World Wide Web,"Syllabi contain information useful to students, faculty, and many other people, and given the ubiquity of the WWW many schools are now putting their syllabi online for these people and the general public to access. Even though these syllabi may be available, they might be hard to find. This means that faculty, students, and anyone else who might have an interest in viewing those syllabi might find it useful to be able to browse a collection of reliable syllabi. To build a collection of reliable syllabi it is necessary to find those syllabi on the WWW but this is made easy with a tool like the Google Web API. Once the syllabi are found on the Web it is necessary to examine those syllabi and look for desired characteristics to be sure they are desired syllabi. The syllabi that contain the desired characteristics are kept and the rest are discarded. This elimination process can be accomplished using a tool like a classification tree, more specifically, tools like the Orange Data Mining Library and C4.5. This paper describes the process of finding syllabi on the WWW using the Google Web API, retrieving those syllabi using Python, and filtering them using the Orange Data Mining Library and C4.5 so that a reliable set of syllabi can be constructed.",2006-11-01
Extraction of Lines And Regions From Grey Tone Line Drawing Images,"An algorithm is described for extracting lines from  grey level digitizations of industrial drawings. The algorithm  is  robust,  non iterati e,  and sequential,  and includes  procedures  for differentiating shaded areas from lines.  Examples are given for complex regions of a typical mechanical  drawing.",1983
A First Step Towards Nuance-Oriented Interfaces for Virtual Environments,"Designing usable interfaces for virtual environments (VEs) is not a trivial task. Much of the difficulty stems from the complexity and volume of the input data. Many VEs, in the creation of their interfaces, ignore much of the input data as a result of this. Using machine learning (ML), we introduce the notion of a nuance that can be used to increase the precision and power of a VE interface. An experiment verifying the existence of nuances using a neural network (NN) is discussed and a listing of guidelines to follow is given. We also review reasons why traditional ML techniques are difficult to apply to this problem.",2001
A Fortran 90 Genetic Algorithm Module for Composite Laminate Structure Design,"The design of the stacking sequence for a composite laminate involves a set of discrete variables (ply material and ply orientation), and is thus well-suited to genetic algorithms for design optimization.  Such algorithms have typically been custom-designed in FORTRAN 77 to suit specific optimization problems.  Fortran 90 is a modern, powerful language with features that support important programming concepts, including those used in object-oriented programming.  The Fortran 90 genetic algorithm module is used to define genetic data types, the functions which use these data types, and to provide a general framework for solving composite laminate structure design problems.  The language's support of abstract data types is used to build genetic structures such as populations, subpopulaions, individuals, chromosomes, and genes, and these data types are combined and manipulated by module subroutines.  The use of abstract data types and long variable names makes the code useful and easily understood, while dynamic memory allocation makes the module flexible enough to be used in design problems of varying size and specification.",1998-08-01
Documentation Production under Next Generation Technologies,"This paper describes the development of the Abstraction Refinement Model as a basis for linking the development and maintenance tasks in software systems.  Documentation is critical in both efforts, and the reliance on development documentation during maintenance is characterized by the model and through a characterization of the development documentation requirement stipulated under DoD-STD-2167A. The Abstraction Refinement Model enables a coherent characterization of the reverse engineering requirements generally caused by a faulty or inadequately documented development process.  Within the context of the model, the Automated Document Design System (ADDS) is characterized, and the system is evaluated with regard to its current capabilities versus future potential.  A set of recommendations regarding ADDS concludes the report.",1989
Distributed Control Parallelism in Multidisciplinary Aircraft Design,"Multidisciplinary design optimization (MDO) for large-scale engineering problems poses many challenges (e.g., the design of an efficient concurrent paradigm for global optimization based on disciplinary analyses, expensive computations over vast data sets, etc.) This work focuses on the application of distributed schemes for massively parallel architectures to MDO problems, as a tool for reducing computation time and solving larger problems.  The specific problem considered here is configuraton optimization of a high speed civil transport (HSCT), and the efficient parallelization of the embedded paradigm for reasonable design space identification.  Two distributed dynamic load balancing techniques (random polling and global round robin with message combining) and two necessary termination detection schemes (global task count and token passing) were implemented and evaluated in terms of effectiveness and scalability to large problem sizes and a thousand processors.  The effect of certain parameters on execution time was also inspected.  Empirical results demonstrated stable performance and effectiveness for all schemes, and the parametric study showed that the selected algorithmic parameters have a negligible effect on performance.",1998-09-01
The Cost of Numerical Integration in Statistical Decision-theoretic Methods for Robust Design Optimization,"The Bayes principle from statistical decision theory provides a conceptual framework for quantifying uncertainties that arise in robust design optimization. The difficulty with exploiting this framework is computational, as it leads to objective and constraint functions that must be evaluated by numerical integration. Using a prototypical robust design optimization problem, this study explores the computational cost of multidimensional integration (computing expectation) and its interplay with optimization algorithms. It concludes that straightforward application of standard off-the-shelf optimization software to robust design is prohibitively expensive, necessitating adaptive strategies and the use of surrogates.",2006
Online Enlightenment: A Phidget Notification System for Online Status,"This paper describes a physical device that presents online presence information in a semi-public space. The device uses a map metaphor to represent a set of connected labs, showing online instant messenger status for members of the community. Device users can combine information from the device with information from the physical environment to identify unfamiliar lab members, determine human-to-human interaction strategies, and plan meetings. The paper reports on design decisions that were considered in creating the device, supplying rationale for decisions that were made. In particular, we focus on how people integrate physical information from the world and virtual information from this (and similar) devices in the environment, reflecting on ways in which this type of device can improve communication and enhance community. We describe four envisioned usage scenarios for the device, with early feedback from people who work in the space and whose information is displayed on the device.",2004
Implications of Natural Categories for Natural Language Generation,"Psychological research has shown that natural taxonomies contain a distinguished or basic level. Adult speakers use the names of these categories most frequently and can list a large number of attributes for them. They typically can list many attributes for superordinate categories and list few additional attributes for subordinate categories. Because natural taxonomies are important to human language, their use in natural language processing systems appears well founded. In the past, however, most AI systems have been implemented around uniform taxonomies in which there is no distinguished level. It has recently been demonstrated that natural taxonomies enhance natural language processing systems by allowing selection of appropriate category names and by providing the means to handle implicit focus. We propose that additional benefits from the use of natural categories can be realized in multi-sentential connected text generation systems.  After discussing the psychological research on natural taxonomies that relates to natural language processing systems, the use of natural categorizations in current natural language processing systems is presented. We then describe how natural categories can be used in multiple sentence generation systems to allow the selection of appropriate category names, to provide the mechanism to help determine salience to aid in the selection of discourse schema. to provide for the shallow modeling audience expertise, and to increase the efficiency of taxonomy inheritance.",1989
An Optimal Boundary to Quadtree Conversion Algorithm,"An algorithm is presented for converting a boundary representation for an image to its region quad-tree representation. Our algorithm is designed for operation on the linear quadtree representation, although it can easily be modified for the traditional pointer-based quadtree representation. The algorithm is a two phase process that first creates linear quadtree node records for each of the border pixels. This list of pixels is then sorted by locational code. The second processing phase fills in the nodes interior to the polygons by simulating a traversal of the corresponding pointer-based quadtree. Three previous algorithms [Same80, Mark85a, Atki86] have described similar conversion routines requiring time complexity of O(n.B)for at least one of the two phases, where B is the number of boundary pixels and n is the depth of the final tree for a 2"" x 2"" image. A fourth algorithm [Webb84] can perform the border construction of this conversion in time O(n+B) with the restriction that the polygon must be positioned at constrained locations in the image space. Our algorithm requires time O(n + B) for the second phase, which is optimal. The first phase can be performed using the algorithm of [Webb84] for total conversion time of O(n + B) with constrained location, or in time O(B log B) using a simple sort to order the border pixels with no restriction in polygon location.",1989
Requirements for a Software Maintenance Methodology,"Software maintenance, although widely recognized as the most costly period in the life of a system, is given only passing consideration in life-cycle models.  An extensive literature review shows the relationship between the development and maintenance phases to be ignored to a large extent.  The Abstraction Refinement Model (ARM) describes the dependency of software maintenance on the quality of the documentation and depicts the adaptive and perfective maintenance forms as relying on earlier design and requirements documents to a greater degree than corrective and preventative maintenance.  The ARM is effective in laying the foundations for a software maintenance methodology, particularly in explaining the role of reverse engineering.  Coupling the ARM with the Objectives/Principles/Attributes procedure for the evaluation for software development methodologies proves effective in drawing the contrast with maintenance requirements. which are specifically identified for further study and assessment.",1990
Formally Reasoning About and Automatically Generating Sequential and Parallel Simulations,"This paper proposes a methodology to automate the construction of simulation programs within the context of a simulation support environment. The methodology starts with a simulation model specification in the form of a set of coupled state transition systems. The paper provides a mechanical method of mapping the transition systems first into a set of formal assertions, permitting formal verification of the transition systems, and second into an executable program. UNITY, a computational model and proof system suitable for development of parallel and distributed programs through step-wise refinement of specifications, is used as the specification and program notation. The methodology provides a means to independently verify the correctness of the transition systems: one can specify properties formally that the model should obey and prove them as theorems using the formal specification.",1992
Guidelines for Selecting and Using Simulation Model Verification Techniques,"There is a lack of sufficient understanding and realization of the importance of simulation model verification in the simulation community. The demands placed on the software which serves as a computer-executable representation of a simulation model are increasing. In the field of software engineering, there is an abundance of software verification techniques that are applicable for simulation model verification. This paper is intended to reduce the communication gap between the software engineering and simulation communities by presenting software verification techniques applicable for simulation model verification in a terminology understandable by a simulationist. A taxonomy of verification techniques is developed to guide the simulationist in selecting and using such techniques. Characteristics, advantages, and disadvantages of verification techniques under each category are described.",1989-08-01
Correlating Workload Characteristics to Performance Metrics for theCray X-MP/Y-MP,"Workload characterization is essential for performance evaluation studies.  For multiprocessor supercomputers, this characterization usually consists of program measurements from uniprocessor execution (e.g., average vector length, percentage vectorization, etc.).  There is no consistent, quantitative correlation between such characteristics and performance metrics across different programs.  We present a methodology for defining and measuring characterization parameters for both single and multi-processor executions.  Using several production codes, we show for the Cray X-MP/Y-MP that these characterization parameters correlate consistently to observed performance metrics across different programs. Moreover, the correlation allows the identification of bottlenecks in system architecture that limit performance.  Presentation slides from the Fourth SIAM Conference on Parallel Processing for Scientific Computing, December, 1989.",1989
Measurement and Evaluation of Complex Navy Systems Designs,"The purpose of this paper is to present a multifaceted approach to the measurement and evaluation of complex Navy system designs with embedded realtime mission critical characteristics.  The approach advocates the use of a visual simulation model, constructed in the Visual Simulation Support Environment (VSSE), representing the design so as to achieve dynamic measurement of the design.  A knowledge-based approach is proposed for the independent evaluation of hundreds of design indicators.  The Objectives/Principles/Attributes (OPA) framework is used as the underlying structure for the measurement and evluation of all three major aspects of a system design: project, process, and product.",1993
Designing for Seamless Task Migration in MPUIs: Bridging Task-Disconnects,"Today, the proliferation of mobile computing has changed the work environment forever. As a consequence, users are forced to orchestrate a complex interaction between multiple devices, moving data and information back and forth, to accomplish their tasks. Users trudge out USB key drives, remote desktop software, e-mail and network file storage in an attempt to mitigate this orchestration. We refer to this break from the task at hand as task-disconnect. Task-disconnect represents the break in continuity that occurs when a user attempts to accomplish his or her tasks using more than one device. Our objective is to study how software can bridge this task-disconnect, enabling users to seamlessly transition their tasks among their devices. We present the theory, definition, and discussion of task-disconnect; our approach towards bridging this disconnect; and our prototype application that was built to be used across the desktop computer and the Tablet PC platforms. We then describe our subjective evaluation to measure the effectiveness of the prototype in bridging the task-disconnect. We then conclude with the results and insights gained from our evaluation.",2004
Optimal Load Allocation for Parallel and Distributed Processing,"The problem of minimizing the execution completion time of a given total load, to be partitioned into interacting tasks and allocated to run on a generalized model of a heterogeneous centralized or distributed multiprocessing system, is examined. The problem is formulated as a nonlinear, nonconvex nonseparable, minimax resource- allocation, continuous, mathematical programming problem. It is assumed that the quantitative functional dependence of the individual processor execution time on the partitioned load allocation is known and specified in analytical or graphical formats of a fairly general nature with no a priori restrictions of differentiability, monotonicity, convexity, and unimodality commonly imposed in previous investigations of the problem. A Theorem stating the necessary and sufficient condition for minimum concurrent processing completion time is derived. The new result represents an analytical breakthrough applicable to a wide class of hitherto analytically unsolved optimization problems in various application disciplines. The derivation starts from a precise representation of the parallel execution time and proceeds through an exact analysis that does not resort to the simplifying assumptions or analytical approximations found in analogous previous investigations. The load partitioning is considered to vary over a continuum, thus allowing the achievement of ideal optimization through one-step repartitioning of the given load. The optimization procedure determines the set of all global minimum points of the completion time function as well as all its local minima, thus allowing further lexicographic optimization and suboptimal trade-offs. The conditions of the Theorem admit a straightforward graphical interpretation which facilitates its implementation and readily extends its applicability to empirically or simulationally determined models of the system.",1989-04-01
"Implementing an Intelligent Retrieval System: The CODER System, Version 1.0","For individuals requiring interactive access to online text, information storage and retrieval systems provide a way to retrieve desired documents and/or text passages. The CODER (COmposite Document Expert/effective/extended Retrieval) system is a testbed for determining how useful various artificial intelligence techniques are for increasing the effectiveness of information storage and retrieval systems. The system, designed previously, has three components: an analysis subsystem for analyzing and storing document contents, a central spine for manipulations and storage of world and domain knowledge, and a retrieval subsystem for matching user queries to relevant documents.  This thesis discusses the implementation of the retrieval subsystem and portions of the spine and analysis subsystem. It illustrates that logic programming, specifically with the Prolog language, is suitable for development of an intelligent information retrieval system.  Furthermore, it shows that system modularity provides a flexible research testbed, allowing many individuals to work on different parts of the system which may later be quickly integrated. The retrieval subsystem has been implemented in a modular fashion so that new approaches to information can be easily compared to more traditional ones.  A powerful knowledge representation language, a comprehensive lexicon, and individually tailored experts using standardized blackboard modules for communication and control allowed rapid prototyping, incremental development and ready adaptability to change.  The system executes on a DEC VAX 11/785 running ULTRIX (TM), a variant of 4.2 BSD UNIX.  It has been implemented as a set of MU-Prolog and C modules communicating through TCP/IP sockets.",1988-05-01
A Project Oriented Course on Software Engineering,"An undergraduate course in Software Engineering has been offered at the University of Wisconsin-LaCrosse for the past three years.  The intent of the course is to present a broad overview of most of the areas of Software Engineering. Most Software Engineering courses offered will also cover the general aspects of Software Engineering.  However, UW-LaCrosse students participate in a group project where they apply these Software Engineering concepts.  Each team of students design a system, make a system design presentation, ""hire"" programmers, integrate the system modules, and document their systems.  The administration of group projects is in general complicated; however, the administration of this type of project with the large amount of interaction among students is worse.  This paper makes an attempt to outline this Software Engineering course with particular attention to the administration of the project.",1990
ACT++: Building a Concurrent C++ with Actors,ACT++ (Actors in C++) is a concurrent object-oriented language being designed for distributed real-time applications. The language is a hybrid of the actor kernel language and the object-oriented language C++. The concurrency abstraction of ACT++ is derived from the actor model as defined by Agha. This paper discusses our experience in building a concurrent extension of C++ with the concurrency abstraction of the actor model. The current design of ACT++ and its implementation are described. Some problems found in the Agha's actor model are discussed in the context of distributed real-time applications.  The use of ACT++ disclosed the difficulty of combining the actor model of concurrency with class inheritance in an object-oriented language.,1989
Synthesis-Oriented Situational Analysis as an Alternative to Analytic Evaluation for Iterative User Interface Design,This report is no longer available - TR-93-20 replaces it.,1993
Large Deformations of a Whirling Elastic Cable,"The large deformations of a whirling elastic cable is studied.  The ends of the cable are hinged but otherwise free to translate along the rotational axis.  The nonlinear governing equations depend on a rotation-elasticity parameter J.  Bifurcation about the straight, axially rotating case occurs when J is greater than or equal to n(pi). Perturbation solutions about the bifurcation points and matched asymptotic solutions for large J are found to second order.  Exact numerical solutions are obtained using quasi-Newton and homotopy methods.",1990
A Comparative Evaluation of Indexing Schemes,"An empirical experment is reported that compares three indexing techniques used to help answer queries for a medium-sized database.  The experiment compares memory and time utilization for B+ trees, superimposed codeword and perfect hashing indexing techniques.  Costs for creating and querying the database are compared over a range at different page and buffer sizes for the database.  We compare and contract the advantages and disadvantages of the indexing techniques.",1990
Interfaces for Cloning in Immersive Virtual Environments,"Three-dimensional objects in many application domains, such as architecture and construction, can be extremely complex and can consist of a large number of components. However, many of these complex objects also contain a great deal of repetition. Therefore, cloning techniques, which generate multiple spatially distributed copies of an object to form a repeated pattern, can be used to model these objects more efficiently. Such techniques are important and useful in desktop three-dimensional modeling systems, but we are not aware of any cloning techniques designed for immersive virtual environments (VEs). In this paper, we present an initial effort toward the design and development of such interfaces. We define the design space of the cloning task, and present five novel VE interfaces for cloning, then articulate the design rationale. We have also performed a usability study intended to elicit subjective responses with regard to affordance, feedback, attention, perceived usefulness, ease of use, and ease of learning in these interfaces. The study resulted in four major conclusions. First, slider widgets are better suited for discrete than for continuous numeric input. Second, the attentional requirements of the interface increase with increased degrees-of-freedom associated with widgets. Third, users prefer constrained widget movement, although more degrees-of-freedom allow more efficient parameter setting. Finally, appropriate feedback can reduce the cognitive load. The lessons we learned will influence our continuing design of cloning techniques, and these techniques will ultimately be applied to VE applications for design, construction, and prototyping.",2004
System Resource Sharing for Synchronous Collaboration,"We describe problems associated with accessing data resources external to the application, which we term externalities, in replicated synchronous collaborative applications (e.g., a multiuser text editor). Accessing externalities such as les, databases, network connections, environment variables and the system clock is not as straightforward in replicated collaborative software as in single-user applications and centralized collaborative systems. We describe ad hoc solutions that have been used previously. Our primary objection to the ad hoc solutions is that the developer must program dierent behavior into the dierent replicas of a multi-user application, which increases the cost and complexity of development. We introduce a novel general approach to accessing externalities uniformly in a replicated collaborative system. The approach uses a semi-replicated architecture where the actual externality resides at a single location and is accessed via replicated proxies. The proxies multiplex input to and output from the single instance of the externality. This approach facilitates the creation of replicated synchronous groupware in two ways: (1) developers use the same mechanisms as in traditional single-user applications (2) developers program all replicas to execute the same behavior. We describe a general design for proxied access to read{only, write{only and read{write externalities. We discuss the tradeos of this semi- replicated approach over full, literal replication and the class of applications to which this approach can be successfully applied. We also describe details of a prototype implementation of this approach within a replicated collaboration-transparency system, called Flexible JAMM (Java Applets Made Multi-user).",1999-09-01
Using a Tablet-PC to Provide Peer-Review Comments,"This reports describes our initial exploration in using a Tablet-PC to provide peer-review comments in the first year Computer Science course. Our exploration consisted of an informal evaluation of how students write comments on other students assignments using three different methods: pen and paper, a Tablet-PC, and a desktop computer. Our ultimate goal is to explore the effect that interface style has on the quality and quantity of the comments provided.",2004
The Elevation Pyramid,"The elevation pyramid, a pyramid-based representation for storing gridded elevation data, is described.  Associated with the root of the pyramid is the corresponding grid's minimum elevation and range.  The elevation value for a specified grid pixel is calculated by transversing a path from the pyramid root to the corresponding leaf node.  As the transversal proceeds, the minimum and range values are refined by interpreting the codes stored at each node along the path.  At the leaf level, the final minimum value equals the associated elevation value. We present results from experiments using 2,3 and 4 bit code words.  For the two bit code, since the total number of nodes in the pyramid is 4/3 the number of pixels required for the bottom level of the pyramid, the amortized storage cost is less than 3 bits per pixel, regardless of vertical resolution.  This corresponds to a 5:1 compression rate for a 16 bit gridded elevation data.  The elevation pyramid is most appropriate for efficient secondary storage archival, such as on a CD-ROM.  It allows efficient retrieval of complete elevation data from any sub-region, at multiple scales, within the entire elevation database.  This is a lossless encoding when the difference between sibling pixels is not ""too great.""  Rapid changes in elevation between adjacent pixels will be smoothed.  Most data sets contain relatively few pixels that cannot be encoded by the techniques studied.  Such pixels can be efficiently stored in an auxiliary table if perfec reconstruction is required.  ""Elevation"" pyramids can be used to store any 2D surface or 3D density data.",1990
Toward a Functional Representation of a Generalized Document Information Storage And Retrieval System,"A model of a generalized document storage and retrieval system is proposed. The model consists of six subsystems (or blocks): logical processor, selector, descriptor file, locator, document file and analysis block. These subsystems function in a partial environment defined by the user and data blocks. Proceeding from a verbal description, a functional representation of each subsystem is developed. The functional representation describes not only what is done but also, to some degree, how tasks are accomplished within each subsystem. An immediate result of the functional representation is the definition of a metalanguage for identifying some necessary characteristics of higher level languages used in the implementation of information storage and retrieval systems. To illustrate the usefulness of the metalanguage, a comparison is made of three languages - FORTRAN, PL/l, and SNOBOL - for implementing document storage and retrieval systems. The functional differences among the blocks of the system are apparent, and the implementation of efficient systems appears to require a multi-language approach.",1975
Entangled Design Knowledge: Relationships as an Approach to Claims Reuse,"As a discipline, human-computer interaction produces creative and innovative designs that could provide a reusable collection of design knowledge on which future efforts could build.  It is unfortunate that so much of this knowledge is not fully reused by designers today.  To encourage the use of previously identified HCI knowledge, we propose a model of reuse building on Carroll?s notion of claims, design knowledge components that capture the positive and negative psychological effects of design features.  We address four challenges associated with reuse in a library of claims, adopted from software engineering?a discipline in which the notion of reuse has been prevalent for quite some time.  Building on Krueger?s definition of reuse and his conceptualization of four key aspects?abstraction, selection, specification, and integration?we propose a reuse approach based on incorporating these four aspects into the design process.  To abstract, select, specify and integrate claims, we identify claim relationships, descriptions of connections between claims.  We portray how claim relationships can be used to aid in identifying claim types, searching for claims, creating new claims, and aggregating claims.  By integrating relationships into a claims library, we demonstrate how they can be applied to assist claims reuse and present studies related to each application of the relationships.",2006
An Empirical Study of Maintenance Activities in Two Object Oriented Systems,"Decades of research on maintenance activities in the procedural paradigm has produced several conclusions.  Among these conclusions are recommendations that a reduction in maintenance cost could be achieved by a more controlled design process, by more rigorous testing of potential problem areas earlier in the life cycle.  With the increasing emphasis on the object oriented paradigm, the authors performed an empirical study of the maintenance patterns in two commercial object oriented systems.  Although this is a preliminary study, intuition is presented as insight into the object oriented maintenance activities.",1993
Heuristics for Laying Out Information Graphs,"The concept of an information graph is introduced as a representation for object-oriented databases.  The retrieval layout problem is an optimization problem defined over the class of information graphs.  The layout abstracts the space efficiency of representing the database as well as the time efficiency of information retrieval.  Heuristics for the retrieval layout problem are identified and evaluated experimentally.  A new heuristic, connectivity traversal, is found to be fast and to produce high quality layouts.",1993
Convergence of Column Generation for Semi-infinite Programs in the Presence of Equality Constraints,A convergence theorem is presented for the standard column generation algorithm which embodies GLM. The primary extension of earlier published theorems is the allowance of equality constraints. A related stability theorem is introduced to demonstrate robustness.,1975
The Relationship Between the Multiplicative And Mixed Generators Modulo 2^b,"MacLaren and Marsaglia [4] comment that their test results suggest  that the multiplicative generator performs better than the mixed generator.  We attempt to answer the above question by showing that for any sequence of real values in (0,1) produced by the multiplicative generator modulo 2^b a corresponding mixed generator exists which, for practical purposes, produces  the same sequence of real values.",1974
Charcoal Iron Industry Analysis: Data Preparation and Computer Program Design,A report of the use of the computer to analyze historical data  on the charcoal iron industry is presented. It includes a discussion  of the problem of analyzing historical data by computer and the use  of the computer to locate errors or inconsistencies in the reporting of  the original data and in the transcription process from document to  punched card. The type of analysis applied to the data is described  and the methods of program construction and verification are explained. The results of this analysis are reported in a separate paper.,1975
KKT conditions satisﬁed using adaptive neighboring in hybrid cellular automata for topology optimization,"The hybrid cellular automaton (HCA) method is a biologically inspired algorithm capable of topology synthesis that was developed to simulate the behavior of the bone functional adaptation process. In this algorithm, the design domain is divided into cells with some communication property among neighbors. Local evolutionary rules, obtained from classical control theory, iteratively establish the value of the design variables in order to minimize the local error between a ﬁeld variable and a corresponding target value. Karush-Kuhn-Tucker (KKT) optimality conditions have been derived to determine the expression for the ﬁeld variable and its target. While averaging techniques mimicking intercellular communication have been used to mitigate numerical instabilities such as checkerboard patterns and mesh dependency, some questions have been raised whether KKT conditions are fully satisﬁed in the ﬁnal topologies. Furthermore, the averaging procedure might result in cancellation or attenuation of the error between the ﬁeld variable and its target. Several examples are presented showing that HCA converges to different ﬁnal designs for different neighborhood conﬁgurations or averaging schemes. Although it has been claimed that these ﬁnal designs are optimal, this might not be true in a precise mathematical sense—the use of the averaging procedure induces a mathematical incorrectness that has to be addressed. In this work, a new adaptive neighboring scheme will be employed that utilizes a weighting function for the inﬂuence of a cell’s neighbors that decreases to zero over time. When the weighting function reaches zero, the algorithm satisﬁes the aforementioned optimality criterion. Thus, the HCA algorithm will retain the benefits that result from utilizing neighborhood information, as well as obtain an optimal solution.",2009
A Processor Utilization Model for a Multiprocessor Computer System,"A processor utilization model for a simplified multiprocessor computer  system is developed. Jobs are assumed to arrive according to a general input  process, and each job is assigned randomly to an available processor. A  finite capacity input buffer is used if no processor is available. The mathematical model is based on the busy period analysis, and two utilization  measures are derived:  (1) processor utilization when the system is busy (the fraction of processor occupation time during a busy period), and  (2) global processor utilization (the fraction of processor occupation time during a busy cycle).  Additionally, the arbitrary time state probability distribution is obtained  and serves as the basis for the above measures in addition to others. Several  approximations enable the development of a computational model from the mathematical model. Experimentation with the computational model reveals the sensitivity  of the model to variability in the arrival process. Comparison of 2-processor  and 4-processor systems from the operator perspective indicates a qualified  preference for the behavior of the 2-processor system. This preference must  be carefully interpreted since processor costs, the increase in overhead with an  increase in processors, and behavioral variables reflecting the user perspective  are excluded.",1975
Formative Evaluation: Ensuring Usability in User Interfaces,"Ensuring usability has become a key goal of interactive system development, as developers have begun to realize that it matters little how effectively an interactive system can compute, if human users cannot communicate effectively with the system. In this paper we discuss what we have found to be two main types of formative user interface evaluation: analytic and empirical. Both these types occur as part of the development process. We do not attempt to survey all approaches to either of these types of formative evaluation, but rather to offer a sampling of some approaches that have been found (by us and by others) to be useful in ensuring usability. We give only an overview of analytic methods, and then focus on empirical methods. We conclude with some observations on future trends in user interface evaluation.",1992
Quantitative Assessment of the Software Maintenance Process,"This paper describes analysis techniques used to assess quantitatively the software maintenance process of a large military contractor, and the results obtained.  The analysis techniques make use of basic data collected throughout the maintenance process.  The data collected are extensive and allow the effects of adding a set of functional enhancements to be traced to process activities and product impact.  The analysis techniques include data snooping, applied to gain insight into relationships and trends in the data.  Simple nonparametric statistical techniques are then applied to test relationships between data items.  The results provide valuable information for predicting process and product characteristics, and assessing the maintenance process.",1993
The Use of Computer Modulated Drawing in the Teaching of Art,"A hardware-software process is described which automatically creates computer modulated drawings from an artist's own works. The process allows a drawing to  act as a constant source of data for a series of renditions of it in widely varying styles. As a result a student of art can study the effects of line structure,  line quality, and pen stroking style in rendering truly constant subject material.",1974
Scalability Analysis of Parallel GMRES Implementations,"Applications involving large sparse nonsymmetric linear systems encourage parallel implementations of robust iterative solution methods, such as GMRES(k). Two parallel versions of GMRES(k) based on different data distributions and using Householder reflections in the orthogonalization phase, and variations of these which adapt the restart value k, are analyzed with respect to scalability (their ability to maintain fixed efficiency with an increase in problem size and number of processors).A theoretical algorithm-machine model for scalability is derived and validated by experiments on three parallel computers, each with different machine characteristics.",2001
The Objectives And Requirements of Model Manageement,"Model management is a technology evolving by necessity, pushed by the attempts to deal with increasingly complex systems and the perceived inadequacies of past efforts. This rapid evolution of Model Management Systems  (MMS) has created different perspectives of the role of the MMS; one arising  the user's interaction with a model data bank and the other view from the  in the database and decision support systems research community stressing  modeling community emphasizing the model development functions. These two  perspectives are clarified and reconciled by relating each to the model life  cycle, which leads to a more comprehensive statement of MMS requirements.",1983
GLM Versus Continuous Approximation for Convex Integer Programs,"GLM is compared to continuous approximation for convex, integer programs.  After noting the stronger bound provided by GLM, Lagrangian duality and  a gap closing heuristic is used to demonstrate how GLM may provide a better  feasible policy as well.",1974
Usability Evaluation in Virtual Environments: Classification and Comparison of Methods,"Virtual environments (VEs) are a relatively new type of human-computer interface in which users perceive and act in a three-dimensional world. The designers of such systems cannot rely solely on design guidelines for traditional two-dimensional interfaces, so usability evaluation is crucial for VEs. We present an overview of VE usability evaluation. First, we discuss some of the issues that differentiate VE usability evaluation from evaluation of traditional user interfaces such as GUIs. We also present a review of VE evaluation methods currently in use, and discuss a simple classification space for VE usability evaluation methods. This classification space provides a structured means for comparing evaluation methods according to three key characteristics: involvement of representative users, context of evaluation, and types of results produced. To illustrate these concepts, we compare two existing evaluation approaches: testbed evaluation [Bowman, Johnson, & Hodges, 1999], and sequential evaluation [Gabbard, Hix, & Swan, 1999]. We conclude by presenting novel ways to effectively link these two approaches to VE usability evaluation.",2001
A File Definition Facility for File Structures,"This paper describes a file definition facility (FDF) for defining files as graph structures. The structure of the file is explicitly declared in the file definition. Primitive functions(from graph theory), operators, and the format of the definition statements are given. The combination of functions and operators appear as directives to the programming system for structuring files. Several simple examples are given to illustrate the use of the FDF.  The data organization for the implementation of this facility is described in detail. Problems of considerable importance that are treated are.  (1) garbage collection, (2) template construction, and (3) runtime address calculation. The external definitions are represented internally by descriptors. The format of the descriptors is given and a discussion of the items in the descriptors is presented.",1974
Finding All Isolated Solutions to Polynomial Systems Using Hompack,"Although the theory of polynomial continuation has been established for over a decade (following the work of Garcia, Zangwill, and Drexler), it is difficult to solve polynomial systems using continuation in practice. Divergent paths (solutions at infinity), singular solutions, and extreme scaling of coefficients can create catastrophic numerical problems.  Further, the large number of paths that typically arise can be discouraging.  In this paper we summarize polynomial-solving homotopy continuation and report on the performance of three standard path-tracking algorithms (as implemented in HOMPACK) in solving three physical problems of varying degrees of difficulty. Our purpose is to provide useful information on solving polynomial systems; including specific guidelines for homotopy construction and parameter settings. The m-homogeneous strategy for constructing polynomial homotopies is outlined, along with more tradition approaches.  Computational comparisons are included to illustrate and contrast the major HOMPACK options. The conclusions summarize our numerical experience and discuss areas for future research.",1987
InTouch Usability Evaluation,"The main purpose of this test was to asses the performance of an actual customer with little or no previous InTouch experience. The usability test measured the total time needed to accomplish information entry and output, and record user critical incidents. Tasks included routine operations, print operations, and import/export of data.  Each test session consisted of a performance test where the user performed a series of tasks, and a post-test interview where the user filled out a brief user preference questionnaire about the functionality and usability of InTouch and was given the opportunity to make comments or ask questions about InTouch.  Evaluation measures included: observations and comments for each critical incident; classification of errors associated with critical incidents including severity, scope, and source of error; the time necessary to complete each task; the percentage of participants who successfully complete each task; and user rankings of the functionality and usability of InTouch.  All participants did fulfill InTouch developer goals of having users succeed in entering information into InTouch within the first 5-10 minutes of use and outputting information from InTouch within the first half hour of use. Although, many participants did have major problems.  User likes included the InTouch documentation, the exclusion option in sort, the option of creating a new group from the group search dialog box, the choice of sounds for reminders, and the repeat option for reminders.  User dislikes included having to search the giant InTouch menu, the difficulty figuring out the function of the different panes in the main window, the lack of on-line help, and the lack of access to group functions from the main window. Users also disliked InTouch not behaving like other Macintosh applications: its files could not be opened by double-clicking, and no one liked having to select InTouch from the menu to start-up the program. Most users also had problems with the print and layout dialog boxes.  Problems were rated in terms of severity and scope, and whenever possible, the source of the problems and potential solutions were indicated. The solutions presented in this report are just recommendations. Re-design efforts should consider alternative solutions with both the problems and potential solutions evaluated in light of the total system.",2001-08-01
Usability Inspection Report of iLumina,"iLumina is a digital library of sharable undergraduate teaching resource materials for science, mathematics, technology, and engineering being developed by the University of North Carolina at Wilmington (UNCW), Collegis, Virginia Tech, Georgia State University, Grand Valley State and The College of New Jersey. Types of iLumina resources include papers, tutorials, applets, presentations, visualizations, experiments, assignments, software, exercises.",2002-09-01
A Tri-Valued Belief Network Model for Information Retrieval,A Tri-Valued Belief Network Model for Information Retrieval.,2001-12-01
Link Capacity Assignment in Dynamic Hierarchical Networks,"The dynamic hierarchy, which is a generalization of the centralized, tree structured network, is introduced in this paper. First, a queueing network model of the dynamic hierarchy is formulated. Following the derivation of a network performance measure, probabilistic and heuristic assignment strategies are created. These strategies are compared through the use of analysis of variance procedures and preferred strategies are selected. It is found that a relatively simple heuristic strategy produces capacity assignments whose quality is comparable to that of the preferred probabilistic strategy.",1987
A Complete Horizontal Microlanguage,"This paper defines a data space whose points are trees with leaves which are [name,value] pairs. Over this space a substitution operator S (meaning informally ""in x for y put z"") is formally defined. Taken together with several auxiliary operators, S is shown to be sufficient to define a large class of high-level languages since S is known to be functionally complete for finite-valued spaces, a functionally complete language is exhibited.",1973
A DOS-M Primer,No abstract available.,1974
Pinch Keyboard: Natural Text Input for Immersive Virtual Environments,"Text entry may be needed for system control tasks in immersive virtual environments, but no efficient and usable techniques exist. We present the pinch keyboard interaction technique, which simulates a standard QWERTY keyboard using Pinch Gloves™ and 6 DOF trackers. The system includes visual and auditory feedback and a simple method of calibration.",2001
Efficient Uncertainty Quantification with the Polynomial Chaos Method for Stiff Systems,"The polynomial chaos method has been widely adopted as a computationally feasible approach for uncertainty quantification. Most studies to date have focused on non-stiff systems. When stiff systems are considered, implicit numerical integration requires the solution of a nonlinear system of equations at every time step. Using the Galerkin approach, the size of the system state increases from $n$ to $S \times n$, where $S$ is the number of the polynomial chaos basis functions. Solving such systems with full linear algebra causes the computational cost to increase from $O(n^3)$ to $O(S^3n^3)$. The $S^3$-fold increase can make the computational cost prohibitive. This paper explores computationally efficient uncertainty quantification techniques for stiff systems using the Galerkin, collocation and collocation least-squares formulations of polynomial chaos. In the Galerkin approach, we propose a modification in the implicit time stepping process using an approximation of the Jacobian matrix to reduce the computational cost. The numerical results show a run time reduction with a small impact on accuracy. In the stochastic collocation formulation, we propose a least-squares approach based on collocation at a low-discrepancy set of points. Numerical experiments illustrate that the collocation least-squares approach for uncertainty quantification has similar accuracy with the Galerkin approach, is more efficient, and does not require any modifications of the original code.",2007
POLSYS_PLP: A Partitioned Linear Product Homotopy Code for Solving Polynomial Systems of Equations,"Globally convergent, probability-one homotopy methods have proven to be very effective for finding all the isolated solutions to polynomial systems of equations.  After many years of development, homotopy path trackers based on probability-one homotopy methods are reliable and fast.  Now, theoretical advances reducing the number of homotopy paths that must be tracked, and in the handling of singular solutions, have made probablitiy-one homotopy methods even more practical.  POLSYS_PLP consists of Fortran 90 modules for finding all isolated solutions of a complex coefficient polynomial system of equations.  The package is intended to be used in conjunction with HOMPACK90 (algorithm 777), and makes extensive use of Fortran 90 derived data types to support a partitioned linear product (PLP) polynomial system structure.  PLP structure is a generalization of m-homogeneous structure, whereby each component of the system can have a different m-homogeneous structure.  POLSYS_PLP employs a sophisticated power series end game for handling singular solutions, and provide support for problem definition both at a high level and via hand-crafted code.  Different PLP structures and their corresponding Bezout numbers can be systematically explored before committing to root finding.",1998-08-01
Parallel Deterministic and Stochastic Global Minimization of Functions with Very Many Minima,"The optimization of three problems with high dimensionality and many local minima are investigated under five different optimization algorithms: DIRECT, simulated annealing, Spall’s SPSA algorithm, the KNITRO package, and QNSTOP, a new algorithm developed at Indiana University.",2011
Building the CODER Lexicon: The Collins English Dictionary and Its Adverb Definitions,"The CODER (COmposite Document Expert/extended/effective Retrieval) project is an investigation of the applicability of artificial intelligence techniques to the information retrieval task of analyzing, storing, and retrieving heterogeneous collections of ""composite documents."" In order to support some of the processing desired, and to allow experimentation in information retrieval and natural language processing, a lexicon was constructed from the machine readable Collins dictionary of the English Language.  After giving background, motivation, and a survey of related work, the Collins lexicon is discussed.  Following is a description of the conversion process, the format of the resulting Prolog database, and characteristics of the dictionary and relations. To illustrate what is present and to explain how it relates to the files produced from Webster's Seventh New Collegiate Dictionary, a number of comparative charts are given. Finally, a summary of adverb definitions is presented, together with a description of defining formula that usually indicate the type of the adverb. Ultimately it is hoped that definitions for adverbs and other words will be parsed so that the relational lexicon being constructed will include many additional relationships and other knowledge about words and their usage.",1986-10-01
Instructional Footprinting: A Basis for Exploiting Concurrency Through Instructional Decomposition and Code Motion: A Research Prospectus,"In many languages, the programmer is provided the capability of communicating, through the use of function calls, with other, separate, independent processes. This capability can be simple, as a service request made to the operating system, or more advanced, as Tuple space operations specific to a Linda programming system. The problem with such calls, however, is that they block while waiting for data of information to be returned. This synchronous nature and lack of concurrency can be avoided by initiating the request for data earlier in the code and retrieving the returned data later when it is needed. In order to facilitate this concurrency of processing, an instructional footprint model is developed which formally describes movement of instruction. This paper presents a proposal for research that involves the development of the instructional footprint model and an algorithmic framework in which to exploit concurrency in programming languages.",1992
A Distributed Parallel Processing Environment Based upon the Linda Paradigm: A Research Prospectus,"As the computing capacity of the uniprocessor is being taxed, and the high cost of parallel and super-computers is still prevalent, alternative methods of achieving parallel performance at an economical price are desired. This proposed research effort offers one such alternative, focusing on the idle CPU cycles existing on local area networks. With the increase in the computing power of workstations and their declining costs, one can effectively transform the unused computing power attached to a local area network into a parallel processing environment. Effectively exploiting such an environment, however, requires a specification and operational framework that is portable, easy to use, and efficient. The environment is constructed around the Linda parallel programming paradigm which provides an effective parallel computational framework.",1992
On the Relationship Between the Object-Oriented Paradigm and Software Reuse: An Empirical Investigation,"This paper describes the results of a controlled experiment designed to evaluate the impact of the object-oriented paradigm on software reuse. The experiment concludes that (1) the object-oriented paradigm substantially improves productivity over the procedural paradigm, (2) language differences are far more important when programmers reuse than when they do not, (3) under both moderate and strong encouragement to reuse, the object-oriented paradigm promotes higher productivity than the procedural paradigm, (4) software reuse improves productivity no matter which language paradigm is used, and (5) the object-oriented paradigm has a particular affinity to the reuse process.",1992
The Abstraction Refinement Model and the Modification-Cost Problem,"A problem common to systems and software engineering is that of estimating the cost of making changes to a system. For system modifications that include changes to the design history of the system this is the ""modification-cost"" problem. A solution to this problem is important to planning changes in large systems engineering projects. In this paper, a cost model based on the Abstraction Refinement Model (ARM) is proposed as a framework for deriving solutions to the modification-cost problem. The ARM is a characterization of software evolution that is also applicable to general systems. Modifications to systems and their design histories are described using the components of the ARM. The cost model is defined by functions on the ARM components. The derived solution is given by an abstract expression of the cost functions.",1992
Implementing Communication Protocols Using Object-Oriented Techniques,"In this paper, elements of an object-oriented implementation of the upper layer OSI protocols are presented. Our goal in the paper is two-fold. First, to communicate to software engineers, particularly those developing systems software, that object-oriented programming techniques facilitate and enhance the implementation of layered architectures. Second, to convey to language designers our experience using object-oriented language features in building communications protocols. We focus on the requirements for the upper layer OSI protocols and illustrate how inheritance, subtyping, and polymorphic function effect their implementation.",1992
Turing's Test and Conscious Thought,"Over 40 years ago, A. M. Turing proposed a test for intelligence in machines. Based as it is, solely on an examinee's verbal responses, the Test misses some important components of human thinking. To bring these manifestations within its scope, the Turing Test would require substantial extension. Advances in the application of AI methods in the design of improved human-computer interfaces are now focusing attention on machine models of thought and knowledge from the altered standpoint of practical utility.",1992
Least Change Secant Update Methods for Undetermined Systems,"Least-change secant updates for nonsquare matrices have been addressed recently in [6].  Here we consider the use of these updates in iterative procedures for the numerical solution of underdetermined systems. Our model method is the normal flow algorithm used in homotopy or continuation methods for determining points on an implicitly defined curve. A Kantorovich-type local convergence analysis is given which supports the use of least-change secant updates in this algorithm. This analysis also provides a Kantorovich-type local convergence analysis for least-change secant update methods in the usual case of an equal number of equations and unknowns. This in turn gives a local convergence analysis for augmented Jacobian algorithms which use least-change secant updates. We conclude with the results of some numerical experiments.  Key words. underdetermined systems, least-change secant update methods, quasi-Newton methods, normal flow algorithm, augmented Jacobian matrix algorithm, continuation methods, homotopy methods, curve-tracking algorithms, parameter-dependent systems",1988
Notational Techniques for Accommodating User Intention Shifts,"Good user interface designs allow for user intention shifts.  The asynchronous nature of direct maniulation interfaces inherently demands consideration of user intention shifts during the performance of a task. Maintaining a focus on the primary function of a task while at the same time accommodating user intention shifts is difficult for interface designers when both these aspects are represented at the same design level.  The User Action Notation (UAN), a technique for representing asynchronous interfaces, contains a mechanism for specifying points in a task where user intention shifts may occur.  A complementary technique, Task Transition Diagrams (TTDs), is used to specify tasks that users can perform to interrupt their current task.  The Task Transition Diagram is a notation that allows a designer to map out the set of tasks and intentions of users without having to be concerned with the minutiae of how a user accomplishes those tasks.",1990
Estimating and Predicting Error Detection Trends and Relative Manpower Utilization,"The work described in this paper outlines an investigative effort focusing on estimating the number of errors remaining in a software product and the effort required to detect those errors.  More specifically, we provide a description of a computational model that, through a non-linear optimization process: (a) determines an error detection trend in a given development effort, (b) provides an estimate of the errors remaining in the product and the time required to detect those errors, and (c) relates the error detection effort to current and future manpower requirements.  The model is simple to use, requiring only error counts and detection dates as input, and is grounded in the time-tested theory underlying Musa's Basic Reliability Model and the Rayleigh Manpower Utilization Curve.  The model has been developed under the auspices of the Software Assurance Technology Center (SATC) at NASA/GSFC.  Data sets from various NASA projects were used in formulating and tuning the model.  For research purposes, the model was initially developed within an Excel framework; a more robust implementation is currently underway.",1998-03-01
Flexible Collaboration Transparency,"This paper presents two distinct contributions: First, we present a critique of collaboration transparency as it is currently implemented in contrast to collaboration-aware implementations.  We find conventional collaboration-transparency systems lacking in terms of efficient use of network resources and support for key groupware principles: concurrent work, relaxed WYSIWIS, and group awareness. Second, we examine the causes of these deficiencies, and then present an alternative implementation approach based on an object-oriented replicated architecture where selected single-user interface objects are dynamically replaced by multi-user extensions.  The replacement is transparent to the single-user application and its developer.  As an instance of this approach, we described its incorporation into a Java-based collaboration-transparency system, called JAMM.",1998-04-01
Sorting by Short Block-Moves,"Sorting permutations by operations such as reversals and block-moves has received much interest because of its applications in computational biology, particularly in the study of genome rearrangements.  A short block-move is an operation on a permutation that moves an element at most two positions away from its original position.  This paper investigates the problem of finding a minimum-length sorting sequence of short block-moves for a given permutation.  A 4/3-approximation algorithm for this problem is presented.  Exact polynomial-time algorithms are presented for woven bitonic permutations and woven double-strip permutations.  A linear-time maximum matching algorithm for a special class of grid graphs is also discovered that improves the time complexity of one of these exact algorithms.",1998-02-01
The JONES Microprocessor: User's Manual,This technical report is no longer available.,1987
Improving Concurrency in Common Object Models,"Most common object models of distributed object systems lack features related to 'concurrency' (the ability to have more than one thread of execution in an object simultaneously).  The lack of support for concurrency restricts the development of new components and limits reuse of existing components that use these advanced features.  In this paper, the concurrency features of the Interoperable Common Object Model (ICOM) centered on statically typed object-oriented languages, is presented.  The ICOM model is an attempt to elevate common object models (with the advanced features of concurrency) closer to the object models of statically typed object-oriented languages.  Specific features of the ICOM object model include: atomic objects and guard methods.  The actor model is used to develop a uniform implementation framework for the ICOM object model in C++ and Modula-3.",1998-03-01
A Generalized Comparison of Quadtree and Bintree Storage Requirements,"The quadtree and bintree data structures are two variants on the principle of hierarchical regular decomposition applied to image representation.  A comparison of the storage requirements for images represented by these two methods is presented.  The relative storage efficiency of quadtrees and bintrees is determined by two factors: the relative mode size for the two representations as determined by the data structure implementation, and the number of quadtree leaf node pairs that merge to form a single leaf node after conversion to the bintree representation.  The merging probability is analyzed, and found to be close to 0.5 for most images.  The resulting storage efficiency for a number for representative implementations is discussed.  Each of the data structures has implementations (and associated applications) for which it is more space efficient.",1989
Automated Theorem-proving and Program Verification: An Annotated Bibliography,"This bibliography contains a synopsis of each reference, taken from  one of three sources:  (1) the author's abstract;  (2) the review of the reference in ACM Computing Reviews;  (3) an abstract prepared locally.  The third alternative was chosen only if the first two were unavailable.  Each reference is identified by a number with prefix PV or A (program  verification or automated theorem-proving) depending upon its contents, and  the identification number is used for cross-referencing the entries. If a  reference deals with both areas, it has two identification numbers. Also  included is an author index, which is arranged alphabetically according to  the first author's name if the source is co-authored. It is anticipated  that this bibliography will be updated as research in the field progresses.",1975
The Mixed Method of Random Number Generation: A Tutorial,"Several motivations are recognized for user-defined random number generators  in preference to built-in generators. The mixed method of random number generation is discussed) and the conditions for achieving full period with a modulus of 2^b  are explained. Implementation of mixed random number generators is affected  both by the computer and language used. Guidelines are presented for realizing  acceptable mixed generators on several machines using the FORTRAN, PL/l and  SNOBOL4 languages.",1974
Finiteness and Bounds of Complete Test Point Sets for Program Verification,It is proven that there exists a finite set of test points that suffices to establish the equivalence of two programs s and t if some finite set S of programs can be identified that contains both s and t. It is also proven that the expected number of  those test points is bounded by the logarithm of the cardinality of S.,1976
Supporting Worker Independence in Collaboration Transparency,"Conventional collaboration-transparency systems are inefficient in their use of network resources and lack support for key groupware principles: concurrent work, relaxed WYSIWIS, and group awareness.  We present an alternative implementation approach to collaboration transparency that provides many features previously seen only in collaboration-aware applications.  Our approach is based on an object-oriented replicated architecture where selected single-user interface objects are dynamically replaced by multi-user extensions. The replacement is transparent to the single-user application and its developer.  As an instance of this approach, we describe its incorporation into a new Java-based collaboration-transparency system, called Flexible JAMM. We conducted an empirical study to evaluate the effectiveness of Flexible JAMM versus a representative conventional collaboration-transparency system, NetMeeting. Completion times were significantly faster in a loosely-coupled task using Flexible JAMM, and were not adversely affected in a tightly-coupled task.  Accuracy was unaffected by the system used.  Participants greatly preferred Flexible JAMM.  The evaluation validates our aim of supporting multiple styles of collaboration.",1998-05-01
On Making Bairstow's Method Work,No abstract available.,1974
Mobilizar: Capturing User Behavior with Mobile Digital Diaries,"In this paper we present Mobilizar, a web-based mobile tool that facilitates the implementation and data collection of self-reported user behavior. Mobilizar was designed with both the researcher and the participant in mind. It provides investigators with a way to setup a new diary study in a matter of minutes and to electronically collect diary data from participants by using internet-enabled mobile devices. These devices promise to alleviate the burden of carrying a paper-and-pencil diary by instead using the participant’s own device. It also gives participants the flexibility to report their behavior in different ways such as making text, voice, or picture entries that fit their current situational constraints. In this paper, we describe the user interface design of Mobilizar and how it may be used to conduct diary studies with mobile devices.",2010
Rapid Prototyping in Human-Computer Interface Development,"Some conventional approaches to interactive system development tend to force commitment to design detail without a means for visualizing the result until it is too late to make significant changes.  Rapid prototyping and iterative system refinement, especially for the human interface, allow early observation of system behavior and opportunities for refinement in response to user feedback.  The role of rapid prototyping for evaluation of interface designs is set in the system development life cycle.  Advantages and pitfalls are weighed, and detailed examples are used to show the application of rapid prototyping in a real development project.  Kinds of prototypes are classified according to how they can be used in the development process, and system development issues are presented.  The future of rapid prototyping depends on solutions to technical problems that presently limit effectiveness of the technique in the context of present day software development environments.",1989
Evaluation of the Maintainability of Object-Oriented Software,"Empirical research is one way of testing software engineering methodologies.  Many claims of the advantages of the object oriented paradigm have been made by research.  Our work involves empirical projects in an attempt to validate these claims.  This paper describes an experiment which compares the maintainability of the two functionally equivalent systems in order to explore the claim that systems developed with object-oriented languages are more easily maintained than those programmed with procedural languages.  We found supporting evidence that programmers produce more maintainable code with an object-oriented language than with a standard procedural language.  Another on-going research effort attempts to measure the reusability of objects (written in C++) versus the reusability of procedures (written in Pascal).  These experiments involve students, but unlike other published reports, deal with large systems.",1990
Deterministic Parallel Global Parameter Estimation for a Model of the Budding Yeast Cell Cycle,"Two parallel deterministic direct search algorithms are used to find improved parameters for a system of differential equations designed to simulate the cell cycle of budding yeast. Comparing the model simulation results to experimental data is difficult because most of the experimental data is qualitative rather than quantitative. An algorithm to convert simulation results to mutant phenotypes is presented. Vectors of parameters defining the differential equation model are rated by a discontinuous objective function. Parallel results on a 2200 processor supercomputer are presented for a global optimization algorithm, DIRECT, a local optimization algorithm, MADS, and a hybrid of the two.",2006
Visualizing the Results of a Complex Hybrid Dynamic-Static Analysis,"Complex static or hybrid static-dynamic analyses produce large quantities of structured data.  In the past, this data was generally intended for use by compilers or other software tools that used the produced information to transform the application being analyzed. However, it is becomingly increasingly common for the results of these analyses to be used directly by humans. For example, in our own prior work we have developed a hybrid dynamic-static escape analysis intended to help developers identify sources of object churn within large framework-base applications. In order to facilitate human use of complex analysis results, visualizations need to be developed that allow a user to browse these results and to identify the points of interest within these large data sets. In this paper we present Hi-C, a visualization tool for our hybrid escape analysis that has been implemented as an Eclipse plugin. We show how Hi-C can help developers identify sources of object churn in a large framework-based application and how we have used the tool to assist in understanding the results of a complex analysis.",2010-06-01
Proceedings of the Fifth Annual Virginia Tech Center for Human-Computer Interaction Research Experience for Undergraduates (REU) Symposium,"Virginia Tech's Center for Human-Computer Interaction presents the project abstracts for the REU 2010 symposium. The REU (Research Experience for Undergraduates) program provides undergraduate students from various universities with the opportunity to spend eight weeks at Virginia Tech, working with our faculty and graduate students on research projects using the state-of-the-art technology and laboratories assembled here. The REU program is sponsored primarily by the National Science Foundation (IIS-0851774, IIS-0552732). Additional support was provided by the NSF (CNS-0540509), and the VT CS Department CSRC.",2010
Some Experimental Observations on the Behavior of Composite Random Number Generators,"A series of experiments with composite random number generators utilizing  shuffling tables is described. The factors investigated are: (1) the magnitude  of the modulus (equivalently, the word-size of the machine), (2) the effect of  the modulus value for the indexing generator, and (3) the table size used for  shuffling. Experimental results indicate that:  (1) on large word-length machines (permitting large modulus values), shuffling accomplishes little in comparison with selected simple generators,  (2) on small word-length machines, shuffling can produce sequences having an increased period and demonstrating acceptable statistical behavior, and  (3) a table size of 2 produces results comparable to those obtained with larger tables.",1974
Some Classical Mathematical Results Related to the Problems of the Firmware/Hardware Interface,The paper reviews the Shannon and Reed-Muller Decomposition Theorems  and notes the relationship of the former to machine instruction set. It  hypothesizes an instruction set based on Galois field operations and applies  the divided difference methods of Newton to the automatic generation of a  polynomial representation of an arbitrary function. Some numeric results  are given for the fields GF(9) and GF(16). An extension of the methods  to the representation of functions by rational forms is suggested.,1975
Machine-inspired Enhancements of the Simplex Algorithm,Using spare storage and over-lapping overhead available during the   generation of the pricing form in the revised-simplex algorithm allows the   production of additional reduced costs of variables which may be effectively   used to avoid redundant pivots and to predict actual gains. Results from a   simulation indicate a dramatic improvement over all other meaningful selection   methods.,1975
The Green500 List: Escapades to Exascale,"Energy efﬁciency is now a top priority. The ﬁrst four years of the Green500 have seen the importance of en- ergy efﬁciency in supercomputing grow from an afterthought to the forefront of innovation as we near a point where sys- tems will be forced to stop drawing more power. Even so, the landscape of efﬁciency in supercomputing continues to shift, with new trends emerging, and unexpected shifts in previous predictions. This paper offers an in-depth analysis of the new and shifting trends in the Green500. In addition, the analysis of- fers early indications of the track we are taking toward exas- cale, and what an exascale machine in 2018 is likely to look like. Lastly, we discuss the new efforts and collaborations toward designing and establishing better metrics, method- ologies and workloads for the measurement and analysis of energy-efﬁcient supercomputing.",2011
Linear Time Distance Transforms for Quadtrees,Linear time algorithms are given for computing the chessboard distance transform for both pointer-based and linear quadtree representations. Comparisons between algorithmic styles for the two representations are made. Both versions of the algorithm consist of a pair of tree traversals.,1989-08-01
DIVERSE: A Software Toolkit to Integrate Distributed Simulations with Heterogeneous Virtual Environments,"We present DIVERSE (Device Independent Virtual Environments- Reconfigurable, Scalable, Extensible), which is a modular collection of complimentary software packages that we have developed to facilitate the creation of distributed operator-in-the-loop simulations. In DIVERSE we introduce a novel implementation of remote shared memory (distributed shared memory) that uses Internet Protocol (IP) networks. We also introduce a new method that automatically extends hardware drivers (not in the operating system kernel driver sense) into inter-process and Internet hardware services. Using DIVERSE, a program can display in a CAVE™, ImmersaDesk™, head mounted display (HMD), desktop or laptop without modification. We have developed a method of configuring user programs at run-time by loading dynamic shared objects (DSOs), in contrast to the more common practice of creating interpreted configuration languages. We find that by loading DSOs the development time, complexity and size of DIVERSE and DIVERSE user applications is significantly reduced. Configurations to support different I/O devices, device emulators, visual displays, and any component of a user application including interaction techniques, can be changed at run-time by loading different sets of DIVERSE DSOs. In addition, interpreted run-time configuration parsers have been implemented using DIVERSE DSOs; new ones can be created as needed.  DIVERSE is free software, licensed under the terms of the GNU General Public License (GPL) and the GNU Lesser General Public License (LGPL) licenses.  We describe the DIVERSE architecture and demonstrate how DIVERSE was used in the development of a specific application, an operator-in-the-loop Navy ship-board crane simulator, which runs unmodified on a desktop computer and/or in a CAVE with motion base motion queuing.",2001
Sign for a Fully Transportable Natural Language Front-end to Database Management Systems,"Natural language front-ends to database management systems represent a major improvement in accessibility for non-expert users. Unfortunately, such interfaces usually require extensive customizing not only of the front-end, but also of the data manager and hence of the DBMS itself. Developing such customized systems represents a huge investment of time and resources. In the 1980s, research has centered on making such systems portable at least across data bases, and in some cases across data base management systems. This report describes an architecture for complete transportability with minimal reprogramming, which has been partially implemented in a prototype system called TIPS. The TIPS architecture is compared briefly with other recent architectures, with attention to ease of portability, amount and locus of reprogramming needed, and extent of coverage both linguistically and in terms of database operations.",1989
Solving Spline Collocation Approximations to Nonlinear Two-point Boundary Value Problems by a Homotopy Method,"The Chow-Yorke algorithm is a homotopy method that has been proved globally convergent for Brouwer fixed point problems, certain classes of zero linding and nonlinear programming problems, and two-point boundary value approximations based on shooting and finite differences. The method is numerically stable and has been successfully applied to a wide range of practical engineering problems. Here the Chow-Yorke algorithm is proved globally convergent for a class of spline collocation approxlmetions to nonlinear two-point boundary value problems. Several numerical implementations of the algorithm are briefly described. and computational results are presented for a fairly difficult hid dynamics boundary value problem.",1984
An Evaluation Procedure for Human-Computer Interface Development Tools,"Human-computer interface development tools--often called user interface management systems or UIMS--are interactive systems that support production and execution of the human-computer interface. Despite their proliferation, no method exists for their systematic evaluation or comparison. We have developed an evaluation procedure that uses a standardized technique to produce quantifiable criteria for evaluating and comparing human-computer interface development tools.  The procedure produces ratings along two dimensions: functionality and usability.  Specification/implementation techniques used by the tool are also quantitatively rated.  An empirical study indicates that the procedure produces reliable results.  The procedure is already being used in one commercial environment.",1988
Storage Reduction Through Minimal Spanning Trees,"In this paper, we shall show that a minimal spanning tree for a set  of data can be used to reduce the amount of memory space required to store  the data. Intuitively, the more points we have, the more likely our method  will be better than the straightforward method where the data is stored in  the form of a matrix. In Section 3, we shall show that once the number of  samples exceeds a certain threshold, it is guaranteed that our method is  better. Experiments were conducted on a set of randomly generated artificial  data and a set of patient data. In the arttficial data experiment, we saved  23% for the worst case and 45% for the best case. In the patient data  experiment, we saved 73% of the memory space.",1975
A Methodology for Integrating Maintainability Using Software Metrics,"Maintainability must be integrated into software early in the development process. But for practical use, the techniques used must be as unobtrusive to the existing software development process as possible. This paper defines a methodology for integrating maintainability into large-scale software and describes an experiment which implemented the methodology into a major commercial software development environment.",1989
User Intention-Based Traffic Dependence Analysis for Anomaly Detection,"This paper describes an approach for enforcing dependencies between network traffic and user activities for anomaly detection. We present a framework and algorithms that analyze user actions and network events on a host according to their dependencies. Discovering these relations is useful in identifying anomalous events on a host that are caused by software flaws or malicious code. To demonstrate the feasibility of user intention-based traffic dependence analysis, we implement a prototype called CR-Miner and perform extensive experimental evaluation of the accuracy, security, and efficiency of our algorithm. The results show that our algorithm can identify user intention-based traffic dependence with high accuracy (average 99:6% for 20 users) and low false alarms. Our prototype can successfully detect several pieces of HTTP-based real-world spyware. Our dependence analysis is fast with a minimal storage requirement. We give a thorough analysis on the security and robustness of the user intention-based traffic dependence approach.",2012
A Critique of Design Approaches for Notification Systems,"The Fall 2004 Virginia Tech Undergraduate Research in Computer Science (VTURCS) class, taught by Dr. McCrickard, covered the following topics: problem, activity and information/interaction phases of design; scenario based design; interruption, reaction, and comprehension (IRC) values; stages of action; ubiquitous computing evaluation areas (UEAs) and participatory negotiation. This critique is my assessment and observations of how these design approaches worked for our project.",2004
EtanaViz: A Visual User Interface to Archaeological Digital Libraries,"Analyzing and hypothesizing are important scientific processes involved in archaeological activity. In this paper, we describe a visual user interface closely coupled with services for archaeological digital libraries, to help archaeologists analyze data and test hypotheses. Our system, EtanaViz, employs a dynamic hyperbolic tree to display hierarchical relationships among excavation records, based on spatial, temporal, and artifact-related taxonomies. Also, EtanaViz provides stacked bar charts to indicate categories. More specifically, we show how EtanaViz can help users analyze data about animal bones excavated from two archaeological sites, Tell Nimrin and Tell al-'Umayri'. The fauna are associated with cultural phases. Comparisons of animal bones from Tell Nimrin across cultural phases provide insights into changing subsistence strategies during these time periods. Inter-site comparisons also show shifts in animal use as well as long-term adaptations to environmental changes.",2005-10-01
Algorithms for Feature Selection in Rank-Order Spaces,"The problem of feature selection in supervised learning situations is considered, where all features are drawn from a common domain and are best interpreted via ordinal comparisons with other features, rather than as numerical values. In particular, each instance is a member of a space of ranked features. This problem is pertinent in electoral, financial, and bioinformatics contexts, where features denote assessments in terms of counts, ratings, or rankings. Four algorithms for feature selection in such rank-order spaces are presented; two are information-theoretic, and two are order-theoretic. These algorithms are empirically evaluated against both synthetic and real world datasets. The main results of this paper are (i) characterization of relationships and equivalences between different feature selection strategies with respect to the spaces in which they operate, and the distributions they seek to approximate; (ii) identification of computationally simple and efficient strategies that perform surprisingly well; and (iii) a feasibility study of order-theoretic feature selection for large scale datasets.",2005
A problem solving environment for the wood-based composites industry,"Product quality and cost efficiency continue to grow in importance for the wood-based composites industry. The complex dynamics of the manufacturing process lends itself to the adoption of simulation models. Simulation models may be used to understand and manipulate the many parameters involved in the manufacturing process. However, most of the simulation models that have been developed over the last two decades have not been implemented due to a lack of continued technical support and a poor user interface. WBCSim is a prototype, Web based, problem solving environment (PSE) that was developed to assist manufacturers and scientists in the design and manufacture of selected wood-based composite products. At the heart of WBCSim is a collection of legacy codes, which are described here. This PSE demonstrates the possibility of implementing scientific computing into a manufacturing application.",2005
Adjusting process count on demand for petascale global optimization⋆,"There are many challenges that need to be met before efficient and reliable computation at the petascale is possible. Many scientific and engineering codes running at the petascale are likely to be memory intensive, which makes thrashing a serious problem for many petascale applications. One way to overcome this challenge is to use a dynamic number of processes, so that the total amount of memory available for the computation can be increased on demand. This paper describes modifications made to the massively parallel global optimization code pVTdirect in order to allow for a dynamic number of processes. In particular, the modified version of the code monitors memory use and spawns new processes if the amount of available memory is determined to be insufficient. The primary design challenges are discussed, and performance results are presented and analyzed.",2011
Global/Local Iteration for Blended Composite Laminate Panel Structure Optimization Subproblems,Composite panel structure optimization is commonly decomposed into panel optimization subproblems. Previous work applied a guide based design approach to the problem for a structure where the local loads were assumed to be fixed for each panel throughout the design process. This paper examines the application of guide based design to a more realistic representation of the structure where the local loads for each panel are determined through a global level analysis that is coupled with the stacking sequence for every design panel. A small problem is selected for which an exhaustive search of the subproblem design space verifies the optimality of the solution found through the global/local iteration process introduced in this work. The efficient discovery of solutions to these guide based design subproblems creates an opportunity to incorporate the solutions into a global level optimization process. A parallel genetic algorithm is proposed to control global optimization in which evaluating the fitness of each member of the population requires the solution of a guide based design subproblem where parallelism is solely within fitness evaluations. Results are presented for a wingbox design problem and compared with known solutions for the same problem to demonstrate weight reductions in a problem thought to already be near optimally solved.,2005
Natural Categories for More Natural Generation,"Psychological research has shown that natural taxonomies contain a distinguished or basic level.  Adult speakers use the names of these categories most frequently and can list a large number of attributes for them.  They typically cannot list many attributes for superordinate categories and few list additional attributes for subordinate categories.  Because natural taxonomies are important to human language, their use in natural language processing systems appears well founded. In the past, however, most AI systems have been implemented around uniform taxonomies in which there is no distinguished level.  It has recently been demonstrated that natural taxonomies enhance language processing systems by allowing selection of appropriate category names and by providing the means to handle implicit focus.  In previous research, we have argued that benefits from the use of natural categories can be realized in multi-sentential connected generation systems.  We briefly summarize the psychological research on natural taxonomies that relates to natural language processing systems, the use of natural categorizations in current natural language processing systems, and the results of our previous research in which we show how natural categories can be used in multiple sentence generation systems to allow the selection of appropriate category names, to provide a mechanism to help determine salience, and to provide for the shallow modeling of audience expertise.  We then describe additional benefits of natural categories in generation systems by demonstrating that natural categories provide a mechanism that aids selection of discourse schemes and increase the efficiency of inheritance.",1990
Preconditioned Conjugate Gradient Algorithms for Homotopy CurveTracking,"These are alogorithms for finding zeros or fixed points of nonlinear systems of equations that are globally convergent for almost all starting points, i.e., with probability one.  The essence of all such algorithms is the construction of an appropriate homotopy map and then tracking some smooth curve in the zero set of this homotopy map. HOMPACK is a mathematical software package implementing globally convergent homotopy algorithms with three different techniques for tracking a homotopy zero curve, and has separate routines for dense and sparse Jacobian Matrices. The HOMPACK alogorithms for sparse Jacobian matrices use a preconditioned conjugate gradient algorithm for the computation of the kernel of the homotopy Jacobian matrix, a required linear algebra step for homotopy curve tracking.  Here variants of the conjugate gradient algorithms are implemented in the context of homotopy curve tracking and compared with Craig's preconditioned conjugate gradient method used in HOMPACK.  The test problems used include actual large scale, sparse structural mechanics problems.",1989
Message Length Effects for Solving Polynomial Systems on a Hypercube,Comparisons between problems solved on uniprocessor systems and those solved on distributed computing systems generally ignore the overhead associated with information transfer from one process to another. This paper considers the solution of polynomial systems of equations via a globally convergent homotopy algorithm on a hypercube and some timing results for different situations.,1986
Evaluation of the Effectiveness of Cloning Techniques for Architectural Virtual Environments,We made the first attempt towards building effective domain-specific interaction techniques for a cloning task. Five interaction techniques were designed and evaluated considering different aspects of domain requirements and human limitations. We demonstrated their effectiveness of designed techniques in two usability studies. The results suggested that no single technique is best for all task conditions. Techniques designed for cloning improved the domain task performance profoundly. The work suggests a further direction: passing domain knowledge to the design process to increase the usefulness of VEs.,2005-09-01
A class of implicit-explicit two-step Runge-Kutta methods,"This work develops implicit-explicit time integrators based on two-step Runge-Kutta methods. The class of schemes of interest is characterized by linear invariant preservation and high stage orders. Theoretical consistency and stability analyses are performed to reveal the properties of these methods.  The new framework offers extreme flexibility in the construction of partitioned integrators, since no coupling conditions are necessary.  Moreover, the methods are not plagued by severe order reduction, due to their high stage orders. Two practical schemes of orders four and six are constructed, and are used to solve several test problems. Numerical results confirm the theoretical findings.",2012-02-01
Building Digital Libraries from Simple Building Blocks,"Metadata harvesting has been established by the Open Archives Initiative (OAI) as a viable mechanism for connecting a provider of data to a purveyor of services. The Open Digital Library (ODL) model is an emerging framework which attempts to break up the services into appropriate components based also on the basic philosophy of the OAI model. This framework has been applied to various projects and evaluated for its simplicity, extensibility and reusability to support the hypothesis that digital libraries (DLs) should be built from simple Web Service-like components instead of as monolithic software applications.",2003
From Cluster to Grid: A Case Study in Scaling-Up a Molecular Electronics Simulation Code,"This paper describes an ongoing project whose goal is to significantly improve the performance and applicability of a molecular electronics simulation code. The specific goals are to (1) increase computational performance on the simulation problems currently being solved by our physics collaborators; (2) allow much larger problems to be solved in reasonable time; and (3) expand the set of resources available to the code, from a single homogeneous cluster to a campus-wide computational grid, while maintaining acceptable performance across this larger set of resources. We describe the sequential performance of the code, the performance of two parallel versions, and the benefits of problem-specific load balancing strategies. The grid context motivates the need for runtime algorithm selection; we present a component-based software framework that makes this possible.",2002-11-01
Proceedings of the First Annual Virginia Tech Center for Human-Computer Interaction Research Experience for Undergraduates (REU) Symposium,"Virginia Tech's Center for Human-Computer Interaction presents the project abstracts for the REU ’06 symposium. The REU (Research Experience for Undergraduates) program provides undergraduate students from various universities with the opportunity to spend eight weeks at Virginia Tech, working with our faculty and graduate students on research projects using the state-of-the-art technology and laboratories assembled here. The REU program is sponsored by a National Science Foundation grant IIS-0552732.",2006-07-01
Configuration Management for Reusable Software ?,"This paper discusses the configuration management of reusable software, and proposes a software libarary architecture that incorporates configuration management.?",2002-12-01
Hierarchical Constraint Satisfaction as a Model for Adding Types with Inheritance to Prolog,"Prolog is a language based on first order logic.  It uses resolution as a rule of inference, and unification is the heart of resolution. Prolog operates on the Herbrand universe, a single, unstructured domain.  In problems with large structured domains, the number of resolution steps may become large.  We have incorporated type inheritance into Prolog to exploit large structured domains to write more concise code and to obtain shorter proofs.  Types are subuniverses corresponding to sets of objects.  The subset of relation between types induces a hierarchy on the universe.  We used the hierarchical constraint satisfaction concept to incorporate these extensions into Prolog.  We also provide a formal proof that our typed unification extends standard Prolog and directly augments the Warren Abstract Machine (WAM) concept.",1990
Time-Parallel Simulation Using Partial State Matching for Queueing Systems,This paper describes partial state matching for approximate time-parallel simulation. The notion of degree of freedom in time-parallel simulation is introduced. Two partial state matching algorithms are proposed to simulate acyclic networks of FCFS G/G/1/K queues in which arriving customers that find the queue full are lost. The algorithms are suitable for SIMD as well as MIMD architectures. The performance of the algorithms is studied. Experiment results with M/M/1/K and M/D/1/K queueing networks show that the potential speedup and simulation accuracy of the algorithms are good. The worst performance of both algorithms occurs when traffic intensity is one. Arguments are made to explain this phenomenon.,1992
The use of Software Quality Metrics in Software Maintenance,"This paper reports on a modest study which relates seven different software complexity metrics to the experience of maintenance activities performed on a medium size sofhvare system. Three different versions of the system that evolved over aperiod of three years were analyzed in this study. A major revision of the system, while still in its design phase, was also analyzed. The results of this study indicate: (1) that the growth in system complexity as determined by the software metrics agree with the general character of the maintenance tasks performed in successive versions; (2) the metrics were able to identify the improper integration of functional enhancements made to the system.; (3) the complexity values of the system components as indicated by the memcs conform well to an understanding of the system by people familiar with the system.; (4) an analysis of the redesigned version of the system showed the usefulness of software metrics in the (re)design phase by revealing a poorly sstructured component of the system This work was supported, in part, by grants from the National Science Foundation (MCS-8103707, DCR-8207110, DCR-8418257).",1985
A Procedure for Evaluation Human-Computer Interface Development,"An evaluation procedure that uses a standardized technique to produce quantifiable criteria for evaluating and comparing human-computer interface development tools is described in this paper.  An empirical validation study to determine the consistency of ratings produced by this procedure is also presented.  These ratings could be used, for example, as important data for the task of choosing a tool for a particular human-computer interface development environment.",1989
Compositional Mining of Multi-Relational Biological Datasets,"High-throughput biological screens are yielding ever-growing streams of information about multiple aspects of cellular activity. As more and more categories of datasets come online, there is a corresponding multitude of ways in which inferences can be chained across them, motivating the need for compositional data mining algorithms. In this paper, we argue that such compositional data mining can be effectively realized by functionally cascading redescription mining and biclustering algorithms as primitives. Both these primitives mirror shifts of vocabulary that can be composed in arbitrary ways to create rich chains of inferences. Given a relational database and its schema, we show how the schema can be automatically compiled into a compositional data mining program, and how different domains in the schema can be related through logical sequences of biclustering and redescription invocations. This feature allows us to rapidly prototype new data mining applications, yielding greater understanding of scientific datasets. We describe two applications of compositional data mining: (i) matching terms across categories of the Gene Ontology and (ii) understanding the molecular mechanisms underlying stress response in human cells.",2007-08-01
Architecture-Aware Optimization on a 1600-core Graphics Processor,"The graphics processing unit (GPU) continues to make significant strides as an accelerator in commodity cluster computing for high-performance computing (HPC). For example, three of the top five fastest supercomputers in the world, as ranked by the TOP500, employ GPUs as accelerators. Despite this increasing interest in GPUs, however, optimizing the performance of a GPU-accelerated compute node requires deep technical knowledge of the underlying architecture. Although significant literature exists on how to optimize GPU performance on the more mature NVIDIA CUDA architecture, the converse is true for OpenCL on the AMD GPU. Consequently, we present and evaluate architecture-aware optimizations for the AMD GPU. The most prominent optimizations include (i) explicit use of registers, (ii) use of vector types, (iii) removal of branches, and (iv) use of image memory for global data. We demonstrate the efficacy of our AMD GPU optimizations by applying each optimization in isolation as well as in concert to a large-scale, molecular modeling application called GEM. Via these AMD-specific GPU optimizations, the AMD Radeon HD 5870 GPU delivers 65% better performance than with the wellknown NVIDIA-specific optimizations.",2011-07-01
The Challenges of Web Engineering and Requirements for Better Tool Support,"We report the experiences of semi-professional developers regarding the challenges, tools, and processes within the domain of web application development. The paper summarizes the main problems in web development, characterizes the habits of programmers and concludes with a ""developer's wish list"" for improvements to web technologies and tools. The report is based on two independent sources - a survey of 31 web developers and an in-depth interview study with 10 participants.",2005
Simulation Support: Prototyping the Automation-Based Paradigm,"This paper describes our research efforts in prototyping the automation-based software paradigm to provide automated support for discrete-event simulation model development. The automation-based paradigm has been suggested as the software technology in the 1990's. The technology needed to support this paradigm does not yet exist. However, the benefits to be gained are so significant that, if achieved, it could profoundly change the way that simulation models are developed. We have been working to achieve this paradigm in the form of an environment composed of an integrated and comprehensive collection of computer-based tools. Our prototyping efforts have focused on the Model Generator, Model Analyzer, and Assistance Manager tools. The Model Generator tool is crucial for the realization of the paradigm and three prototypes have been developed.  Our experimentations with the prototypes indicate that the paradigm can be achieved if a small problem domain is chosen. The problem becomes quite complex in the domain-independent case; nevertheless, we believe that the challenge can be met by way of an evolutionary development of prototypes.",1987
A Homotopy Algorithm for the Combined H-squared/H-to Infinity Model Reduction Problem,"The problem of finding a reduced order model, optimal in the H-squared sense, to a given system model is a fundamental one in control system analysis and design.  The addition of a H-to infinity constraint to the H-squared optimal model reduction problem results in a more practical yet computationally more difficult problem.  Without the global convergence of probability-one homotopy methods the combined H-squared/H-to infinity model reduction problem is difficult to solve. Several approaches based on homotoppy methods have been proposed.  The issues are the number of degrees of freedom, the well posedness of the finite dimensional optimization problem, and the numerical robustness of the resulting homotopy algorithm.  Homotopy algorithms based on two formulations - input normal form; Ly, Bryson, and Cannon's 2 x 2 block parametrization - are developed and compared here.",1993-05-01
Minimax Resource Allocation with Continuous Variables: The Definitive Solution,"The necessary and sufficient conditions of local global optimization are derived for constrained resource allocation with continuous variables and objective functions of the forms max (sub i) {f-sub i(x-sub i)} and mini{f-sub i(x-sub i)} where {f-sub i} can be nondifferentiable, nonmonotone, nonconvex, multimodal functions.  All previous theoretical results, which are sufficient conditions for global optimization with monotone {f-sub i}, are special, restrictive cases of the new criteria.  The powerful criteria also enable complete determination of all the global optimal solutions, thus allowing further lexicographic optimization.  The criteria also enable determination of all the local maxima and minima, a previously unaddressed facet of the solution, thus providing illuminating information on the behavior of the objective function and its overall ""topography"", which could be useful in suboptimal multi-criteria trade-offs.  The new results admit a straightforward graphical interpretation and implementation, which facilitates their utilization and extends their applicability to practical problems where {f-sub i} are specified only in graphical formats derived from empirical or simulation data.  Except for the mild and practically insignificant restrictions of continuity and ""local monomodality"" retained on {f-sub i} by the analysis, the results of this paper constitute the complete and definitive solution of the problem.",1990
Document Translation: Dissertations and Technical Reports,"This report describes the work on document translation, electronic publishing and integrating WATERS and Techrep carried out during Summer 93.  The documents used for the project were Ph.D. dissertations and, in related efforts, technical reports.  Because users often edit documents with different software and then experience problems moving them between different formats (e.g., MS Word to FrameMaker), it would be helpful to have a common format that is independent of the operating system and software on which the document was originally created.  In this report, we describe different approaches possible to develop and establish sich a format.  Section 2 discusses the access system necessary for a large database.  Section 3 talks about the electronic page image approach for storing the document electronically.  Section 4 describes the markup approach taken for developing an independent document format.  Section 5 describes future work.",1993
Globally Convergent Homotopy Algorithms for Nonlinear Systems of Equations,"Probability-one homotopy methods are a class of algorithms for solving nonlinear systems of equations that are accurate, robust, and converge from an arbitrary starting point almost surely.  These new globally convergent homotopy techniques have been successfully applied to solve Brouwer fixed point problems, polynomial systems of equations, constrained and unconstrained optimization problems, discretizations of nonlinear two-point boundary value problems based on shooting, finite differences, collocation, and finite elements, and finite difference, collocation, and Galerkin approximations to nonlinear partial differential equations.  This paper introduces, in a tutorial fashion, the theory of globally convergent homotopy algorithms, describes some computer algorithms and mathematical software, and presents several nontrivial engineering applications.",1990
Development and Testing of an Evaluation Procedure for User InterfaceManagement (UIMS),"A user interface management system or UIMS is an interactive system for supporting the design, production, and execution of human-computer interfaces.  This paper reports on the development and empirical testing of an evaluation procedure to produce quantifiable criteria for evaluating and comparing UIMS.  The form-based evaluation procedure results in quantitative ratings along two dimensions: functionality and usability.  Specification/implementation techniques used by a UIMS are also quantitatively rated.  An empirical study has indicated that the procedure produces reliable, useful results.",1989
Device-Based Isolation for Securing Cryptographic Keys,"In this work, we describe an eective device-based isolation approach for achieving data security. Device-based isolation leverages the proliferation of personal computing devices to provide strong run-time guarantees for the condentiality of secrets. To demonstrate our isolation approach, we show its use in protecting the secrecy of highly sensitive data that is crucial to security operations, such as cryptographic keys used for decrypting ciphertext or signing digital signatures. Private key is usually encrypted when not used, however, when being used, the plaintext key is loaded into the memory of the host for access. In our threat model, the host may be compromised by attackers, and thus the condentiality of the host memory cannot be preserved. We present a novel and practical solution and its prototype called DataGuard to protect the secrecy of the highly sensitive data through the storage isolation and secure tunneling enabled by a mobile handheld device. DataGuard can be deployed for the key protection of individuals or organizations.",2012
"Incremental, Semi-automatic, Mapping-Based Integration of Heterogeneous Collections into Archaeological Digital Libraries: Megiddo Case Study","Automation is an important issue when integrating heterogeneous collections into archaeological digital libraries. We propose an incremental approach through intermediary- and mapping-based techniques. A visual schema mapping tool within the 5S framework allows semi-automatic mapping and in-cremental global schema enrichment. 5S also helped speed up development of a new multi-dimensional browsing service. Our approach helps integrate the Me-giddo excavation data into a growing union archaeological DL, ETANA-DL.",2005
Load-Varying LINPACK: A Benchmark for Evaluating Energy Efficiency in High-End Computing,"For decades, performance has driven the high-end computing (HEC) community. However, as highlighted in recent exascale studies that chart a path from petascale to exascale computing, power consumption is fast becoming the major design constraint in HEC. Consequently, the HEC community needs to address this issue in future petascale and exascale computing systems. Current scientific benchmarks, such as LINPACK and SPEChpc, only evaluate HEC systems when running at full throttle, i.e., 100% workload, resulting in a focus on performance and ignoring the issues of power and energy consumption. In contrast, efforts like SPECpower evaluate the energy efficiency of a compute server at varying workloads. This is analogous to evaluating the energy efficiency (i.e., fuel efficiency) of an automobile at varying speeds (e.g., miles per gallon highway versus city). SPECpower, however, only evaluates the energy efficiency of a single compute server rather than a HEC system; furthermore, it is based on SPEC's Java Business Benchmarks (SPECjbb) rather than a scientific benchmark. Given the absence of a load-varying scientific benchmark to evaluate the energy efficiency of HEC systems at different workloads, we propose the load-varying LINPACK (LV-LINPACK) benchmark. In this paper, we identify application parameters that affect performance and provide a methodology to vary the workload of LINPACK, thus enabling a more rigorous study of energy efficiency in supercomputers, or more generally, HEC.",2010-12-01
Reinforcing Reachable Routes,"This paper studies the evaluation of routing algorithms from the perspective of reachability routing, where the goal is to determine all paths between a sender and a receiver. Reachability routing is becoming relevant with the changing dynamics of the Internet and the emergence of low-bandwidth wireless/ad-hoc networks. We make the case for reinforcement learning as the framework of choice to realize reachability routing, within the confines of the current Internet infrastructure. The setting of the reinforcement learning problem offers several advantages,including loop resolution, multi-path forwarding capability, cost-sensitive routing, and minimizing state overhead, while maintaining the incremental spirit of current backbone routing algorithms. We identify research issues in reinforcement learning applied to the reachability routing problem to achieve a fluid and robust backbone routing framework. This paper also presents the design, implementation and evaluation of a new reachability routing algorithm that uses a model-based approach to achieve cost-sensitive multi-path forwarding; performance assessment of the algorithm in various troublesome topologies shows consistently superior performance over classical reinforcement learning algorithms. The paper is targeted toward practitioners seeking to implement a reachability routing algorithm.",2003
Exposing Useful Trends in Metric Data Through Group Level Analysis,"In this paper the results of experiments which applied both structure and code metrics to three large scale systems are presented. This metric research is distinct in that trends in the data are uncovered through the use of group level analysis. Components are partitioned into groups based on their various metric values and on observed measures of complexity (ie. errors, coding time). Crosstabulation data is given which indicates that trends between some of the metrics and the observed data do exist. Code metrics typically formed groups of increasing complexity which corresponded to increases in the mean values of the observed data. The strength of the Information Flow metric and the Invocation measure is their ability to form a group containing highly complex components which was found to be populated by outliers in the observed data.",1985
Cluster Algebra: A Query Language for Heterogeneous Databases,"This report describes a query language based on algebra for heterogeneous databases. The database logic is used as a uniform framework for studying the heterogeneous databases. The data model based on the database logic is referred to as cluster data model in this report. Generalized Structured Query Language (GSQL) is used for expressing ad-hoc queries over the relational, hierarchical and network database uniformly. For the purpose of query optimization, a query language that can express the primitive heterogeneous database operations is required. This report describes such a query language for the clusters (i.e., heterogeneous databases). The cluster algebra consists of (a) generalized relational operations such as selection, union, intersection, difference, semi-join, rename and cross-product; (b) modified relational operations such as normal projection and normal join; and (c) new operations such as normalize, embed, and unembed.",1992
A Reliability Model Incorporating Software Quality Factors,"In this paper we describe our initial work on a long-term project to develop and validate a reliability model and a new class of software complexity metrics which are related to this model. In contrast to previous ""black box"" approaches, the reliability model is novel because it incorporates knowledge about the system in the form of quantitative software complexity metrics. While the initial model uses existing software metrics a parallel effort in this project is investigating new classes of metrics, interface and dynamic metrics, which are useful in their own right but are also of particular relevance to the reliability model. The initial definitions of both the model and the metrics are given along with a description of the next research milestones.",1988-05-01
Deceleration of a Porous Rotating Disk in a Viscous Fluid,"The flow due to a rotating disk decelerating with an angular velocity  inversely proportional to time with either surface suction (or injection)  which again varies with time is investigated. The unsteady Navier-Stokes  equations are transformed to non-linear ordinary differential equations  using similarity transformations. The resulting equations are solved  numerically using a globally convergent homotopy method. The flow depends  on two non-dimensional parameters, namely an unsteadiness parameter S and  a suction (or injection) parameter A. Some interesting numerical results  are presented graphically and discussed.",1983
Magnetohydrodynamic Flow Past a Porous Rotating Disk in a Circular Magnetic Field,"This paper studies the effects of a circular magnetic field on the flow of a conducting fluid about a porous rotating disk.  Using modern quasi-Newton and globally convergent homotopy methods, numerical solutions are obtained for a wide range of magentic field strengths, suction and injection velocities and Alfven and disk speeds.  Results are presented graphically in terms of three nondimensional parameters. There is excellent agreement with previous work and asymptotic formulas.",1987
An Annotated Bibliography on Optimization of Programs During Compilation,This annotated bibliography covers a collection of articles assembled by the authors in a graduate level course on advanced compilation techniques.,1978
Enrichment Procedures for Soft Clusters: A Statistical Test and its Applications,"Clusters, typically mined by modeling locality of attribute spaces, are often evaluated for their ability to demonstrate ‘enrichment’ of categorical features. A cluster enrichment procedure evaluates the membership of a cluster for significant representation in pre-defined categories of interest. While classical enrichment procedures assume a hard clustering deﬁnition, in this paper we introduce a new statistical test that computes enrichments for soft clusters. We demonstrate an application of this test in reﬁning and evaluating soft clusters for classification of remotely sensed images.",2010-02-01
Analysis of the Worst Case Space Complexity of a PR Quadtree,"We demonstrate that a resolution-r PR quadtree containing n points has, in the worst case, at most  nodes. This captures the fact that as n tends towards 4r, the number of nodes in a PR quadtree quickly approaches O(n). This is a more precise estimation of the worst case space requirement of a PR quadtree than has been attempted before.",1992
ODRPACK95: A Weighted Orthogonal Distance Regression Code with Bound Constraints,"ODRPACK (TOMS Algorithm 676) has provided a complete package for weighted orthogonal distance regression for many years. The code is complete with user selectable reporting facilities, numerical and analytic derivatives, derivative checking, and many more features. The foundation for the algorithm is a stable and efficient trust region Levenberg-Marquardt minimizer that exploits the structure of the orthogonal distance regression problem. ODRPACK95 is a modification of the original ODRPACK code that adds support for bound constraints, uses the newer Fortran 95 language, and simplifies the interface to the user called subroutine.",2004
Integration of Heterogeneous Digital Libraries with Semi-automatic Mapping and Browsing: From Formalization to Specification to Visualization,"In this paper, we formalize the digital library (DL) integration problem and propose an overall approach based on the 5S framework. We apply 5S to domain-specific (archaeological) DLs, illustrating our solutions for key problems in DL integration. We use ETANA-DL as a case study to describe the process of semi-automatically generating a union catalog and a unified browsing service in an archaeological DL. A visual schema mapping tool is developed for union catalog creation. A pilot user study aids tool evaluation. Our approach is further validated through application of a general browsing component to two integrated DLs.",2005
Source Book on Digital Libraries,"This extensive report outlines the steps necessary to create a national, electronic Science, Engineering and Technology Library.  Step one is for NSF to play a lead role in launching a concerted R&D program in the area.  Step two involves partnerships, cooperative ventures, and production conversion of backarchives.  ARPA, NASA, NIST, Library of Congress, NLM, NAI, and many other groups must become involved if we are to serve the broad base of users; it will only be successful if supported by top-quality research on information storage and retrieval, hypertext, document processing, human-computer interaction, scaling up of information systems, networking, multimedia systems, visualization, education, and training.  NOTE:  Because of its large size, this reports is not available in hard copy from the department.  It can be obtained electronically through anonynous FTP to fox.cs.vt.edu (in directory /pub/DigitalLibrary).  To obtain a hard copy, write to Mark Roope at University Printing Services; ""Documents on Demand""; Virginia Tech; Blacksburg VA 24061-0243; or call (703) 231-6701.",1993
Probabilistic Modeling of Errors from Structural Optimization Based on Multiple Starting Points,"With optimization increasingly used in engineering applications, a series of optimization runs may be required, and it may be too expensive to converge them to very high accuracy. A procedure for estimating average optimization convergence errors from a set of poorly converged optimization runs is developed. A probabilistic model is fitted to the errors in optimal objective function values of poorly converged runs. The Weibull distribution was identified as a reasonable error model both for the Rosenbrock function problem and the structural optimization of a high speed civil transport. Once a statistical model for the error is identified, it can be used to estimate average errors from a set of pairs of runs. In particular, by performing pairs of optimization runs from two starting points, accurate estimates of the mean and standard deviation of the convergence errors can be obtained.",2002-07-01
"Social Media for Cities, Counties and Communities","Social media (i.e., Twitter, Facebook, Flickr, YouTube) and other tools and services with user- generated content have made a staggering amount of information (and misinformation) available. Some government officials seek to leverage these resources to improve services and communication with citizens, especially during crises and emergencies. Yet, the sheer volume of social data streams generates substantial noise that must be filtered. Potential exists to rapidly identify issues of concern for emergency management by detecting meaningful patterns or trends in the stream of messages and information flow. Similarly, monitoring these patterns and themes over time could provide officials with insights into the perceptions and mood of the community that cannot be collected through traditional methods (e.g., phone or mail surveys) due to their substantive costs, especially in light of reduced and shrinking budgets of governments at all levels. We conducted a pilot study in 2010 with government officials in Arlington, Virginia (and to a lesser extent representatives of groups from Alexandria and Fairfax, Virginia) with a view to contributing to a general understanding of the use of social media by government officials as well as community organizations, businesses and the public. We were especially interested in gaining greater insight into social media use in crisis situations (whether severe or fairly routine crises, such as traffic or weather disruptions).",2011
Taming Multi-core Parallelism with Concurrent Mixin Layers,"The recent shift in computer system design to multi-core technology requires that the developer leverage explicit parallel programming techniques in order to utilize available performance.  Nevertheless, developing the requisite parallel applications remains a prohibitively-difficult undertaking, particularly for the general programmer.  To mitigate many of the challenges in creating concurrent software, this paper introduces a new parallel programming methodology that leverages feature-oriented programming (FOP) to logically decompose a product line architecture (PLA) into concurrent execution units.  In addition, our efficient implementation of this methodology, that we call concurrent mixin layers, uses a layered architecture to facilitate the development of parallel applications.  To validate our methodology and accompanying implementation, we present a case study of a product line of multimedia applications deployed within a typical multi-core environment.  Our performance results demonstrate that a product line can be effectively transformed into parallel applications capable of utilizing multiple cores, thus improving performance.  Furthermore, concurrent mixin layers significantly reduces the complexity of parallel programming by eliminating the need for the programmer to introduce explicit low-level concurrency control.  Our initial experience gives us reason to believe that concurrent mixin layers is a promising technique for taming parallelism in multi-core environments.",2008
A Total Algorithm for Polynomial Roots Based Upon Bairstow's Method,"This program uses Bairstow's method to find the real and  complex roots of a polynomial with real coefficients. There are several reasons  for developing a routine based upon Bairstow's method. It is sometimes the  case that all of the roots of a polynomial with real coefficients are desired.  Bairstow's method provitles an iterative process for finding both the real and  complex roots using only real arithmetic. Further, since it is based on  Newton's method for a system of two nonlinear equations in two unknowns, it  has the rapid convergence property of Newton's method for systems of equations.  The major drawback of this method is that it sometimes fails to converge  [11, p. 110]. This is because it is difficult to find an initial starting  guess which satisfies the strict conditions necessary to assure convergence.  When these conditions are not satisfied, the sequence of approximations may  jump away from the desired roots or may iterate away from the roots indefinitely.",1974
Requirements for Model Development Environments,"This paper deals with the initial phase of our ongoing  research project on the Definition of a Discrete Event Simulation MDE which started on 1 June 1983. The first phase of the rapid prototyping approach we are using in designing the MDE involves the requirements specification. A literature review revealed eleven current problems in modeling. To address these problems, a MDE was identified as composed of four layers: (1) hardware and operating system, (2) kernel MDE, (3) minimal MDE, and (4:) MDEs. Requirements were then perceived for each layer and are reported in this paper. The feasibility of the requirements have been assessed throughout our proto typing efforts. This paper has provided significant guidance to our research group in designing the MDE and its associated tools. We believe that the designers and implementers of other types of MDEs can benefit from the research described herein.",1983
Comparison of a Graphical and a Textual Design Language Using Software Quality Metrics,"For many years the software engineering community has been attacking the software reliability problem on two fronts. First via design methodologies, languages and tools as a precheck on quality and second by measuring the quality of produced software as a postcheck.  This research attempts to unify the approach to creating reliable software by providing the ability to measure the quality of a design prior to its implementation. A comparison of a graphical and a textual design language is presented in an effort to support research findings that the human brain works more effectively in images than in text.",1988
Schwarz Splitting Using ELLPACK,"This report describes a high level implementation of Schwarz splitting, an approach to solving partial differential equations which seems particularly attractive from the point of view of parallelism. We discuss the basic scheme of Schwarz splitting and an implementation using ELLPACK. We consider two numerical examples in detail. The appendices contain a listing of an ELLPACK program which implements Schwarz splitting, and a complete set of data from the two examples. The examples were done on a sequential machine, but they provide  clues as to the potential for parallelism exhibited by the Schwarz technique.",1988
Contemplations of a Simulated Navel,"The Model Development Environment Project has the goal of defining the software utilities and the database support needed for creating, validating, and experimenting with complex simulation models. This project review, emphasizing the needs and explaining some of the guiding concepts and principles, serves to underscore key issues extending beyond discrete event simulation. An introspective summary presents an optimistic reaction to the fear that technically naive modelers might use the more sophisticated capabilities to produce catastrophic results.",1988
New Homotopy Solution Techniques Applied to Variable Geometry Trusses,"A VGT, or Variable Geometry Truss, can be thought of as a statically determinate truss that has been modified to contain some number of variable length members.  These extensible members allow the truss to vary its configuration in a controlled manner.  Some of the typical applications envisioned for VGTs are booms to position equipment in space, as supports for space antennae, and as berthing devices. Recently, they have also been proposed as parallel-actuated, long-chain, high dexterity manipulators.  This paper will demonstrate the use of homotopy continuation in solving the kinematics of relatively complex variable geometry trusses (VGTs), including the octahedron and the decahedron.  The procedural aspects are described in detail with the help of examples.  Results of the example problems are also presented.",1990
Vorticity Induced by a Moving Elliptic Belt,The viscous flow inside an elliptic moving belt is studied using Newton's method on a Hermite collocation approximation.  The streamlines and especially the vorticity distribution are found for Reynolds number up to 1000 and aspect ratio up to five.  For low Reynolds numbers vorticity diffuses from regions of high curvature.  For high Reynolds numbers there exists a closed boundary layer and a core of constant vorticity.  The core vorticity compares well with the estimation from the mean square law.,1989
RAxML-Cell: Parallel Phylogenetic Tree Inference on the Cell Broadband Engine,"Phylogenetic tree reconstruction is one of the grand challenge problems in Bioinformatics. The search for a best-scoring tree with 50 organisms, under a reasonable optimality criterion, creates a topological search space which is as large as the number of atoms in the universe. Computational phylogeny is challenging even for the most powerful supercomputers. It is also an ideal candidate for benchmarking emerging multiprocessor architectures, because it exhibits various levels of fine and coarse-grain parallelism. In this paper, we present the porting, optimization, and evaluation of RAxML on the Cell Broadband Engine.  RAxML is a provably efficient, hill climbing algorithm for computing phylogenetic trees based on the Maximum Likelihood (ML) method.  The algorithm uses an embarrassingly parallel search method, which also exhibits data-level parallelism and control parallelism in the computation of the likelihood functions. We present the optimization of one of the currently fastest tree search algorithms, on a real Cell blade prototype.  We also investigate problems and present solutions pertaining to the optimization of floating point code, control flow, communication, scheduling, and multi-level parallelization on the Cell.",2006
Dissipative Waves in Fluids Having Both Positive and Negative Nonlinearity,"The present study examines weakly dissipative, weakly nonlinear waves in which the fundamental derivative changes sign. The undisturbed state is taken to be at rest, uniform and in the vicinity of the 0 locus. The cubic Burgers equation governing these waves is solved numerically; the resultant solutions are compared and contrasted to those of the invisced theory. Further results include the presentation of a natural scaling law and inviscid solutions not reported elsewhere.",1986
An Exact Update for Harris' Tread,The purpose of this note is to show how Harris' TREAD value can be  computed without approximation.,1975
Information Storage and Retrieval of Composite Documents: A User Oriented Model of a Computer Message System,"This paper outlines a new model of computer message systems based on a useroriented rather than a communications perspective. Most network users spend their time working with various types of 'composite documents' such as mail messages, news, reports, or entries in directories of names. They perform operations that are aided by a logically centralized but physically distributed world-wide information storage and retrieval system. Their concerns should be with creating, filing, submitting, searching, and retrieving documents, not with routing or transmission matters. Computer networks are rapidly proliferating. Their number and size are increasing, and interconnection is commonplace. High vol~ime applications like mail and news transmission are well established in offices, large corporations, and governmentally supported internets. Computer conferencing and directory assistance services are continuing to evoive. Standards for message formats, transmission protocols, and name/address directories have been recommended by DARPA, NBS, and CCI'IT. Research in document modeling, distributing databases, and retrieving text objects is being adapted to improve the functionality of computer message systems. Advanced techniques for text analysis, automatic indexing, query construction, description of comprehensive interest profiles, and retrospective or current awareness retrieval can simplify and improve the effectiveness of user agent software. Initial experimentation has shown the value of recognizing the structure of composite documents. Current work with the CSNET name server database, and with a collection of messages extracted from AIList digests distributed over the DARPA Internet, indicates that the proposed model accomodates mail, news, and directory assistance. Other work with book passage retrieval suggests that large files can be processed too. It is hoped that integration of these services, along with conferencing and database handling, will occur, as a new network for the Virginia Center for Innovative Technology is proposed and eventually implemented.",1985-08-01
"Reducing the Mean Time to Remove Faults Through Early Fault Detection, an Experiment in Independent Verification and Validation","This paper presents the results of a study investigating the extent to which Independent Verification and Validation (IV&V) impacts early fault detection, and consequently, the mean time to remove faults. Two separate development groups, one with an IV&V team, are tasked to produce a software system from the same set of requirements.  For each development phase, fault detection and removal data are recorded.  An analysis of that data reveals that the group having the IV&V contingent: (a) detected errors earlier in the software development process, and (b) on the average, required substantially less time to remove those faults.",1996
A Digital Library Framework for Biodiversity Information Systems,"Biodiversity information systems (BISs) involve all kinds of heterogeneous data, which include ecological and geographical features. However, available information systems offer very limited support for managing such data in an integrated fashion. Furthermore, such systems do not fully support image content management (e.g., photos of landscapes or living organisms), a requirement of many BIS end-users. In order to meet their needs, these users - e.g., biologists, environmental experts - often have to alternate between distinct biodiversity and image information systems to combine information extracted from them. This cumbersome operational procedure is forced on users by lack of interoperability among these systems. This hampers the addition of new data sources, as well as cooperation among scientists. The approach provided in this paper to meet these issues is based on taking advantage of advances in Digital Library (DL) innovations to integrate networked collections of heterogeneous data. It focuses on creating the basis for a biodiversity information system under the digital library perspective, combining new techniques of content-based image retrieval and database query processing mechanisms. This approach solves the problem of system switching, and provides users with a flexible architecture from which to tailor a BIS to their needs. To illustrate the use of this architecture, it has been instantiated to support the creation of a BIS for fish species in a real application. The goal is to help researchers on ichthyology to identify fish specimen by using search retrieval techniques. Experimental results suggest that this new approach improves the effectiveness of the fish identification process, if compared to the tradition key-based method.",2004
CPU MISER: A Performance-Directed Run-Time System for Power Aware Cluster,"Performance and power are two primary design constraints in today’s high-end computing systems. Because of the inherent dependency between performance and power, reducing power consumption without impacting system performance is a challenge for the HPC community. In this paper, we present a run-time system as well as its underlying performance model for performance-directed, power-aware cluster computing. Experimental results based on physical measurements show that NPB benchmarks benefit up to 36% energy saving and 21% performance gain. On average, our run-time system leads to 10.7% energy saving with 1.2% performance loss over 9 NPB benchmarks, and is 1.59X improvement in ED2P than CPUSPEED. We also show that our system is performance directed in the sense that the performance loss for most application is within the user specified limit. We attribute the promising results to the accurate performance modeling and prediction, and effective performance control techniques.",2007
"Chitra: Visual Analysis of Parallel and Distributed Programs in the Time, Event, and Frequency Domains","Chitra analyzes a program execution sequence (PES) collected during execution of a program and produces a homogeneous, semi-Markov chain model fitting the PES. The PES represents the evolution of a program state vector in time. Therefore Chitra analyzes the time-dependent behavior of a program. The paper describes a set of transforms that map a PES to a simplified PES. Because the transforms are program-independent, Chitra can be used with any program. Chitra provides a visualization of PES's and transforms, to allow a user visually to guide transform selection in an effort to generate a simple yet accurate semi-Markov chain model. The resultant chain can predict performance at program parameters different than those used in the input PES, and the chain structure can diagnose performance problems.",1992
Determining Object Orientations from Run Length Encodings,"Run length codes are widely used for image compression, and efficient algorithms have been devised for identifying objects and calculating geometric features directly from these codes. However, if the image objects are rotated it can be difficult to determine their orientation and position so that they can be grasped by manipulators. This paper describes a method for structural determination of object orientation directly from the run length codes of successive image scan lines.  An algorithm is described that makes use of the equations of object boundary segments to form hypotheses about object orientations that are refined as scanning progresses. 2-dimensional polygonal objects are discussed, and it is assumed that objects do not touch or overlap, although the algorithm could be extended to include those situations.",1987
Functionally Complete Machines,"This paper defines a functionally complete machine as a machine which is capable of evaluating every two place function over its data space. Necessary conditions on memory size for completeness are developed. These conditions are applied to System/360 as modelled by the space of bytes, the space of halfwords, and the space of words. Sufficiently large (> 64K bytes) models of System/360 are shown to be complete for the space of bytes. No models of System/360 are complete for the spaces of halfwords or words. The inequalities developed and known examples of universal decision elements suggest structures for complete machines.",1973
Globally Convergent Parallel Algorithm for Zeros of Polynomial Systems,"Certain classes of nonlinear systems of equations, such as polynomial systems, have properties that make them particularly amenable to solution on distributed computing systems. Some algorithms, considered unfavorably on a single processor serial computer, may be excellent on a distributed system. This paper considers the solution of polynomial systems of equations via a globally convergent homotopy algorithm on a hypercube. Some computational results are reported.",1986-05-01
Progress Towards a World-Wide Code of Conduct,"In this paper, the work of the International Federation for Information Processing (IFIP) Task Force on Ethics is described and the recommendations presented to the General Assembly are reviewed.  While a common code of ethics or conduct has not been recommended for consideration by the member societies of IFIP, a set of guidelines for the establishment and evaluation of codes has been produced and procedures for the assistance of code development have been established within IFIP.  This paper proposes that the data collected by the Task Force and the proposed guidelines can be used as a tool for the study of codes of practice providing a teachable, learnable educational module in courses related to the ethics of computing and computation, and looks at the next steps in bringing ethical awareness to the IT community.",1995-04-01
Computer Analysis of User Interfaces Based on Repetition in Transcripts of User Sessions,"It is generally acknowledged that the production of quality user interfaces requires a thorough understanding of the user and that this involves evaluating the interface by observing the user working with the system, or by performing human factors experiments.  Such methods traditionally involve the use of video tape, protocol analysis, critical incident analysis, etc.  These methods require time consuming analyses and may be invasive.  In addition, the data obtained through such methods represent a relatively small portion of the use of a system.  An alternative approach is to record all user input and systems output, i.e., log the user session.  Such transcripts can be collected automatically and non-invasively over a long period of time. Unfortunately, this produces voluminous amounts of data.  There is, therefore, a need for tools and techniques that allow an evaluator to identify potential performance and usability problems from such data. It is hypothesized that repetition of user actions is an important indicator of potential user interface problems.",1990
Use of Subimages in Fish Species Identification: A Qualitative Study,"Many scholarly tasks involve working with subdocuments, or contextualized fine-grain information, i.e., with information that is part of some larger unit. A digital library (DL) facil- itates management, access, retrieval, and use of collections of data and metadata through services. However, most DLs do not provide infrastructure or services to support working with subdocuments. Superimposed information (SI) refers to new information that is created to reference subdocu- ments in existing information resources. We combine this idea of SI with traditional DL services, to define and develop a DL with SI (SI-DL). We explored the use of subimages and evaluated the use of a prototype SI-DL (SuperIDR) in fish species identification, a scholarly task that involves work- ing with subimages. The contexts and strategies of working with subimages in SuperIDR suggest new and enhanced sup- port (SI-DL services) for scholarly tasks that involve working with subimages, including new ways of querying and search- ing for subimages and associated information. The main contribution of our work are the insights gained from these findings of use of subimages and of SuperIDR (a prototype SI-DL), which lead to recommendations for the design of digital libraries with superimposed information.",2011-03-01
New Results for the Minimum Weight Triangulation Problem,"Given a finite set of points in a plane, a triangulation is a maximal set of non-intersecting line segments connecting the points.  The weight of a triangulation is the sum of the Euclidean lengths of its line segments.  Given a set of points in a plane, the minimum weight triangulation problem is to find a triangulation whose weight is minimal.  No polynomial time algorithm is known to solve this problem, and it is unknown whether the problem is NP-hard.  The current best polynomial time approximation algorithm produces a triangulation that can be 0(log n) times the weight of the optimal triangulation.  We propose an algorithm that triangulates a set P, of n points in a plane in 0(n-cubed) time and that never does worse than the greedy triangulation.  The algorithm produces an optimal triangulation if the points P are the vertices of a convex polygon.  The algorithm has the flavor of a heuristic proposed by Lingas and analysis similar to his can be performed for our algorithm also, but experimental results indicate that our algorithm performs much better than the heuristic of Lingas. The results comparing the optimal triangulation with the performance of our algorithm, the heuristic of Lingas, and the greedy algorithm are within 0(1) of an optimal triangulation.  We investigate issues of local optimality pertaining to known triangulation algorithms.  We define the notion of k-optimality which suggests an interesting new approach to studying triangulation algorithms.  We restate the minimum weight triangulation problem as a graph problem and show that NP-hardness of a closely related graph problem.  Finally, we show that the constrained problem of computing the minimum weight of triangulation, given a set of points in a plane and enough edges to form a triangulation, is NP-hard. These results are an advance towards a proof that the minimum weight triangulation problem is NP-hard.",1990
Toward Empirically Derived Methodologies and Tools for Human-Computer Interface Development,"This term, unknown only a few years ago, now conjures up images of icons and objects, windows and words that comprise the human-computer interface. A UIMS is an interactive system composed of high-level tools that support production and execution of human-computer interfaces. UIMS have become a major topic of both academic and trade journal articles, conference technical presentations, demonstrations, and special interest sessions. Many commercial software packages and research products even tangentially related to the area of human-computer interaction now claim to be UIMS. As young and exciting as the field is, there are already signs of promises unfulfilled, due to a lack of both functionality and usability factors that can make the difference between whether UIMS are a passing fad or a viable tool.  But what does the future hold for UIMS? All indications are that they are here to stay. We perceive a trend in UIMS evolution that we have divided into generations based primarily on common characteristics and only loosely on chronology.",1988
A Fast and Efficient Method Dispatching Statically Typed MultipleInheritance Object-Oriented Languages,"Inheritance is an invaluable mechanism for object-oriented programming.  The benefits of inheritance have been well recognized over the last few years.  However, these benefits typically come at the expense of run time overhead in time and space.  While an efficient late binding mechanism based on indexing has been popularly used in supporting single inheritance, a mechanism for multiple inheritance which can provide a comparable efficiency has been sought.  In this paper, we describe a late binding mechanism for statically typed object-oriented programming with multiple inheritance.  Our technique, based on the partitioning of a multiple inheritance hierarchy, is a significant improvement in both space and time over existing techniques. The fast and efficient late binding mechanism called hierarchy partitioning is presented.  An analysis of the predicted performance of the technique and a detailed comparison with other related work are also provided.",1989
Snap2Diverse: Coordinating Information Visualizations and Virtual Environments,"The field of Information Visualization is concerned with improving with how users perceive, understand, and interact with visual representations of data sets. Immersive Virtual Environments (VEs) excel at providing researchers and designers a greater comprehension of the spatial features and relations of their data, models, and scenes. This project addresses the intersection of these two fields where information is visualized in a virtual environment. Specifically we are interested in visualizing abstract information in relation to spatial information in the context of a virtual environment. We describe a set of design issues for this type of integrated visualization and demonstrate a coordinated, multiple-views system supporting 2D and 3D visualization tasks such as overview, navigation, details-on-demand, and brushing-and-linking selection. Software architecture issues are discussed with details of our implementation applied to the domain of chemical information and visualization. Lastly, we subject our system to an informal usability evaluation and identify usability issues with interaction and navigation that may guide future work in these situations.",2003
An Empirical Study of the Object-Oriented Paradigm and Software Reuse,"Little or no empirical validation exists for many of software engineering's basic assumptions.  While some of these assumptions are intuitive, the need for scientific experimentation remains clear.  Several assumptions are made about the factors affecting software reuse, and in particular, the role of the object-oriented paradigm.  This paper describes the preliminary results of a controlled experiment designed to evaluate the impact of the object-oriented paradigm on software reuse.  The experiment concludes that (1) the object-oriented paradigm substantially improves productivity, although a significant part of this improvement is due to the effect of reuse, (2) reuse without regard to language paradigm improves productivity, (3) language differences are far more important when programmers reuse than when they do not, and (4) the object-oriented paradigm has a particular affinity to the reuse process.",1991
Development of New Heuristics for the Euclidean Traveling SalesmanProblem,"Many heuristics have been developed to approximate optimal tours for the Euclidean Traveling Salesman Problem (ETSP).  While much progress has been made, there are few quick heuristics which consistently produce tours within 4 percent of the optimal solution.  This project examines a few of the well known heuristics and introduces two improvements, Maxdiff and Checks.  Most algorithms, during tour constrution, add a city to the subtour because the city best satisfies some criterion.  Maxdiff, applied to an algorithm, ranks a city according to its effect (based on the algorithm's criterion) if it is not added to the subtour.",1989
An Empirical Study of the Object-Oriented Paradigm and Software Reuse,"This paper describes the preliminary results of a controlled experiment designed to evaluate the impact of the object-oriented paradigm on software reuse. The experiment concludes that (1) the object-oriented paradigm substantially improves productivity, although a significant part of this improvement is due to the effect of reuse, (2) reuse without regard to language paradigm improves productivity, (3) language differences are far more important when programmers reuse than when they do not, and (4) the object-oriented paradigm has a particular affinity to the reuse process.",1992
Problem Identification and Decomposition within the Requirements Generation Process,"Only recently has the real importance of the requirements generation process and its requisite activities been recognized. That importance is underscored by the evolving partitions and refinements of the once all-encompassing (and somewhat miss-named) Requirements Analysis phase of the software development lifecycle. Continuing along that evolutionary line, we propose an additional refinement to the requirements generation model that focuses on problem identification and its decomposition into an associated set of user needs that drive the requirements generation process. Problem identification stresses the importance of recognizing and identifying the difference between a perceived state of the system and the desired one. We mention pre- and post-conditions that help identify and bound the problem and then present some methods and techniques that assist in refining that boundary and also in recognizing essential characteristics of the problem. We continue by presenting a process by which the identified problem and its characteristics are decomposed and translated into a set of user needs that provide the basis for the solution description, i.e, the set of requirements. Finally, to place problem identification and decomposition in perspective, we present them within the framework of the Requirements Generation Model.",2002
Modeling Transcient Trace Data,This paper introduces a novel technique to construct an empirical workload model fitting time-varying (transient) trace data.  The trace can be a categorical or numerical time-series.  We model the trace as a Piecewise Independent stochastic process.  To estimate the parameters for our model we first build a Rate Evolution Graph from the trace data.  Piecewise linear regression is then used to construct a joint time-dependent probablity mass function for the trace data.  Two methods are proposed to build a parsi- monious model.  The modeling approach is demonstrated by the application of our model to twelve traces from the performance analysis domain.,1996-10-01
A Practical Method to Estimate Information Content in the Context of 4D-Var Data Assimilation. II: Application to Global Ozone Assimilation,"Data assimilation obtains improved estimates of the state of a physical system by combining imperfect model results with sparse and noisy observations of reality. Not all observations used in data assimilation are equally valuable. The ability to characterize the usefulness of different data points is important for analyzing the effectiveness of the assimilation system, for data pruning, and for the design of future sensor systems.  In the companion paper (Sandu et al., 2012) we derive an ensemble-based computational procedure to estimate the information content of various observations in the context of 4D-Var. Here we apply this methodology to quantify the signal and degrees of freedom for signal information metrics of satellite observations used in a global chemical data assimilation problem with the GEOS-Chem chemical transport model. The assimilation of a subset of data points characterized by the highest information content yields an analysis comparable in quality with the one obtained using the entire data set.",2012-03-01
Management Indicators: Assessing Product Reliability and Maintainability,"This report discusses the role of Management Indicators in validating the predictive capability of the bottom-up evaluation process, which is defined by the Procedural Approach to the Evaluation of Software Development Methodologies. The bottom-up evaluation process provides a framework for determining the extent to which software engineering objectives, e.g., reliability and maintainability, are present in a software product from a design perspective of the code and supporting documentation. The bottom-up evaluation process is observed to be a predictor of the extent to which the objectives are realized in the post-developed product. Employment of the bottom-up evaluation process to determine the extent to which the objectives are present in the product is accomplished by the utilization of Design Indicators.  Management Indicators are proposed as a counterpart to Design Indicators and enable one to measure the extent to which the objectives are realized in a developed product. While Design Indicators focus on design structure characteristics of the product, Management Indicators focus on the acquisitional, behavioral, and maintenance characteristics. To accomplish the validation of the predictive capability, the correlation between the values obtained by utilizing Design Indicators and those obtained by utilizing Management Indicators must be investigated. The author has chosen to study and present the software engineering objectives of reliability and maintainability as they related to a future validation effort.",1988
Predicting Maintainability with Software Quality Metrics,"Maintenance of software makes up a large fraction of the time and money spent in the software life cycle. By reducing the need for maintenance these costs can also be reduced.  Predicting where maintenance is likely to occur can help to reduce maintenance by prevention.  This paper details a study of the use of software quality metrics to determine high complexity components in a software system. By the use of a history of maintenance done on a particular system, it is shown that a predictor equation can be developed to identify components which needed maintenance activities. This same equation can also be used to determine which components are likely to need maintenance in the future. Through the use of these predictions and software metric complexities it should be possible to reduce the likelihood of a component needing maintenance. This might be accomplished by reducing the complexity of that component through further decomposition. Even though this is only one study, this methodology of developing maintenance predictors could be applied in any environment.",1988
Magnetohydroynamic Free Convection from a Disk Rotating in a Vertical Plane,"The non-axisymmetric motion (produced by a buoyancy induced cross flow) of a fluid in contact with a rotating disk and in the presence of a magnetic field normal to the disk is studied. Using modern quasi- Newton techniques, B-splines, and a Galerkin approximation to the fluid motion equations, numerical solutions are obtained for a wide range of magnetic field strengths and Prandtl numbers (ratio of kinematic viscosity to thermal conductivity). Results are presented both in tabular and graphical form in terms of two non-dimensional parameters. There is excellent agreement with previous work.",1988
A Robust Hybrid Algorithm for Computing Multiple Equilibrium Solutions,This paper describes a hybrid method that seeks to combine the efficiency of a quasi-Newton method capable of locating stable and unstable equilibrium configurations with a robust homotopy method that is capable of tracking equilibrium paths with turning points while exploiting symmetry and sparsity of the Jacobian matrices. Numerical results are presented for a shallow arch problem.,1984
A Review of Publishing and Access Issues for Optical Discs and CD-ROM's,"Optical discs in general and CD-ROM in particular are helping fuel a revolution in information access that media, hardware, and information providers and producers hope will not only reach a variety of targeted groups, but will also lead to interactive involvement of the general public. This chapter reviews the events of recent years in this dynamic area; discusses the variety of read-only, write once, and erasable media that have been developed; considers standards that provide consumers with confidence to acquire.  new systems and products; describes approaches and activities to publishing and accessing CD-ROM and related materials; surveys the broad application areas; considers the research underway and required; anticipates future trends; and draws conclusions regarding the importance of this exciting technology. While videodiscs are frequently referred to, the focus is on CD-ROM, along with related products like CD-I. The importance of having second generation products that are highly interactive and which draw on the best work on hypertext, hypermedia, and retrieval methods is emphasized.",1988
Reuse Level Metrics,"Reuse level is an abstract metric that can be applied to any reusable asset.  This paper presents extensions to the reuse level metric, defines the metric formally, and discusses an implementation of the abstract metric for C in an enhanced version of the rl program.",1994
Search Tool Implementation for Historical Archive,"Dr. Linda Arnold's archival project ""Mexican-American War and the Media"" is an underutilized resource. Providing contrasting primary sources on the War, it is the only archive of its kind. In order to make the archive's massive amount of information more accessible to researchers and students, I added search functionality to the site. Several tools were implemented and tested. Perlfect, a Perl-based open-source approach, was determined to be the best option. This report includes an outline of the steps taken to implement the search tool, a user's manual, a developer's manual, and options for future work. The archive may be accessed at http://www.majbill.vt.edu/history/mxamwar/index.htm.",2004
An End-User Development Perspective on State-of-the-Art Web Development Tools,"We reviewed and analyzed nine commercially available web development tools from the perspective of suitability for end-user development to compare and contrast alternative and best-of-breed approaches for particular problem areas within web application development (Getting Started, Workflow, Level of Abstraction, Layout, Database, Application Logic, Testing and Debugging, Learning and Scaling, Security, Collaboration, and Deployment). End-user development involves the creation of dynamic websites with support for features like authentication, conditional display, and searching/sorting by casual web developers who have some experience creating static websites but little or no programming knowledge. We found that current tools do not lack functionality, but rather have a variety of problems in ease of use for end users who are nonprogrammers. In particular, while many tools offer wizards and other features designed to facilitate specific aspects of end-user development, none of the tools that we reviewed supports a holistic approach to web application development. We discuss the implications of these problems and conclude with recommendations for the design of improved web development tools that would lower the entry barrier into web programming.",2005
Implementation of Motion Without Movement on Real 3D Objects,"Researchers have developed a technique such that when the colors of a static image are toggled on a screen, the illusion of continuous movement in a certain direction is produced. This phenomenon is known as motion without movement. In our research we aim to extend the applications of this technique and apply it to real three-dimensional objects. In order to achieve the projection of images onto three-dimensional shapes, we use projectors called shader lamps, which apply an algorithm to a two-dimensional image so that it appears undistorted when projected onto a three-dimensional object. The resulting effect is that a static object appears to be moving continuously in a desired direction. In addition to applying the motion without movement technique to entire objects, we also examine its use on parts of an image as small as a pixel. Using a technique called optical flow, we determine the exact movement oparts of an image by taking a second image similar to the original, where image shapes have moved and determine in which direction. Finally, we extend the motion without movement technique beyond its previous applications to use on color rather than solely grayscale images, thus producing even more realistic results.",2004
"Measuring Software Quality in ADA Packages: An Objectives,Principles, Attributes Framework",This paper describes preliminary results stemming from a research effort focusing on assessing the quality of an Ada-based product.  The presentation emphasizes,1990
Sparse Approximate Inverses in Preconditioning Distributed Linear Systems,"Using a direct approximation of sparse matrix inverse in preconditioning is viewed as a good alternative to the preconditioning techniques that require a matrix factorization.  A sparse approximate inverse is easy to compute and apply, and it is suitable for parallel implementations.  For distributed linear systems of varying difficulty, approximate block LU preconditioning using sparse approximate inverse techniques and an incomplete LU factorization used in Block-Jacobi preconditioning are compared.",1997-07-01
RGML: A Markup Language for Characterizing Requirements Generation Processes,"In this paper we present the Requirements Generation Markup Language (RGML). The RGML supports the formal characterization of (a) the physical structure of a requirements generation process, (b) individual activities inherent to that process, and (c) artifacts produced and consumed during the generation process. The inclusion of templates, application instantiation, and the expression of temporally-based pre- and post-conditions increase the flexibility of RGML and its ability to capture variations in requirements generation processes.  We envision the RGML as providing the specification basis for (automatically) producing interactive environments that lead (or guide) the requirements engineer through a structured set of integrated activities that foster the evolution of quality requirements.",2003
Path Planning Through Time and Space in Dynamic Domains,"Realistic robot problems involve navigating the robot through time as well as space. The obstacles that a robot must avoid and the pathways on which it travels are subject to changes throughout time.  These changes can occur in a predictable or unpredictable fashion.  This paper presents an integrated route planning and spatial representation system that allows paths to be calculated in dynamic domains. The path planner finds the ""best route"" through a given n-dimensional space. The ""best route"" is defined as the path through space-time with the best score as determined by a set of user-defined evaluation",1987-05-01
The UAN: A User-Oriented Representation for Direct ManipulationInterface Designs,"Almost all existing interface representation techniques, especially those associated with UIMS, are constructional, focused on interface implementation, and therefore do not adequately support a user-centered focus.  But it is the behavioral domain of the user that interface designers and evaluators do their work.  We are seeking to complement constructional methods by providing a tool supported technique capable of specifying the behavioral aspects of an interactive system -- the tasks and the actions a user performs to accomplish those tasks.  In particular, this paper is a practical introduction to use of the User Action Notation (UAN), a task- and User-oriented notation for behavioral representation of asynchronous, direct manipulation interface designs. Interfaces are specified in the UAN as a quasi-hierarchy of asynchronous tasks.  At the lower levels, user actions are associated with feedback and system state changes.  The notation makes use of visually onomatopoeic symbols, and is simple enough to read without much instruction.  The UAN has been used by growing numbers of interface developers and researchers over the past few years.  In addition to its design role, current research is investigating how the UAN can support the production and maintenance of code and documentation.",1990
Design Strategies for Algebraic Specifications,"The algebraic specification of abstract data types can be affected by serious flaws.  Rather than attempting, as it is usually done, to detect defects in a specification a posteriori, i.e., after the specification has been designed, we propose two strategies for addressing this problem a priori, i.e., during design phase of a specification.  We investigate two common flaws of algebraic operations, under- and over-specification, determine sufficient and/or necessary conditions to avoid them, and show how to obtain these conditions in a constructive way.  Our approach is based on the completeness and parsimony properties of sets of tuples and on a recursive mechanism which extends primitive recursion from natural numbers to abstract data types.  Finally, we attempt to assess the power and the limitations of our approach and relate the properties on which it is based to other similar properties which have appeared in the literature.  We formally prove a number of results and illustrate their application to the design of specifications by means of examples.",1989
Design of Laminated Plates for Maximum Buckling Load,"The buckling load of laminated plates having midplane symmetry is maximized for a given total thickness. The thicknesses of the layers are taken as the design variables. Buckling analysis is carried out using the finite element method. The optimality equations are solved by a homotopy method which permits tracing optima as a function of total thickness. It is shown that for any design with a given stacking sequence of ply orientations, there exists a design associated with any other stacking sequence which possesses the same bending stiffness matrix and same total thickness. Hence, from the optimum design for a given stacking sequence, one can directly determine the optimum design for any rearrangement of the ply orientations, and the optimum buckling load is independent of the stacking sequence.",1988
SchemaMapper: A tool for visualization of schema mapping,"The world has changed significantly in the past few years with an increasing thrust towards the use of digital information. Every kind of application domain has found reasons to use digital information sources extensively. As a result, different types of data representation models or schemas have been developed. This poses a problem when there is a need for data integration from several sources. Diverse representations must be merged in order to create a single global representation. Hence there is a need for schema mapping tools that will enable amalgamation of heterogeneous data representations. That goal is difficult to achieve today since existing schema mapping tools are domain unaware. SchemaMapper, a new tool we have developed, tries to be domain aware and hence help speed up the schema mapping process. Further, it supports visualization of the mapping process by using a hyperbolic tree representation. This has not been used before in the context of schema mapping. Although the primary motivation for SchemaMapper comes from ETANA-DL (a digital library to promote integration of information and services from diverse archaeological sites), it can potentially be used in any other similar domains in the future, or further extended for different types of schema mappings. This report describes in detail the prototype developed for exploring the feasibility of such a tool, providing architecture and implementation details. Experiments were conducted to evaluate SchemaMapper and the initial results have been very encouraging. All the schemas used during the evaluation process were real life examples taken from ETANA-DL. Analysis of the evaluation results suggests that domain awareness is extremely useful for the schema mapping process. Also, the linear tree representation of schemas which existing tools use appears to have inherent disadvantages which need to be overcome in order to make the process more effective.",2005
Subsequence and Run Heuristics for Sorting by Transpositions,"Sorting by tranpositions is the problem of finding the minimum number of transpositions required to sort a permutation pi.  A transposition involves repositioning a contiguous sequence (block) of elements by inserting it elsewhere in the permutation.  The problem has applications in the study of genome rearrangements and phylogeny reconstruction. In this paper, several heuristics based on analyses of subsequences and runs in a permutation are employed.  Experimental results are provided. The algorithm based on the longest increasing subsequence in a permutation appears most promising.",1997-11-01
A General Probabilistic Model of the PCR Process,"This paper rigorously derives a general probabilistic model for the PCR process; this model includes as a special case the Velikanov-Kapral model where all nucleotide reaction rates are the same. In this model the probability of binding of deoxy-nucleoside triphosphate (dNTP) molecules with template strands is derived from the microscopic chemical kinetics. A recursive solution for the probability distribution of binding of dNTPs is developed for a single cycle and is used to calculate expected yield for a multicycle PCR. The model is able to reproduce important features of the PCR amplification process quantitatively. With a set of favorable reaction conditions, the amplification of the target sequence is fast enough to rapidly outnumber all side products. Furthemore, the final yield of the target sequence in a multicycle PCR run always approaches an asymptotic limit that is less than one. The amplification process itself is highly sensitive to initial concentrations and the reaction rates of addition to the template strand of each type of dNTP in the solution.",2004
Using Belbin's Role to Improve Team Effectiveness,"This paper presents a controlled experiment conducted with software engineering students that demonstrates the utility of forming teams based on R. Meredith Belbin's set of team roles.  The overall research effort is a demonstration of the general utility of Belbin's roles in improving the effectiveness of teams, even industry teams.  The significance of this work is twofold: performance and team viability. Performance improvements clearly improve a team's productivity; viability issues are important because if employees remain with a team or employer, then employee replacement costs are reduced.  To address this problem, as an initial step, controlled experiments have been conducted to demonstrate that teams that contain certain roles perform better than teams that do not.  In a laboratory setting, a number of teams were formed that contained a single leader; other teams were formed that had no leader or multiple leaders.  The results of this single experiment are positive.  They demonstrate that indeed Belbin's roles are useful knowledge in forming teams.  The specific conclusion of this first controlled experiment is that a single leader on a team perform better than having multiple leaders or no leader.  In other words, as one would expect, the mean time to completion for the leaderless group of teams was significantly larger than the group of teams with leaders.  This means that Belbin's roles can be utilized in team formation, making sure that a team has a single leader, and also for evaluation on extant teams.  Both of these aspects, formation and evaluation, are extremely useful to managers of software programmers.",1997-04-01
The Virginia Tech Computational Grid: A Research Agenda,"An important goal of grid computing is to apply the rapidly expanding power of distributed computing resources to large-scale multidisciplinary scientic problem solving. Developing a usable computational grid for Virginia Tech is desirable from many perspectives. It leverages distinctive strengths of the university, can help meet the research computing needs of users with the highest demands, and will generate many challenging computer science research questions. By deploying a campus-wide grid and demonstrating its effectiveness for real applications, the Grid Computing Research Group hopes to gain valuable experience and contribute to the grid computing community. This report describes the needs and advantages which characterize the Virginia Tech context with respect to grid computing, and summarizes several current research projects which will meet those needs.",2002
Generalized Linear Product Homotopy Algorithms and the Computation of Reachable Surfaces,"In this paper, we apply a homotopy algorithm to the problem of finding points in a moving body that lie on specific algebraic surfaces for a given set of spatial configurations of the body. This problem is a generalization of Burmester's determination of points in a body that lie on a circle for five planar positions. We focus on seven surfaces that we term ""reachable"" because they correspond to serial chains with two degree-of-freedom positioning structures combined with a three degree-of-freedom spherical wrist. A homotopy algorithm based on generalized linear products is used to provide a convenient estimate of the number of solutions of these polynomial systems. A parallelized version of this algorithm was then used to numerically determine all of the solutions.",2003
A genetic algorithm with memory for mixed discrete-continuous design optimization,"This paper describes a new approach for reducing the number of the fitness function evaluations required by a genetic algorithm (GA) for optimization problems with mixed continuous and discrete design variables. The proposed additions to the GA make the search more effective and rapidly improve the fitness value from generation to generation. The additions involve memory as a function of both discrete and continuous design variables, multivariate approximation of the fitness function in terms of several continuous design variables, and localized search based on the multivariate approximation. The approximation is demonstrated for the minimum weight design of a composite cylindrical shell with grid stiffeners.",2003
Uncertainty and Probability,"Advocates of probability theory as a primary tool for reasoning in contexts of uncertainty and incomplete information have increased in number in recent years. At the same time, opponents have put forward a variety of arguments against using probabilities in this field.  This paper examines the relationship between probability theory and reasoning in uncertainty, and argues that (contra opposing views) probability theory does have a place, but that its place is more restricted than many of its advocates claim.  In particular, two major  theses are presented and argued for. (1) Reasoning from probabilities works well in domains which permit a clear analysis in terms of events over outcome spaces and for which either relatively large bodies of evidence or long periods of ""kaining"" are available; but such domains are relatively rare, and even there, care must be taken in interpreting probability results. And (2) some generalizations with which AI applications must concern themselves am not statistical in nature, in the sense that statistical generalizations neither capture their meanings nor even preserve their truth values. For these contexts, different models will be needed.",1986
Steady Viscous Flow in a Triangular Cavity,"Steady recirculating viscous flow inside an equilateral triangular cavity is generated by translating one side. The Navier-Stokes equations are solved numerically using finite difference on a transformed geometry. The results show a primary eddy and a series of secondary eddies at the stagnant corner. For high Reynolds numbers the interior of the primary eddy has constant vorticity, but its value cannot be predicted by the mean-squared law.",1992
Applying Software Engineering Principles to Process Modeling,"Process models are constructed using specific modeling methods or techniques. These techniques impart certain characteristics to the models they produce. Application of the software engineering principles of information hiding, top-down functional decomposition and stepwise refinement to process modeling imparts many desirable characteristics to the process models produced. This paper describes an approach to process modeling which applies these software engineering principles to control flow diagrams in order to produce software process models.",1992
Response Surface Approximations for Aerodynamic Parameters in High Speed Civil Transport Optimization,"A procedure for generating and using polynomial approximations to the range or to the cruise drag components in terms of 29 design variables for a High Speed Civil Transport configuration design is presented.  Response surface methodology is used to fit quadratic polynomials to data gathered from a series of numerical analyses of different aircraft designs.  Several techniques are employed to minimize the number of required analyses and to maintain accuracy.  Approximate analysis techniques are used to find regions of the design space where reasonable aircraft designs could occur and response surface models are built using higher fidelity analysis results of designs in this ""reasonable"" region.  This is a means of using results from advanced CFD methods at the early design stage.  Regression analysis and analysis of variance are used to reduce the number of polynomial terms in the response surface model functions.  Optimization runs of the aircraft configuration are then carried out with the response surface models and compared to the previous optimization runs without the response surface models.  It is shown that considerable reduction of the amount of numerical noise in optimization is achieved with response surface models and the convergence rate is improved.  Careful attention was required to keep the accuracy of the models at an acceptable level.",1997-08-01
Multilayered Heterogeneous Parallelism Applied to Atmospheric Constituent Transport Simulation,"Heterogeneous multicore chipsets with many levels of parallelism are becoming increasingly common in high-performance computing systems.  Effective use of parallelism in these new chipsets constitutes the challenge facing a new generation of large scale scientific computing applications.  This study examines methods for improving the performance of two-dimensional and three-dimensional atmospheric constituent transport simulation on the Cell Broadband Engine Architecture (CBEA).  A function offloading approach is used in a 2D transport module, and a vector stream processing approach is used in a 3D transport module.  Two methods for transferring incontiguous data between main memory and accelerator local storage are compared.  By leveraging the heterogeneous parallelism of the CBEA, the 3D transport module achieves performance comparable to two nodes of an IBM BlueGene/P, or eight Intel Xeon cores, on a single PowerXCell 8i chip.  Module performance on two CBEA systems, an IBM BlueGene/P, and an eight-core shared-memory Intel Xeon workstation are given.",2008-10-01
Clustering constrained by dependencies,"Clustering is the unsupervised method of grouping data samples to form a partition of a given dataset. Such grouping is typically done based on homogeneity assumptions of clusters over an attribute space and hence the precise definition of the similarity metric affects the clusters inferred. In recent years, new formulations of clustering have emerged that posit indirect constraints on clustering, typically in terms of preserving dependencies between data samples and auxiliary variables. These formulations ﬁnd applications in bioinformatics, web mining, social network analysis, and many other domains. The purpose of this survey is to provide a gentle introduction to these formulations, their mathematical assumptions, and the contexts under which they are applicable.",2009
Emerging from the MIST: A Connector Tool for Supporting Programming by Non-programmers,"Software development is an iterative process. As user re-quirements emerge software applications must be extended to support the new requirements. Typically, a programmer will add new code to an existing code base of an application to provide a new functionality. Previous research has shown that such extensions are easier when application logic is clearly separated from the user interface logic. Assuming that a programmer is already familiar with the existing code base, the task of writing the new code can be considered to be split into two sub-tasks: writing code for the application logic; that is, the actual functionality of the application; and writing code for the user interface that will expose the functionality to the end user.  The goal of this research is to reduce the effort required to create a user interface once the application logic has been created, toward supporting scientists with minimal pro-gramming knowledge to be able to create and modify pro-grams. Using a Model View Controller based architecture, various model components which contain the application logic can be built and extended. The process of creating and extending the views (user interfaces) on these model components is simplified through the use of our Malleable Interactive Software Toolkit (MIST), a tool set an infrastructure intended to simplify the design and extension of dynamically reconfigurable interfaces.  This paper focuses on one tool in the MIST suite, a connec-tor tool that enables the programmer to evolve the user interface as the application logic evolves by connecting related pieces of code together; either through simple drag-and-drop interactions or through the authoring of Python code. The connector tool exemplifies the types of tools in the MIST suite, which we expect will encourage collabora-tive development of applications by allowing users to inte-grate various components and minimizing the cost of de-veloping new user interfaces for the combined compo-nents.",2010-04-01
An Application-Oriented Approach for Accelerating Data-Parallel Computation with Graphics Processing Unit,"This paper presents a novel parallelization and quantitative characterization of various optimization strategies for data-parallel computation on a graphics processing unit (GPU) using NVIDIA's new GPU programming framework, Compute Unified Device Architecture (CUDA). CUDA is an easy-to-use development framework that has drawn the attention of many different application areas looking for dramatic speed-ups in their code. However, the    performance tradeoffs in CUDA are not yet fully understood, especially for data-parallel applications. Consequently, we study two fundamental mathematical operations that are common in many data-parallel applications: convolution and accumulation. Specifically, we profile and optimize the performance of these operations on a 128-core NVIDIA GPU. We then characterize the impact of these operations on a video-based motion-tracking algorithm called vector coherence mapping, which consists of a series of convolutions and dynamically weighted accumulations, and present a comparison of different implementations and their respective performance profiles.",2009-03-01
The State of Computer Science in National Colleges And Universities,"The author served on the State Council for Higher Education in Virginia (SCHEV) task force on computer science education during the academic year 1983; this report was originally offered as the first chapter in the report of that task force. The task force felt that the chapter would be too long and would detract from the remainder of the report and findings on the state of computer science in Virginia.  In this author's opinion the state of computer science (a generic term which is intended to cover computer science per se, information science and data processing) in Virginia is probably little worse than in the rest of the nation, but there are no pinnacles of excellence. The pragmatic nature of the senior institutions may be an advantage in light of the need for technology transfer in the industrial environment of the state. The four year colleges are by no means meeting the needs of the industry and are graduating only a small proportion of the number of graduates needed by industry. On the other hand, this report suggests that the need expressed by the industry is exagerated; the demand should be for highly qualified applications programmers rather than computer scientists.  The community college program is clearly meeting the needs of their communities though there may be some question as to whether the level of expertise in those localities and the consequent level of education of students will permit them to have any sensible mobility in today's society. One major result of that parochialism is the inability of students from community colleges to transfer effectively to four year institutions. While this is not a major mission of the community colleges the number of students who believe this to be a means of gaining a bachelor's degree is too large to ignore.  The reader should refer to the Task Force report for the opinion of the task force on the condition of computing in the Commonwealth.",1983
HANA: A Model of Type and Inheritance for Object-Oriented Programming Languages,"Most current object-oriented languages consider inheritance as subtyping.  However, a type system which views inheritance as subtyping allows neither multiple representations of a type nor method exclusion without violating typing constraints.  These problems and the difference between inheritance and subtyping have been well recognized recently. Another approach to subtyping found in the literature is to separate inheritance from subtyping.  In this approach, subtyping is solely determined by inheritance conformance.  While subtyping based on interface conformance can support multiple representations, it cannot distinguish intended conformance from accidental conformance.  This paper describes a new model of type and inheritance, called HANA, in which subtyping and inheritance are not separated, although differentiated.  The notion of subtyping in HANA is based on both inheritance and interface conformance.  HANA integrates multiple inheritance multiple representations, method exclusion, and method name overloading with static typing.  HANA extends other existing methods of inheritance and subtyping with enhanced expressive power, increased reusability, and program efficiency.  We show that the differentiation made by HANA between inheritance and subtyping allows name collision to be resolved without compromising the integrity of the type system.  The capability of excluding an inherited method without violating static typing offers a sound solution for resolving name collision in multiple inheritance when used together with other mechanisms.  We show that the mechanisms of method exclusion, addition, and renaming are othogonal with respect to the power of resolving name collision in multiple inheritance.",1990
Approximate Time-Parallel Simulation of Queueing Systems with Losses,This paper presents a guideline of a partial state matching approach for time-parallel simulation. Two algorithms using this approach to simulate FCFS G/G/1/K and G/D/1/K queues in which arriving customers that find the queue full are lost are proposed. Experiments with M/M/1/K and M/D/1/K models show that the performance of the algorithms in terms of convergence speed and accuracy is good in general cases. The worst performance of the algorithms occurs when traffic intensity approaches one. An argument is made to explain this phenomenon.,1992
A Practical Blended Analysis for Dynamic Features in JavaScript,"The JavaScript Blended Analysis Framework is designed to perform a general-purpose, practical combined static/dynamic analysis of JavaScript programs, while handling dynamic features such as run-time generated code and variadic func- tions. The idea of blended analysis is to focus static anal- ysis on a dynamic calling structure collected at runtime in a lightweight manner, and to rene the static analysis us- ing additional dynamic information. We perform blended points-to analysis of JavaScript with our framework and compare results with those computed by a pure static points- to analysis. Using JavaScript codes from actual webpages as benchmarks, we show that optimized blended analysis for JavaScript obtains good coverage (86.6% on average per website) of the pure static analysis solution and nds ad- ditional points-to pairs (7.0% on average per website) con- tributed by dynamically generated/loaded code.",2012-08-01
Supporting Memorization and Problem Solving with Spatial Information Presentations in Virtual Environments,"While it has been suggested that immersive virtual environments could provide benefits for educational applications, few studies have formally evaluated how the enhanced perceptual displays of such systems might improve learning.  Using simplified memorization and problem-solving tasks as representative approximations of more advanced types of learning, we are investigating the effects of providing supplemental spatial information on the performance of learning-based activities within virtual environments. We performed two experiments to investigate whether users can take advantage of a spatial information presentation to improve performance on cognitive processing activities. In both experiments, information was presented either directly in front of the participant or wrapped around the participant along the walls of a surround display. In our first experiment, we found that the spatial presentation caused better performance on a memorization and recall task. To investigate whether the advantages of spatial information presentation extend beyond memorization to higher level cognitive activities, our second experiment employed a puzzle-like task that required critical thinking using the presented information. The results indicate that no performance improvements or mental workload reductions were gained from the spatial presentation method compared to a non-spatial layout for our problem-solving task. The results of these two experiments suggest that supplemental spatial information can support performance improvements for cognitive processing and learning-based activities, but its effectiveness is dependent on the nature of the task and a meaningful use of space.",2011-09-01
A Library of Reusable Model Components for Visual Simulation of the NCSTRL System,This paper presents a library of reusable model components for visual simulation of the Networked Computer Science Technical Report Library (NCSTRL) and illustrates how a visual simulation model can be developed for a new NCSTRL configuration by way of component reuse with no programming.  Such a repository of reusable model components can be created for visual simulation of any digital library for the purpose of performance evaluation and tuning and conducting what-if analyses for different system design configurations.,1998
A Library for Pattern-based Sparse Matrix Vector Multiply,"Pattern-based Representation (PBR) is a novel approach to improving the performance of Sparse Matrix-Vector Multiply (SMVM) numerical kernels. Motivated by our observation that many matrices can be divided into blocks that share a small number of distinct patterns, we generate custom multiplication kernels for frequently recurring block patterns. The resulting reduction in index overhead significantly reduces memory bandwidth requirements and improves performance.  Unlike existing methods, PBR requires neither detection of dense blocks nor zero filling, making it particularly advantageous for matrices that lack dense nonzero concentrations.  SMVM kernels for PBR can benefit from explicit prefetching and vectorization, and are amenable to parallelization. The analysis and format conversion to PBR is implemented as a library, making it suitable for applications that generate matrices dynamically at runtime. We present sequential and parallel performance results for PBR on two current multicore architectures, which show that PBR outperforms available alternatives for the matrices to which it is applicable, and that the analysis and conversion overhead is amortized in realistic application scenarios.",2009
Multi-Dimensional Characterization of Temporal Data Mining on Graphics Processors,"Through the algorthmic design patterns of data parallelism and task parallelism, the graphics processing unit (GPU) offers the potential to vastly accelerate discovery and innovation across a multitude of disciplines. For example, the exponential growth in data volume now presents an obstacle for high-throughput data mining in ﬁelds such as neuroinformatics and bioinformatics. As such, we present a characterization of a MapReduce-based data-mining application on a general-purpose GPU (GPGPU). Using neuroscience as the application vehicle, the results of our multi-dimensional performance evaluation show that a “one-size-ﬁts-all” approach maps poorly across different GPGPU cards. Rather, a high-performance implementation on the GPGPU should factor in the 1) problem size, 2) type of GPU, 3) type of algorithm, and 4) data-access method when determining the type and level of parallelism. To guide the GPGPU programmer towards optimal performance within such a broad design space, we provide eight general performance characterizations of our data-mining application.",2009
"Dynamic size and speed cursor for large, high-resolution displays","As larger displays become more available their lack of adequate input techniques becomes apparent. In this paper we show the scalability of the dynamic size and speed cursor for large, high-resolution displays. We introduce the idea of a dynamic paradigm for input devices, explain three implementations of the dynamic size and speed (DSS) cursor and explain results of an experiment. In our experiment we compared the three different implementations of the dynamic size and speed cursor to cursor warping and standard cursor settings. In the experiment we found gender bias for two different tasks (clicking and simple drag and drop), found that one of the dynamic size and speed cursor implementations generally outperformed cursor warping and the standard cursor setting, and explain how distance to and size of targets effected results. We conclude by suggesting the use of a dynamic size and speed cursor with large, high-resolution displays.",2006-07-01
On the Robust Mapping of Dynamic Programming onto a Graphics Processing Unit,"Graphics processing units (GPUs) have been widely used to accelerate algorithms that exhibit massive data parallelism or task parallelism. When such parallelism is not inherent in an algorithm, computational scientists resort to simply replicating the algorithm on every multiprocessor of a NVIDIA GPU, for example, to create such parallelism, resulting in embarrassingly parallel ensemble runs that deliver significant aggregate speed-up. However, the fundamental issue with such ensemble runs is that the problem size to achieve this speed-up is limited to the available shared memory and cache of a GPU multiprocessor. An example of the above is dynamic programming (DP), one of the Berkeley 13 dwarfs. All known DP implementations to date use the coarse-grained approach of embarrassingly parallel ensemble runs because a ﬁner-grained parallelization on the GPU would require extensive communication between the multiprocessors of a GPU, which could easily cripple performance as communication between multiprocessors is not natively supported in a GPU. Consequently, we address the above by proposing a ﬁne-grained parallelization of a single instance of the DP algorithm that is mapped to the GPU. Our parallelization incorporates a set of techniques aimed to substantially improve GPU performance: matrix re-alignment, coalesced memory access, tiling, and GPU (rather than CPU) synchronization. The specific DP algorithm that we parallelize is cal led Smith-Waterman (SWat), which is an optimal local-sequence alignment algorithm. We use this SWat algorithm as a baseline to compare our GPU implementation, i.e., CUDA-SWat, to our Cell implementation, i.e., Cell-SWat.",2009
The Expected Fitness Cost of a Mutation Fixation under the One-dimensional Fisher Model,"This paper employs Fisher’s model of adaptation to understand the expected fitness eﬀect of ﬁxing a mutation in a natural population. Fisher’s model in one dimension admits a closed form solution for this expected ﬁtness eﬀect. A combination of different parameters, including the distribution of mutation lengths, population sizes, and the initial state that the population is in, are examined to see how they affect the expected ﬁtness effect of state transitions. The results show that the expected fitness change due to the ﬁxation of a mutation is always positive, regardless of the distributional shapes of mutation lengths, effective population sizes, and the initial state that the population is in. The further away the initial state of a population is from the optimal state, the slower the population returns to the optimal state. Effective population size (except when very small) has little effect on the expected ﬁtness change due to mutation fixation. The always positive expected ﬁtness change suggests that small populations may not necessarily be doomed due to the runaway process of fixation of deleterious mutations.",2009
GreenVis: Energy-Saving Color Schemes for Sequential Data Visualization on OLED Displays,"The organic light emitting diode (OLED) display has recently become popular in the consumer electronics market. Compared with current LCD display technology, OLED is an emerging display technology that emits light by the pixels themselves and doesn’t need an external back light as the illumination source. In this paper, we offer an approach to reduce power consumption on OLED displays for sequential data visualization. First, we create a multi-objective optimization approach to find the most energy-saving color scheme for given visual perception difference levels. Second, we apply the model in two situations: pre-designed color schemes and auto generated color schemes. Third, our experiment results show that the energy-saving sequential color scheme can reduce power consumption by 17.2% for pre-designed color schemes. For auto-generated color schemes, it can save 21.9% of energy in comparison to the reference color scheme for sequential data.",2012-03-01
Graph Layout Using Queues,"We study the problem of laying out the edges of a graph using queues. In a k queue layout, vertices of the graph are placed in some linear order and each edge is assigned to exactly one of the k queues so that the edges assigned to each queue obey a first-in/first-out discipline. This layout problem abstracts a design problem of fault-tolerant processor arrays and a problem of sorting with parallel queues.  We relate the queue layout problem to the corresponding stack layout problem using stacks (the book embedding problem) and immediately derive some asymptomic bounds for d-valent graph.  We show that every 1-queue graph is a 2-stack graph and that every 1-stack graph is a 2-queue graph.  We characterize the 1-queue graphs (they are almost leveled-planar graphs) and prove that the problem of recognizing 1-queue graphs is NP-complete.  We give some queue layouts for specific classes of graphs.  Relationships to cutwidth, bandwidth, and bifurcators are presented.  We show a tradeoff between queuenumber and stacknumber for a fixed linear order of the vertices of G.",1989
The Granularity of Parallel Homotopy Algorithms for Polynomial Systems of Equations,"Polynomial systems consist of n polynomial functions in n variables, with real or complex coefficients. Finding zeros of such systems is challenging because there may be a large number of solutions, and Newton-type methods can rarely be guaranteed to find the complete set of solutions. There are homotopy algorithms for polynomial systems of equations that are globally convergent from an arbitrary starting point with probability one, are guaranteed to find all the solutions, and are robust, accurate, and reasonably efficient. There is inherent parallelism at several levels in these algorithms. Several parallel homotopy algorithms with different granularities are studied on several different parallel machines, using actual industrial problems from chemical engineering and solid modeling.",1988-05-01
Characterizing World Wide Web Queries,"Locating information on the WWW is a major activity for users, and Web Information Retrieval Systems (IRS) are becoming more important to support their endeavors.  In this paper we characterize queries performed by Web users to such systems and give distributions for accesses to different Web IRS.  We characterize clients' accesses, queries and user sessions.  Our purpose is to reduce network and bandwidth by identifying ways to optimize interactions with the Web.  We characterize clients' sessions by a sequence of Browsing, Searching, and Next steps, and demonstrate that more search steps correlate with a reduction in the number of bytes transferred.",1997-02-01
Collaboration Transparency in Java through Event Broadcasting,"Widespread use of the Internet for education and research is yielding many opportunities for network-based synchronous collaboration.  For example, the Java runtime environment provides a platform independent vehicle for new collaborative applications.  While many toolkits are becoming available that support development of collaborative applications, they do not enable collaborative use of existing single-user Java applets, a process called collaboration transparency. This paper discusses two approaches to collaboration transparency: display broadcasting and event broadcasting.  We then consider the suitability of each within the context of the Java runtime environment. Unfortunately, simple modifications to the standard Java class libraries as they are currently implemented are not sufficient to support collaboration transparency.  We describe the problems and suggest solutions.",1997-02-01
Automated Second Echelon Commitments for the C3EVAL Model,"The C3EVAL model is being developed by the Institute for Defense Analyses (IDA) for the Joint Chiefs of Staff (JCS/J6). The model is to provide one element of a workstation being developed for JCS that is to be used to evaluate theater level command, control and communications (C3) in terms of combat consequences. The red commander enhancement to the C3EVAL model provides a red side that is independent of the blue side. The enhancement also automates the commitment of second echelon forces on the red side. This research report describes the foundation for this enhancement.",1988
"Streams, Structures, Spaces, Scenarios, Societies (5S): A Formal Model for Digital Libraries","Digital libraries (DLs) are complex information systems and therefore demand formal foundations lest development efforts diverge and interoperability suffers. In this paper, we propose the fundamental abstractions of Streams, Structures, Spaces, Scenarios, and Societies (5S), which allow us to define digital libraries rigorously and usefully. Streams are sequences of arbitrary items used to describe both static and dynamic (e.g., video) content. Structures can be viewed as labeled directed graphs, which impose organization. Spaces are sets with operations on those sets that obey certain constraints. Scenarios consist of sequences of events or actions that modify states of a computation in order to accomplish a functional requirement. Societies are sets of entities and activities and the relationships between and among them. Together these abstractions provide a formal foundation to define, relate, and unify concepts - among others, of digital objects, metadata, collections, and services - required to formalize and elucidate ""digital libraries"". The applicability, versatility and unifying power of the 5S model are demonstrated through its use in three distinct applications: building and interpretation of a DL taxonomy, informal and formal analysis of case studies of digital libraries (NDLTD and OAI), and utilization as a formal basis for a DL description language.",2003
POLSYS GLP: A Parallel General Linear Product Homotopy Code for Solving Polynomial Systems of Equations,"Globally convergent, probability-one homotopy methods have proven to be very effective for finding all the isolated solutions to polynomial systems of equations. After many years of development, homotopy path trackers based on probability-one homotopy methods are reliable and fast. Now, theoretical advances reducing the number of homotopy paths that must be tracked, and in the handling of singular solutions, have made probability-one homotopy methods even more practical. POLSYS GLP consists of Fortran 95 modules for nding all isolated solutions of a complex coefficient polynomial system of equations. The package is intended to be used on a distributed memory multiprocessor in conjunction with HOMPACK90 (Algorithm 777), and makes extensive use of Fortran 95 derived data types and MPI to support a general linear product (GLP) polynomial system structure. GLP structure is intermediate between the partitioned linear product structure used by POLSYS PLP (Algorithm 801) and the BKK-based structure used by PHCPACK. The code requires a GLP structure as input, and although nding the optimal GLP structure is a dicult combinatorial problem, generally physical or engineering intuition about a problem yields a very good GLP structure. POLSYS GLP employs a sophisticated power series end game for handling singular solutions, and provides support for problem denition both at a high level and via hand-crafted code. Dierent GLP structures and their corresponding Bezout numbers can be systematically explored before committing to root finding.",2004
Performance of a Parallel Transport Code for Molecular Electronics Simulations,"We describe the sequential and parallel performance of a nonlinear transport simulation code. This code is used by researchers at Virginia Tech to investigate phenomena underlying the emerging eld of molecular electronics. The computational requirements of the code are summarized, and an initial distributed-memory parallel implementation of the code is evaluated. We conclude with several suggestions for improving the parallel performance and scalability of the code.",2002-02-01
Usability Inspection Report of NCSTRL,An evaluation report of the www.ncstrl.org site outlining usability problems and solutions to these problems.,2002-04-01
Generation of a User Interface Prototype from an Integrated Scenario Specification,"This report discusses the design for the generation of a device independent user interface prototype for services offered by a digital library from an Integrated Scenario Specification using class diagrams and collaboration diagrams as input. The project was conceived as an extension to the SUIP tool, which generates a User interface in java. But this approach has an inherent problem. The interfaces thus generated have the java look and feel and this can't be changed if the user so desires. Our design overcomes this drawback by generating the interface in UIML which is device independent and thus it is possible to render the code in java, HTML, WML and other languages. The report aims at providing details about the intricacies of the design and deployment. In addition, it also lists possible enhancements to the code that could be taken up as future work.",2002
Independent Verification and Validation: A Missing Link in Simulation Methodology?,"Independent verification and validation (IV&V) is a powerful tool that can be used to mitigate the increasing complexities associated with an ever-expanding set of modeling and simulation problems.  In this paper we discuss the use of independent V&V within the modeling and simulation community.  Literature reviews and conversations with experienced technical managers serve as a basis for our conjecture that (a) validation is the major focus of most modeling and simulation efforts, (b) verification plays only a secondary role, and (c) independent V&V is, for all practical purposes, being ignored.  In an effort to raise the awareness of the benefits and applicability of independent V&V within the modeling and simulation community, we describe in a step-by-step fashion the application of independent V&V to one particular life cycle model of a simulation study.",1996-12-01
Literature Survey on Interaction Techniques for Large Displays,"When designing for large screen displays, designers are forced to deal with cursor tracking issues, interacting over distances, and space management issues.  Because of the large visual angle of the user that the screen can cover, it may be hard for users to begin and complete search tasks for basic items such as cursors or icons.  In addition, maneuvering over long distances and acquiring small targets understandably takes more time than the same interactions on normally sized screen systems. To deal with these issues, large display researchers have developed more and more unconventional devices, methods and widgets for interaction, and systems for space and task management.         For tracking cursors there are techniques that deal with the size and shape of the cursor, as well as the “density” of the cursor.  There are other techniques that help direct the attention of the user to the cursor.         For target acquisition on large screens, many researchers saw fit to try to augment existing 2D GUI metaphors.  They try to optimize Fitts’ law to accomplish this.  Some techniques sought to enlarge targets while others sought to enlarge the cursor itself.  Even other techniques developed ways of closing the distances on large screen displays.  However, many researchers feel that existing 2D metaphors do not and will not work for large screens.  They feel that the community should move to more unconventional devices and metaphors.  These unconventional means include use of eye-tracking, laser-pointing, hand-tracking, two-handed touchscreen techniques, and other high-DOF devices.         In the end, many of these developed techniques do provide effective means for interaction on large displays.  However, we need to quantify the benefits of these methods and understand them better.  The more we understand the advantages and disadvantages of these techniques, the easier it will be to employ them in working large screen systems.  We also need to put into place a kind of interaction standard for these large screen systems.  This could mean simply supporting desktop events such as pointing and clicking.  It may also mean that we need to identify the needs of each domain that large screens are used for and tailor the interaction techniques for the domain.",2006
Science of Digital Libraries(SciDL),"Our purpose is to ensure that people and institutions better manage information through digital libraries (DLs). Thus we address a fundamental human and social need, which is particularly urgent in the modern Information (and Knowledge) Age. Our goal is to significantly advance both the theory and state-of-theart of DLs (and other advanced information systems) - thoroughly validating our approach using highly visible testbeds. Our research objective is to leverage our formal, theory-based approach to the problems of defining, understanding, modeling, building, personalizing, and evaluating DLs. We will construct models and tools based on that theory so organizations and individuals can easily create and maintain fully functional DLs, whose components can interoperate with corresponding components of related DLs. This research should be highly meritorious intellectually. We bring together a team of senior researchers with expertise in information retrieval, human-computer interaction, scenario-based design, personalization, and componentized system development and expect to make important contributions in each of those areas. Of crucial import, however, is that we will integrate our prior research and experience to achieve breakthrough advances in the field of DLs, regarding theory, methodology, systems, and evaluation. We will extend the 5S theory, which has identified five key dimensions or onstructs underlying effective DLs: Streams, Structures, Spaces, Scenarios, and Societies. We will use that theory to describe and develop metamodels, models, and systems, which can be tailored to disciplines and/or groups, as well as personalized. We will disseminate our findings as well as provide toolkits as open source software, encouraging wide use. We will validate our work using testbeds, ensuring broad impact. We will put powerful tools into the hands of digital librarians so they may easily plan and configure tailored systems, to support an extensible set of services, including publishing, discovery, searching, browsing, recommending, and access control, handling diverse types of collections, and varied genres and classes of digital objects. With these tools, end-users will for be able to design personal DLs. Testbeds are crucial to validate scientific theories and will be thoroughly integrated into SciDL research and evaluation. We will focus on two application domains, which together should allow comprehensive validation and increase the significance of SciDL's impact on scholarly communities. One is education (through CITIDEL); the other is libraries (through DLA and OCKHAM). CITIDEL deals with content from publishers (e.g, ACM Digital Library), corporate research efforts e.g., CiteSeer), volunteer initiatives (e.g., DBLP, based on the database and logic rogramming literature), CS departments (e.g., NCSTRL, mostly technical reports), educational initiatives (e.g., Computer Science Teaching Center), and universities (e.g., theses and dissertations). DLA is a unit of the Virginia Tech library that virtually publishes scholarly communication such as faculty-edited journals and rare and unique resources including image collections and finding aids from Special Collections. The OCKHAM initiative, calling for simplicity in the library world, emphasizes a three-part solution: lightweightprotocols, component-based development, and open reference models. It provides a framework to research the deployment of the SciDL approach in libraries. Thus our choice of testbeds also will nsure that our research will have additional benefit to and impact on the fields of computing and library and information science, supporting transformations in how we learn and deal with information.",2003
Variability of User Interaction with Multi-Platform News Feeds,"The development of the World Wide Web (WWW) and proliferation of web enabled devices have allowed various news agencies to enrich their traditional method of distribution of news through TV, radio and print with simultaneous broadcast through the Web. The varying nature of devices through which the Web is accessed warrants different ways to feed the same content. This precipitates some variation in the way users interact with the news feeds. In this paper, we investigate how mental models and information scent affect this variation and user interaction on the whole. We present results from a preliminary survey conducted to capture the current news gathering behavior of general population and verify our assumptions. We then present observations from the study conducted using BBC news site over laptop, PDA and a cell phone.",2004
A Task and Resource Scheduling System for Automated Planning,"Planning is done at both the strategic and tactical levels. This paper classifies some previous planning techniques into these different levels, and details some of their problems. A planning technique known as heuristic task scheduling is then presented along with a planner architecture that integrates task-scheduling with more traditional techniques to form a system that bridges the strategic/tactical deviation.",1987
XML for ETDs,The main objective of this project was to devise a tool/procedure to aid students at Virginia Tech in developing their electronic theses and dissertations (ETDs) in eXtensible Markup Language (XML) and to document properly all the work that was done at Virginia Tech in this regard. The project began by studying the other ETD-XML projects done earlier. Both the approaches (DTD and XSD) explored at Virginia Tech were studied and an attempt was made to improve the XSD approach using VBA (Visual Basic for Applications). The proposed approach was completely implemented and documented in a way that should be easy for the students to comprehend. This should help ease student efforts to prepare theses in XML.,2002
Claims Reuse for Notification Systems Design: LINK-UP Vision and IRC Equations,"Extending previous work on the concept of claims reuse, an approach for cataloging and reusing design knowledge in human-computer interaction, we introduce a vision for a system, LINK-UP. The system is intended to parallel a usability engineering process that involves claims analysis. While we initially target notification system design support, we believe that the general method is extensible to other design concerns. A key aspect of the LINK-UP system is its iterative assessment of critical parameters---essential target values that describe anticipated user goals. In notification systems design, three critical parameters are interruption, reaction, and comprehension, referred to as IRC. While the parameter values represent abstract concepts, a pivotal challenge in the development of LINK-UP is determining methods for consistent and accurate parameter specification. To this end, we introduce equations for calculating user's model IRC parameters, either from analytical or empirical data. Presented here are details of variable justification and equation behavior. Future work will assess consistency and accuracy of artifact classifications using the equations.",2003
Experiments with Conjugate Gradient Algorithms for Homotopy Curve Tracking,"There are algorithms for finding zeros or fixed points of nonlinear systems of equations that are globally convergent for almost all starting points, i.e., with probability one.  The essence of all such algorithms is the construction of an appropriate homotopy map and then tracking some smooth curve in the zero set of this homotopy map. HOMPACK is a mathematical software package implementing globally convergent homotopy algorithms with three different techniques for tracking a homotopy zero curve, and has separate routines for dense and sparse Jacobian matrices.  The HOMPACK algorithms for sparse Jacobian matrices use a preconditioned conjugate gradient algorithm for the computation of the kernal of the homotopy Jacobian matrix, a required linear algebra step for homotopy curve tracking.  Here variants of the conjugate gradient algorithm are implemented in the context of homotopy curve tracking and compared with Craig's preconditioned conjugate gradient method used in HOMPACK.  The test problems used include actual large scale, sparse structural mechanics problems.",1990
Link Models for Networks with Dynamic Topologies,"Dynamic hierarchical networks represent an architectural strategy for employing adaptive behavior in applications sensitive to highly variable external demands or uncertain internal conditions. The characteristics of such architectures are described, and the significance of adaptive capability is discussed. The necessity for assessing the tradeoffs between performance improvements (reduced and bounded message transmission time, increased throughput) and the added costs (reconfiguration delays, redundant links, etc.)  leads to the use of complex queueing models. The assumptions underlying the general model are stated, and a class of applicable models (queues in random environments or RE-queues) is introduced. Matrix-geometric methods are reviewed in terms of their suitability for addressing several variations of a subclass of RE-queue models.  Matrix-geometric techniques are considered to offer the greatest promise for obtaining usable results for assessing the cost/benefit tradeoffs.",1987
A UNIX Micro Requirement for Computer Science Majors,"In the fall of 1985, over 230 freshmen majoring in computer science at Virginia Tech purchased microcomputers running the UNIX operating system, because of a vote in spring 1983 by the faculty of the Department of Computer Science to institute a requirement of personal computers.  This paper reviews the planning and decision-making process, and explains why UNIX was chosen as the most suitable system to use in the training of computer science students.  Implementation of the microcomputer program is discussed, including revisions to the freshman curriculum and the distribution of required hardware and software.",1986
Immersive Virtual Environments for University Education: Views from the Classroom,"Education has long been touted as an important application area for immersive virtual environments (VEs). VEs can allow students to visualize and interact with complex three-dimensional (3D) structures, perform virtual experiments,#157; view scenes with natural head and body movements, and experience environments that would be otherwise inaccessible because of distance (the surface of the Moon), scale (a complex molecule), or danger (a sunken ship). Many researchers have explored the use of VEs for education [1, 2], with some degree of success. However, few VE systems have been deployed for actual classroom use, and little is known about effective methods for employing VEs in real-world settings (the work of Johnson et al. is a notable exception [4]). In this paper, we describe three VE applications developed to teach university students concepts in the areas of computer graphics, building structures, and computer networking, and discuss our experience in using them as integral parts of appropriate classes at Virginia Tech. We differ from Johnson et al. in our focus on postsecondary education and in our use of VEs as tools within a traditional lecture-based class. We present our observations of what worked and what did not, and offer guidelines for others wishing to incorporate VEs into the classroom.",2003
A Lifecycle Which Incorporates Software Metrics,"The traditional waterfall life cycle model of software development provides a systematic method to separate the development process into different stages with explicit communication boundaries between each subsequent stage.  But the waterfall model does not provide quantitative measurements for the products of each phase in the software life cycle. The model provides a base to develop methodologies which emphasize the completeness of the documents, the use of certain disciplines, and the consistency among documents.  On the other hand, it is very hard to use the model to develop methodologies which provide 1) the quantitative evaluation of the quality of the documents (products) from each phase, 2) feedback information to help the manager make management decisions, and 3) criteria for redesigning or recoding a system.  To ensure the quality of software products, a common basis for more meaningful evaluation leading to better understanding of software quality must be provided.",1990
Computational Steering in the Problem Solving Environment WBCSim,"Computational steering allows scientists to interactively control a numerical experiment and adjust parameters of the computation on-the-ﬂy and explore “what if ” analysis. Computational steering effectively reduces computational time, makes research more efficient, and opens up new product design opportunities. There are several problem solving environments (PSEs) featuring computational steering. However, there is hardly any work explaining how to enable computational steering for PSEs embedded with legacy simulation codes. This paper describes a practical approach to implement computational steering for such PSEs by using WBCSim as an example. WBCSim is a Web based simulation system designed to increase the productivity of wood scientists conducting research on wood-based composites manufacturing processes. WBCSim serves as a prototypical example for the design, construction, and evaluation of small-scale PSEs. Various changes have been made to support computational steering across the three layers—client, server, developer—comprising the WBCSim system. A detailed description of the WBCSim system architecture is presented, along with a typical scenario of computational steering usage.",2009
ScALPEL: A Scalable Adaptive Lightweight Performance Evaluation Library for application performance monitoring,"As supercomputers continue to grow in scale and capabilities, it is becoming increasingly difficult to isolate processor and system level causes of performance degradation. Over the last several years, a significant number of performance analysis and monitoring tools have been built/proposed. However, these tools  suffer from several important shortcomings, particularly in distributed environments. In this paper we present ScALPEL, a Scalable Adaptive Lightweight Performance Evaluation Library for application performance monitoring at the functional level. Our approach provides several distinct advantages. First, ScALPEL is portable across a wide variety of architectures, and its ability to selectively monitor functions presents low run-time overhead, enabling its use for large-scale production applications. Second, it is run-time configurable, enabling both dynamic selection of functions to profile as well as events of interest on a per function basis. Third, our approach is transparent in that it requires no source code modifications. Finally, ScALPEL is implemented as a pluggable unit by reusing existing performance monitoring frameworks such as Perfmon and PAPI and extending them to support both sequential and MPI applications.",2009-02-01
Programming Environments,"This report examines in detail the contents of the environments which are established during the progression of design, check-out and execution of a program. The level of support given by vendors to the elements of these environments is then examined with the conclusion as to where additional effort needs to be expended to achieve some improvements in program development support.",1983
The Effects of Finger-Walking in Place (FWIP) on Spatial Knowledge Acquisition in Virtual Environments,"Spatial knowledge, necessary for efficient navigation, comprises route knowledge (memory of landmarks along a route) and survey knowledge (overall representation like a map). Virtual environments (VEs) have been suggested as a power tool for understanding some issues associated with human navigation, such as spatial knowledge acquisition. The Finger-Walking-in-Place (FWIP) interaction technique is a locomotion technique for navigation tasks in immersive virtual environments (IVEs). The FWIP was designed to map a human’s embodied ability overlearned by natural walking for navigation, to finger-based interaction technique. Its implementation on Lemur and iPhone/iPod Touch devices was evaluated in our previous studies. In this paper, we present a comparative study of the joystick’s flying technique versus the FWIP. Our experiment results show that the FWIP results in better performance than the joystick’s flying for route knowledge acquisition in our maze navigation tasks.",2009-12-01
Knowledge Reuse Through Categorical Breakdown Analysis: A Method for Collaborative Systems Evaluation,"Designing Computer-Supported Cooperative Work (CSCW) systems that support the widely varying needs of targeted users is difficult. There is no silver bullet technology that enables users to effectively collaborate with one another in different contexts. We propose a method of collaborative systems evaluation that enables novice evaluators to make insightful observations about the systems they evaluate at a level comparable to experts in certain situations. These observations come in the form of a categorical breakdown analysis of a laboratory study. The quantity and type of breakdowns can then be connected to recommended CSCW tools and features developed and described in the related literature. We conducted a study to explore the results generated when the method was applied by both experts and novices in the field of CSCW. We observed that experts found the method to be usable, and that novices capitalized on the knowledge embodied in the breakdown categories to make categorizations similar to those of experts.",2008-05-01
Texture Region Growing Based Upon a Structural Model of Texture,"In this paper the problem of texture boundary formation is  approached by a region growing technique that is based upon a structural  model of texture.  Region growing is based upon similarities among  texture elements and upon the spatial proximities of these elements. The region grower itself is a global algorithm that makes use of a minimal spanning tree of what is called the associated texture graph of  a texture. A primary advantage of the method is that the texture  regions are self-organizing in the sense that no artificial windows need  be superimposed over the source textures. While the region described in  this paper uses extrema-based texture elements, the application to other  types of elements is straightforward.",1979
Space-time adaptive solution of inverse problems with the discrete adjoint method,"Adaptivity in both space and time has become the norm for solving problems modeled by partial differential equations. The size of the discretized problem makes uniformly refined grids computationally prohibitive. Adaptive refinement of meshes and time steps allows to capture the phenomena of interest while keeping the cost of a simulation tractable on the current hardware. Many fields in science and engineering require the solution of inverse problems where parameters for a given model are estimated based on available measurement information. In contrast to forward (regular) simulations, inverse problems have not extensively benefited from the adaptive solver technology. Previous research in inverse problems has focused mainly on the continuous approach to calculate sensitivities, and has typically employed fixed time and space meshes in the solution process. Inverse problem solvers that make exclusive use of uniform or static meshes avoid complications such as the differentiation of mesh motion equations, or inconsistencies in the sensitivity equations between subdomains with different refinement levels. However, this comes at the cost of low computational efficiency. More efficient computations are possible through judicious use of adaptive mesh refinement, adaptive time steps, and the discrete adjoint method.   This paper develops a framework for the construction and analysis of discrete adjoint sensitivities in the context of time dependent, adaptive grid, adaptive step models. Discrete adjoints are attractive in practice since they can be generated with low effort using automatic differentiation. However, this approach brings several important challenges. The adjoint of the forward numerical scheme may be inconsistent with the continuous adjoint equations. A reduction in accuracy of the discrete adjoint sensitivities may appear due to the intergrid transfer operators. Moreover, the optimization algorithm may need to accommodate state and gradient vectors whose dimensions change between iterations. This work shows that several of these potential issues can be avoided for the discontinuous Galerkin (DG) method. The adjoint model development is considerably simplified by decoupling the adaptive mesh refinement mechanism from the forward model solver, and by selectively applying automatic differentiation on individual algorithms.   In forward models discontinuous Galerkin discretizations can efficiently handle high orders of accuracy, $h/p$-refinement, and parallel computation. The analysis reveals that this approach, paired with Runge Kutta time stepping, is well suited for the adaptive solutions of inverse problems. The usefulness of discrete discontinuous Galerkin adjoints is illustrated on a two-dimensional adaptive data assimilation problem.",2010-11-01
Wing Design for a High-Speed Civil Transport Using a Design of Experiments Methodology,"The presence of numerical noise inhibits gradient-based optimization and therefore limits the practicality of performing aircraft multidisciplinary design optimization (MDO).  To address this issue, a procedure has been developed to create noise free algebraic models of subsonic and supersonic aerodynamic performance for use in the MDO of high-speed civil transport (HSCT) configurations.  This procedure employs methods from statistical design of experiments theory to select a set of HSCT wing designs (fuselage/tail/engine geometry fixed) for which numerous detailed aerodynamic analyses are performed. Polynomial approximations (i.e., response surface models) are created from the aerodynamic data to provide analytical models relating aerodynamic quantities (e.g., wave drag and drag-due-to-lift) to the variables which define the HSCT wing configuration.  A multidisciplinary design optimization of the HSCT is then performed using the response surface models in lieu of the traditional, local gradient based design methods.  The use of response surface models makes possible the efficient and robust application of MDO to the design of an  aircraft system.  Results obtained from five variable and ten variable wing design problems presented here demonstrate the effectiveness of this response surface modeling method.",1996-07-01
CP-Rays in Simplicial Cones,"In classical mathematics, interest in the concept of regularity of a triangle, is mainly centered on the property that for every interior point of the triangle, its orthogonal projection on the line containing each side must lie in the relative interior of that side. We generalize the concept of regularity using this property, and extend this work to simplicial cones in R^n, and derive very efficient necessary and sufficient conditions for this property to hold in them. We show that these concepts have important ramifications in algorithmic studies of the linear complementarity problem. We relate our results to other well known properties of square matrices.",1987
"FAD, a Functional Programming Language that Supports Abstract Data Types","The paper outlines the programming language FAD. FAD is a functional programming system of the kind described by Backus [Backus78]. FAD supports abstract data types, parameterized types, and generic functions. A single scope rule establishes the encapsulation requirements for data type specification and program structuring. Certain syntactic additions improve program readability as compared to pure functional notation.",1980
Large High Resolution Displays for Co-Located Collaborative Intelligence Analysis,"Large, high-resolution vertical displays carry the potential to increase the accuracy of collaborative sensemaking, given correctly designed visual analytics tools. From an exploratory user study using a fictional intelligence analysis task, we investigated how users interact with the display to construct spatial schemas and externalize information, as well as how they establish shared and private territories. We investigated the spatial strategies of users partitioned by tool type used (document- or entity-centric). We classified the types of territorial behavior exhibited in terms of how the users interacted with the display (integrated or independent workspaces). Next, we examined how territorial behavior impacted the common ground between the pairs of users. Finally, we recommend design guidelines for building co-located collaborative visual analytics tools specifically for use on large, high-resolution vertical displays.",2011-11-01
Terminating Parallel Discrete Event Simulations,This thesis analyzes the simulation termination problem of implementing global termination conditions and collecting output measures in discrete event simulations.,1991
Motion Planning of Uncertain Ordinary Differential Equation Systems,"This work presents a novel motion planning framework, rooted in nonlinear programming theory, that treats uncertain fully and under-actuated dynamical systems described by ordinary differential equations. Uncertainty in multibody dynamical systems comes from various sources, such as: system parameters, initial conditions, sensor and actuator noise, and external forcing. Treatment of uncertainty in design is of paramount practical importance because all real-life systems are affected by it, and poor robustness and suboptimal performance result if it’s not accounted for in a given design. In this work uncertainties are modeled using Generalized Polynomial Chaos and are solved quantitatively using a least-square collocation method. The computational efficiency of this approach enables the inclusion of uncertainty statistics in the nonlinear programming optimization process. As such, the proposed framework allows the user to pose, and answer, new design questions related to uncertain dynamical systems.   Specifically, the new framework is explained in the context of forward, inverse, and hybrid dynamics formulations. The forward dynamics formulation, applicable to both fully and under-actuated systems, prescribes deterministic actuator inputs which yield uncertain state trajectories. The inverse dynamics formulation is the dual to the forward dynamic, and is only applicable to fully-actuated systems;  deterministic state trajectories are prescribed and yield uncertain actuator inputs. The inverse dynamics formulation is more computationally efficient as it requires only algebraic evaluations and completely avoids numerical integration. Finally, the hybrid dynamics formulation is applicable to under-actuated systems where it leverages the benefits of inverse dynamics for actuated joints and forward dynamics for unactuated joints; it prescribes actuated state and unactuated input trajectories which yield uncertain unactuated states and actuated inputs.  The benefits of the ability to quantify uncertainty when planning the motion of multibody dynamic systems are illustrated through several case-studies. The resulting designs determine optimal motion plans—subject to deterministic and statistical constraints—for all possible systems within the probability space.",2011-04-01
Reasoning About Knowledge Using Extensional Logics,"When representing statements about knowledge in a extensional logic, it occasionally happens that undesired conclusions arise. Such extraneous conclusions are often the result of substitution of equals for equals or existential instantiation within intensional operators such as Know. In the past, efforts at solving this problem have centered on modifications to the logic. In this thesis, I propose a solution that leaves the logic intact and changes the representation of the statements instead. The solution presented here has four main points: 1) Only propositions can be known. 2) Relations rather than functions should be used to describe objects. 3) Temporal reasoning is often necessary to represent many real-world problems. 4) In cases where more than one label can apply to the same object, an agent's knowledge about labels must be explicitly represented. When these guidelines are followed, statements about knowledge can be represented in standard first-order predicate logic in such a way that extraneous conclusions cannot be drawn. Standard first-order theorem provers (like Prolog) can then be used to solve problems which involve reasoning about knowledge.",1987
Formulated Problem Verification As an Explicit Requirement of Model Credibility,"This paper deals with the formulation and formulation verification of a class of problems to which ""modeling solutions"" are applied. Two main objectives are to develop a procedure for problem formulation and to propose indicators for the formulated problem verification. The class of problems considered is analyzed in two categories as requiring prescriptive or descriptive solutions. A detailed study of each category results in a procedure to guide the analyst during the problem formulation. This procedure is illustrated by a traffic intersection problem. The formulated problem is measured by using indicators to accomplish an evaluation for the formulated problem verification. Indicators are developed to measure: (1) the probability of failing to solve the actual problem, (2) the acceptability of an alternative set of possible outcomes, and (3) how well the formulated problem is structured. An evaluation questionnaire, included in the Appendix, is employed in applying the proposed indicators.",1983
Explicit Parallel Programming: User's Guide,"The Explicit Parallel Programming (EPP) language is defined and illustrated with several examples.  EPP is a prototype implementation of a language for writing parallel programs for shared memory multiprocessors. EPP may be viewed as a coordination language, since it is used to define the sequencing or ordering of various tasks, while the tasks themselves are defined in some other compilable language.  The prototype described here requires FORTRAN as the base language, but there is no inherent reason why some other imperative language could not be used instead.  EPP encourages a structured and readable style of writing parallel programs, and it allows virtually any type of parallelism to be expressed.  It maintains as strict a separation as possible between the two main components of a parallel program: semantic actions and logic sequencing.   This paper is intended for the first-time user of EPP.",1991-05-01
Implementation of Predicate-Based Protection in MULTISAFE,"This paper reports some implementation work done within the MULTI SAFE database protection research project group at Virginia Tech. It describes the evolution of an approach to database security from a formal model of predicate-based protection, through an implementation model, to an on-going implementation. The implementation model is based on a relational database approach to the management of protection information (stored representations of authorizations). Classes of access decision dependency are reviewed. Protection policies, design deci¬sions, and special implementation problems are discussed. Detailed examples are used to illustrate the use of this flexible and generalized approach to database security within the MULTI SAFE system architecture.",1980
Ensemble-based chemical data assimilation I: An idealized setting,"Data assimilation is the process of integrating observational data and model predictions to obtain an optimal representation of the state of the atmosphere. As more chemical observations in the troposphere are becoming available, chemical data assimilation is expected to play an essential role in air quality forecasting, similar to the role it has in  numerical weather prediction. Considerable progress has been made recently in the development of variational tools for chemical data assimilation. In this paper we assess the performance of the ensemble Kalman filter (EnKF). Results in an idealized setting show that EnKF is promising for chemical data assimilation.",2006-03-01
Methods for detecting inter-protein covarying sites,"Covarying sites are defined to be sites in a protein whose rate of evolution changes over time. We design software to group protein sites into three rate pools: conserved, variant, and temporary invariant. Other software is written to find sites which are closely correlated. The algorithms used by the software require a multiple sequence alignment and phylogenetic tree as input and rely heavily on tree-corrected information entropy. Through a study of the protein Cu, Zn Superoxide Dimutase it is shown that temporary invariant sites have interactions with at least one site which is either closely correlated or binary-switching. From this result it is reasonable to assume that temporary invariant sites which interact with no such intra-protein sites must be sites of protein-protein interaction. Temporary invariant sites are also shows to reflect the animal plant divergence.",2009
A Controlled Experiment to Evaluate Maintainability of Object-Oriented Software,"New software tools and methodologies make claims that managers often believe intuitively without evidence.  Many unsupported claims have been made about object-oriented programming.  However, without scientific evidence, it is impossible to accept these claims as valid.  Although experimentation has been done in the past, most of the research is very recent and the most relevant research has serious drawbacks.  This paper describes an experiment which compares the maintainability of two functionally equivalent systems, in order to explore the claim that systems developed with object-oriented languages are more easily maintained than those programmed with procedural languages.  We found supporting evidence that programmers produce more maintainable code with an object-oriented language than with a standard procedural language.",1990
A Hybrid Variational/Ensemble Filter Approach to Data Assimilation,"Two families of methods are widely used in data assimilation: the four dimensional variational (4D-Var) approach, and the ensemble Kalman filter (EnKF) approach. The two families have been developed largely through parallel research efforts, and each method has its advantages and disadvantages. It is of interest to combine the two ap- proaches and develop hybrid data assimilation algorithms. This paper investigates the theoretical equivalence between the suboptimal 4D-Var method (where only a small number of optimization iterations are performed) and the practical EnKF method (where only a small number of ensemble members are used) in a linear Gaussian setting. The analysis motivates a new hybrid algorithm: the optimization directions obtained from a short window 4D-Var run are used to construct the EnKF initial ensemble. Numerical results show that the proposed hybrid ensemble filter method performs better than the regular EnKF method for both linear and nonlinear test problems.",2009-08-01
A Pluggable Framework for Lightweight Task Ofﬂoading in Parallel and Distributed Computing,"Multicore processors have quickly become ubiquitous in supercomputing, cluster computing, datacenter computing, and even personal computing. Software advances, however, continue to lag behind. In the past, software designers could simply rely on clock-speed increases to improve the performance of their software. With clock speeds now stagnant, software designers need to tap into the increased horsepower of multiple cores in a processor by creating software artifacts that support parallelism. Rather than forcing designers to write such software artifacts from scratch, we propose a pluggable framework that designers can reuse for lightweight task ofﬂoading in a parallel computing environment of multiple cores, whether those cores be colocated on a processor within a compute node, between compute nodes in a tightly-coupled system like a supercomputer, or between compute nodes in a loosely-coupled one like a cloud computer. To demonstrate the efﬁcacy of our framework, we use the framework to implement lightweight task ofﬂoading (or software acceleration) for a popular parallel sequence-search application called mpiBLAST. Our experimental results on a 9-node, 36-core AMD Opteron cluster show that using mpiBLAST with our pluggable framework results in a 205% speed-up.",2008
Three Papers on the Completeness Properties of Abelian Semi groups and Groups,"The three papers presented here all arise from recent research into  sets of operators which are sufficient to define a horizontal microlanguage for a computer and which are (in some undefined sense) natural for human beings to use. It is this criterion of naturalness which leads us to consider dyadic operators which are commutative and associative, that is, structures which are abelian semigroups.",1974
Multidisciplinary Optimization of a Supersonic Transport Using Design of Experiments Theory and Response Surface Modeling,"The presence of numerical noise in engineering design optimization problems inhibits the use of many gradient-based optimization methods.  This numerical noise may result in the inaccurate calculation of gradients which in turn slows or prevents convergence during optimization, or it may promote convergence to spurious local optima.  The problems created by numerical noise are particularly acute in aircraft design applications where a single aerodynamic or structural analysis of a realistic aircraft configuration may require tens of CPU hours on a supercomputer.  The computational expenses of the analyses coupled with the convergence difficulties created by numerical noise are significant obstacles to performing aircraft multidisciplinary design optimization.  To address these issues, a procedure has been developed to create noise-free algebraic models of subsonic and supersonic aerodynamic performance qualities for use in the optimization of high-speed civil transport (HSCT) aircraft configurations.  This procedure employs methods from statistical design of experiments theory and response surface modeling to create the noise-free algebraic models. Results from a sample HSCT design problem involving ten variables are presented to demonstrate the utility of this method.",1997-07-01
Tuning Complex Systems by Sonifying Their Performance Data,"In the modern computing landscape, the challenge of tuning software systems is exacerbated by the necessity to accommodate multiple divergent execution environments and stakeholders. Achieving optimal performance requires a different configuration for every combination of hardware setups and business requirements. In addition, the state of the art in system tuning can involve complex statistical models, which require deep expertise not commonly possessed by the average software developer. This paper presents a novel approach to tuning complex software systems by leveraging sound to convey performance information during execution. We conducted a scientific survey to determine which sound characteristics (e.g., loudness, panning, pitch, tempo, etc.) are most accurate to express information to the average programmer. As determined by the survey, the characteristics that scored the highest across all the participants were used to create a proof-of-concept demonstration. The demonstration showed that a programmer who is not an expert in either software tuning or enterprise computing can configure the parameters of a real world enterprise application server, so that its resulting performance surpasses that exhibited under the standard configuration. Our results indicate that sound-based tuning approaches can provide valuable solutions to the challenges of configuring complex computer systems.",2013
Mining Posets from Linear Orders,"There has been much research on the combinatorial problem of generating the linear extensions of a given poset. This paper focuses on the reverse of that problem, where the input is a set of linear orders, and the goal is to construct a poset or set of posets that generates the input. Such a problem ﬁnds applications in computational neuroscience,  systems biology, paleontology, and physical plant engineering. In this paper, several algorithms are presented for efficiently ﬁnding a single poset that generates the input  set of linear orders. The variation of the problem where a minimum set of posets that cover the input is also explored. It is found that the problem is polynomially  solvable for one class of simple posets (kite(2) posets) but NP-complete for a related class (hammock(2,2,2) posets).",2009
JavaBeans as a Framework for Collaborative Software,"We describe the use of a popular component framework, JavaBeans, to facilitate developing collaborative software.  We describe the class of collaboration-unaware components that can be effectively shared under this framework, and an extension that supports development of efficient collaboration-aware components.",1998-05-01
Cost-Effective Parallel Processing for H-squared/H-to infinity Controller Synthesis,"A distributed version of a homotopy algorithm for solving the H-squared/H-to infinity mixed-norm controller synthesis problem is presented.  The main purpose of the study is to explore the possibility of achieving high performance with low cost.  Existing UNIX workstations running PVM (Parallel Virtual Machine) are utilized.  Only the Jacobian matrix computation is distributed and therefore the modification to the original sequential code is minimal.  The same algorithm has also been implemented on an Intel Paragon parallel machine.  Our implementation shows that acceptable speedup is achieved and the larger the problem sizes, the higher the speedup.  Comparing with the results from the Intel Paragon, the study concludes that utilizing the existing UNIX workstations can be a very cost-effective approach to shorten computation time.  Furthermore, this economical way to achieve high performance computation can easily be realized and incorporated in a practical industrial design environment.",1995-07-01
Developing Curriculum for Use in an Interactive Computational Environment,"The advent of the interactive computer classroom and  laboratory on the campus leads naturally to the increased  usage of the computational facilities in undergraduate  education.  This report reviews a number of alternative  plans for the integration of these facilities into various  curricula, and provides outlines for the development of  course syllabi which are appropriate.",1979
A Procedural Approach to Evaluating Software Development Methodologies: The Foundation,"A procedure for evaluating software development methodologies is developed. Beginning with the issue of what constitutes a methodology, development of the evaluation procedure relies on the establishment of linkages among objectives, principles, and attributes intrinsic to software development methodologies. The evaluation of software development methodologies reflects an assessment structured by the needs, process, and product sequence for system development. Linkages are defined from recognized software engineering sources, providing a foundation for both objective and subjective ""analysis"" of a given methodology. Application of the evaluation procedure to two software development methodologies currently used by the United States Navy reveals the inherent power of the procedural approach.",1986
Whiggism in Computer Science: Views of the Field,"The teaching of any science is not complete without the inclusion of those elements that constitute the foundations of the field. While the history of computing is an element of those fundamentals in computer science, history should not be taught merely as a diversion from the technology but as a series of case studies supporting scholarship from ethics to virtual reality. Understanding the successes of the field provides only a biased view of the technology and hides the true nature of pioneers; understanding failures provides direction and guidance, and occasionally new insights that were ahead of their time and whose time has come.",1992
Query Processing Strategies for Distributed Database Systems,No abstract available.,1983
Coverage Analysis Methods for Formal Software Testing,"Software creation requires not only testing during the development cycle by the development staff, but also independent validation following the completion of the implementation. However in the latter case, the amount of testing that can be carried out is often limited by time and resources. At the very most, independent testing can be expected to provide 100% coverage of the requirements (or specifications) associated with the software element. This report describes a methodology by which the amount of testing required to provide 100% coverage of the requirements is assured while at the same time minimizing the total number of tests included in a test suite. A collateral procedure provides recommendations on which tests which might be eliminated if less than 10O% coverage of the requirements is permitted. This latter process will be useful in determining the risk of not running the minimum set of tests for 100% coverage. A second process selects from the test matrix the set of tests to be applied to the system following maintenance modification of any module-- that is, to provide a submatrix for regression testing.",1987
Testbed Evaluation of Virtual Environment Interaction Techniques,Testbed Evaluation of Virtual Environment Interaction Techniques,2001
Dynamic Multigrain Parallelization on the Cell Broadband Engine,"This paper addresses the problem of orchestrating and scheduling  parallelism at multiple levels of granularity on heterogeneous multicore processors. We present policies and mechanisms for adaptive exploitation and scheduling of multiple layers of parallelism on the Cell Broadband Engine.  Our policies combine event-driven task scheduling with malleable loop-level parallelism, which is exposed from the runtime system whenever task-level parallelism leaves cores idle.  We present a runtime system for scheduling applications with layered parallelism on Cell and investigate its potential with RAxML, a computational biology application which infers large phylogenetic trees, using the Maximum Likelihood (ML) method. Our experiments show that the Cell benefits significantly from dynamic parallelization methods, that selectively exploit the layers of parallelism in the system, in response to workload characteristics. Our runtime environment outperforms naive parallelization and scheduling based on MPI and Linux by up to a factor of 2.6. We are able to execute RAxML on one Cell four times faster than on a dual-processor system with Hyperthreaded Xeon processors, and 5--10\% faster than on a single-processor system with a dual-core, quad-thread IBM Power5 processor.",2006
Remote Usability Evaluation at a Glance,"Much traditional user interface evaluation is conducted in usability laboratories, where a small number of selected users is directly observed by trained evaluators.  However, as the network itself and the remote work setting have become intrinsic parts of usage patterns, evaluators often have limited access to representative users for usability evaluation in the laboratory and the users' work context is difficult or impossible to reproduce in a laboratory setting.  These barriers to usability evaluation led to extending the concept of usability evaluation beyond the laboratory, typically using the network itself as a bridge to take interface evaluation to a broad range of users in their natural work settings.",1997-07-01
Principle of Optimal Page Replacement and the LRU Stack Model,"Program reference strings generated by the LRU stack model are  considered, and expressions for the expected times to next reference for all pages occupying different LRU stack positions are derived. Using these expressions, necessary and sufficient conditions as well as sufficient conditions on the distance distribution are obtained which guarantee implementation by  the LRU replacement algorithm of the ""informal principle of optimality"" for  page replacements. The sufficient conditions are found to be the same as those under which the LRU replacement algorithm is shown to be optimal. Relaxed conditions are also obtained for special cases where the number of page frames  is fixed.",1974
Architectural Refactoring for Fast and Modular Bioinformatics Sequence Search,"Bioinformaticists use the Basic Local Alignment Search Tool (BLAST) to characterize an unknown sequence by  comparing it against a database of known sequences, thus detecting evolutionary relationships and biological properties. mpiBLAST is a widely-used, high-performance, open-source parallelization of BLAST that runs on a computer cluster delivering super-linear speedups. However, the Achilles heel of mpiBLAST is its lack of modularity, adversely affecting maintainability and extensibility; an effective architectural refactoring will benefit both users and developers.   This paper describes our experiences in the architectural refactoring of mpiBLAST into a modular, high-performance software package. Our evaluation of five component-oriented designs culminated in a design that enables modularity while retaining high-performance. Furthermore, we achieved this refactoring effectively and efficiently using eXtreme Programming techniques. These experiences will be of value to software engineers faced with the challenge of creating maintainable and extensible, high-performance, bioinformatics software.",2006-09-01
"Defining Software Quality Measures: A Systematic Approach Embedded in the Objectives, Principles, Attributes Framework","Currently, software quality measures and metrics are being developed in isolation and often without the benefit of a guiding framework. In this paper we describe a systematic process for identifying measurement approaches and defining corresponding metrics that definitively support software quality assessment. That systematic process embodies five well-defined steps that reflect quality assessment within a framework which links the achievement of desirable software engineering objectives to the use of appropriate principles, and the use of principles to the manifestation of desirable product attributes. Ada is the language we have chosen to examine; the Ada package is used to illustrate the identification and definition process.",1992
Simulation Model Development Environments: A Research Prototype,"The objectives of this paper are to: (1) describe the current state of research on simulation support environments, (2) use the Simulation Model Development Environment (SMDE) research prototype as an example for describing concepts and principles employed, experiences gained, and guidelines (potential principles) derived in the design and creation of the prototype, and (3) identify and speculate on future directions.",1986
Note on the End Game in Homotopy Zero Curve Tracking,"Homotopy algorithms to solve a nonlinear system of equations f(x)=0 involve tracking the zero curve of a homotopy map p(a,theta,x) from theta=0 until theta=1.  When the algorithm nears or crosses the hyperplane theta=1, an ""end game"" phase is begun to compute the solution x(bar) satisfying p(a,theta,x(bar))=f(x(bar))=0.  This note compares several end game strategies, including the one implemented in the normal flow code FIXPNF in the homotopy software package HOMPACK.",1995-03-01
Convergence Theory of Probability-one Homotopies for Model Order Reduction,"The optimal H-square model reduction problem is an inherently nonconvex problem and thus provides a nontrivial computational challenge.  This paper  systematically examines the requirements of probability-one homotopy methods to guarantee global convergence.  Homotopy algorithms for nonlinear systems of equations construct a continuous family of systems, and solve the given system by tracking the continuous curve of solutions to the family.  The main emphasis is on guaranteeing transversality for several homotopy maps based upon the pseudogramian formulation of the optimal projection equations and variations based upon canonical forms.  These results are essential to the probability-one homotopy approach by guaranteeing good numerical properties in the computation- al implementation of the homotopy algorithms.",1996-06-01
Autoregressive Models of Background Errors for Chemical Data Assimilation,"The task of providing an optimal analysis of the state of the atmosphere requires the development of  dynamic data-driven systems that efficiently integrate the observational data and the models. Data assimilation (DA) is the process of adjusting the states or parameters of a model in such a way that its outcome (prediction) is close, in some distance metric, to observed (real) states. It is widely accepted that a key ingredient of successful data assimilation is  a realistic estimation of the background error distribution.  This paper introduces a new method for estimating the background errors which are modeled using autoregressive processes. The proposed approach is computationally inexpensive and captures the error correlations along the flow lines.",2006-10-01
A comparison of three Algorithms for Tracing Nonlinear Equilibrium Paths of Structural Systems,"The relative efficiencies of the Riks/Wempner, Crisﬁeld, and normal flow solution algorithms for tracking nonlinear equilibrium paths of structural systems are compared. It is argued that the normal flow algorithm maybe both more computationally efficient and more robust compared to the other two algorithms when tracing the path through severe nonlinearities such as those associated with structural collapse. This is demonstrated qualitatively by comparing the relative behaviors of each algorithm in the vicinity of a severe nonlinearity. Quantitative results are presented for the collapse a blade stiffened panel.",2001
Attainable Resource Allocations Bounds,"Upper and lower bounds attained by the variables of a generalized resource allocation problem are distinguished from the a priori specified bounds defining the feasible set.  General theoretical criteria directly relating attainable bounds to specified bounds are presented, which are computationally superior to the traditional ""modified simplex method.""",1989
A Revised Stoneman for Distributed ADA Support Environments,"This paper extends the conceptual model of the ""STONEMAN"" document to more completely model the interfaces and protocols that exist in the Ada Programming Support Environment (APSE). A previous extension to the STONEMAN model is reviewed and critiqued, the guidelines for the APSE set forth in STONEMAN are reviewed, and an updated model is proposed. The new model is shown to meet the guidelines set forth in STONEMAN, and to include subsequent ideas as well. The new model is then applied to the problem of user communication with an APSE, and it is shown how the new model extends to include distributed APSEs as well as single host APSEs. The issue of security enforcement, as a necessary subset of dynamic verification, is also included in the new model.",1983
An Agenda for Human-Computer Interaction Research: User Interface Development Processes and Methodologies,"This paper is the result of one working group in a workshop entitled ""An Agenda for Human-Computer Interaction Research:  Science and Engineering Serving Human Needs.""  The workshop, sponsored by the National Science Foundation, brought together ""20-25 of the most prominent HCI researchers from the disciplines of computer science, engineering, information science, psychology, and human factors along with several NSF staff members.""  The workshop results, to appear in the HCI literature, identify critical research issues and potential avenues for assaulting them, along with necessary infrastructure recommendations related to educational, professional, and facility problems.  The overall topical area was divided into five areas, each with an individual researcher in charge of directing discussion and reporting on the area.  The areas included theory and taxonomy of HCI models, the interface development process, I/O devices and interaction styles, software tools, and computer supported collaborative work.",1991
A Web Based Architecture for Visually Coordinating and Publishing Multiple View Visualizations,"User coordinated visualizations are a powerful mechanism for building a visual interface. They allow a user to tailor an interface to their needs by choosing components, allowing visual representations customized to a task or user preferences. Snap-Together Visualization (Snap) is a web based system that allows for the creation and sharing of user customized visual interfaces. Users can bring their data to the system, compose a multiple-view interface, and visualize the data. They are then able to share the data and visual interface with other web users by providing a single URL. Snap provides a flexible architecture for coordinating independent interface components and sharing the resulting visualization.",2002
An Analysis of 10-Gigabit Ethernet Protocol Stacks in Multicore Environments,"This paper analyzes the interactions between the protocol stack (TCP/IP or iWARP over 10-Gigabit Ethernet) and its multicore environment. Specifically, for host-based protocols such as TCP/IP, we notice that a significant amount of processing is statically assigned to a single core, resulting in an imbalance of load on the different cores of the system and adversely impacting the performance of many applications. For host-offloaded protocols such as iWARP, on the other hand, the portions of the communication stack that are performed on the host, such as buffering of messages and memory copies, are closely tied with the associated process, and hence do not create such load imbalances. Thus, in this paper, we demonstrate that by intelligently mapping different processes of an application to specific cores, the imbalance created by the TCP/IP protocol stack can be largely countered and application performance significantly improved. At the same time, since the load is a better balanced in host-offloaded protocols such as iWARP, such mapping does not adversely affect their performance, thus keeping the mapping generic enough to be used with multiple protocol stacks.",2007
A fully Distributed Parallel Global Search Algorithm,"The n-dimensional direct search algorithm DIRECT of Jones,Perttunen, and Stuckman has attracted recent attention from the multidisciplinary design optimization community. Since DIRECT only requires function values (or ranking)and balances global exploration with local refinement better than n-dimensional bisection, it is well suited to the noisy function values typical of realistic simulations. While not efficient for high accuracy optimization, DIRECT is appropriate for the sort of global design space exploration done in large scale engineering design. Direct and pattern search schemes have the potential to exploit massive parallelism, but efficient use of massively parallel machines is nontrivial to achieve. This paper presents a fully distribute control version of DIRECT that is designed for massively parallel (distribute memory architectures. Parallel results are presented for a multidisciplinary design optimization problem — configuration design of a high speed civil transport.",2000
Proceedings of the Third Annual Virginia Tech Center for Human-Computer Interaction Research Experience for Undergraduates (REU) Symposium,"Virginia Tech's Center for Human-Computer Interaction presents the project abstracts for the REU ’08 symposium. The REU (Research Experience for Undergraduates) program provides undergraduate students from various universities with the opportunity to spend eight weeks at Virginia Tech, working with our faculty and graduate students on research projects using the state-of-the-art technology and laboratories assembled here. The REU program is sponsored by a National Science Foundation grant IIS-0552732.",2008
Performance Comparison of Three Parallel Implementations of a SchwarzSplitting Algorithm,"We describe three implementations of a Schwarz splitting algorithm for the numerical solution of two dimensional, second-order, linear elliptical partial differential equations.  One implementation makes use of the SCHEDULE package.  A second uses the language extensions available in SEQUENT Fortran for creating and controlling parallel processes.  The third implementation is a hybrid of the first two -- using explicit (non-portable) calls to create a control parallel processes, but using data structures and overhead similar to the implementation that uses SCHEDULE.  We report from several experiments with typical test problems on a Sequent Symmetry S81 shared-memory multiprocessor.  We measure the overhead involved in using SCHEDULE, and discuss advantages and disadvantages of this approach.",1989
User-System Interfaces: An Important Element of Information Systems Strategies,"A successful human-computer interface is one that exhibits high usability. This paper discusses several key development issues involved in attaining high usability: dialogue independence; an evaluation-centered methodology and support tools for developing the interface, and rapid prototyping.  It relates each of these issues to the critical cooperative work that must occur between the two major developer roles: computer scientists (and the software engineering role) and behavioral scientists (and the human factors role).  In particular, it examines the need for human factorable human-computer interfaces, rather than ones that are simply human factored, and discusses how each of the key development issues can help achieve human factorable interfaces.",1988
Two Point Constraint Approximation in Structural Optimization,The use of constraint approximations is recognized as a primary means of achieving computational efficiency in structural optimization. Existing approximation methods are based upon the value of the constraint function and its derivatives at a single point. The present paper explores the use of approximations based upon the value of the constraint and its derivative at two points. Several candidate approximations are suggested and tested for randomly generated rational constraint functions* Several of the approximations prove to be superior to the single point approximations.,1986
WWW Proxy Traffic Characterization with Application to Caching,"Characterizing World Wide Web proxy traffic helps identify parameters that affect caching, capacity planning and simulation studies.  In this paper we identify invariants that hold across a collection of ten traces representing traffic seen by caching-proxy servers.  The traces were collected from governmental, industry, university, high school, and an online service provider environment, with request rates that range from a few accesses to millions of accesses per hour.  We also show that the examined traffic is semi-similar.  We explore sources of Web self-similarity and we conclude that a strong source is the periodicity in the users behavior.  The tests revealed that there is a strong connection between access rate from hour to hour.  We also report the hit rate and weighted hit rate obtained by running a trace driven simulation on the workloads to simulate a proxy with infinite cache, similarly, accesses to unique servers and URLs are a small portion of the total.  By considering these characteristics of traffic we can improve the utility of caching for WWW clients.",1997-02-01
On Locally Linear Classification by Pairwise Coupling,"Locally linear classification by pairwise coupling addresses a nonlinear classification problem by three basic phases: decompose the classes of complex concepts into linearly separable subclasses, learn a linear classifier for each pair, and combine pairwise classifiers into a single classifier.  A number of methods have been proposed in this framework.  However, these methods have several deficiencies: 1) lack of a systematic evaluation of the framework, 2) naive application of general clustering algorithms to generate subclasses, and 3) no valid method to estimate and optimal number of subclasses.  This paper proves the equivalence between three popular combination schemas under general settings, defines several global criterion functions for measuring the goodness of subclasses, and presents a supervised greedy clustering algorithm to minimize the proposed criterion functions.  Extensive experiments has also been conducted on a set of benchmark data to validate the effectiveness of the proposed techniques.",2008
On Reducing the Set of Feasible Query Trees in Distributed Query Processing Optimization,No abstract available.,1983
Inter-Block GPU Communication via Fast Barrier Synchronization,"The graphics processing unit (GPU) has evolved from a ﬁxed-function processor with programmable stages to a programmable processor with many ﬁxed-function components that deliver massive parallelism. Consequently, GPUs increasingly take advantage of the programmable processing power for general-purpose, non-graphics tasks, i.e., general-purpose computation on graphics processing units (GPGPU). However, while the GPU can massively accelerate data parallel (or task parallel) applications, the lack of explicit support for inter-block communication on the GPU hampers its broader adoption as a general-purpose computing device. Inter-block communication on the GPU occurs via global memory and then requires a barrier synchronization across the blocks, i.e., inter-block GPU communication via barrier synchronization. Currently, such synchronization is only available via the CPU, which in turn, incurs signiﬁcant overhead. Thus, we seek to propose more efﬁcient methods for inter-block communication. To systematically address this problem, we ﬁrst present a performance model for the execution of kernels on GPUs. This performance model partitions the kernel’s execution time into three phases: (1) kernel launch to the GPU, (2) computation on the GPU, and (3) inter-block GPU communication via barrier synchronization. Using three well-known algorithms — FFT, dynamic programming, and bitonic sort — we show that the latter phase, i.e., inter-block GPU communication, can consume more than 50% of the overall execution time. Therefore, we propose three new approaches to inter-block GPU communication via barrier synchronization, all of which run only on the GPU: GPU simple synchronization, GPU tree-based synchronization, and GPU lock-free synchronization. We then evaluate the efficacy of each of these approaches in isolation via a micro-benchmark as well as integrated with the three aforementioned algorithms. For the micro-benchmark, the experimental results show that our GPU lock-free synchronization performs 7.8 times faster than CPU explicit synchronization and 3.7 times faster than CPU implicit synchronization. When integrated with the FFT, dynamic programming, and bitonic sort algorithms, our GPU lock-free synchronization improves the performance by 8%, 24%, and 39%, respectively, when compared to the more efﬁcient CPU implicit synchronization.",2009
Stack and Queue Layouts of Posets,"The stacknumber (queuenumber) of a poset is defined as the stacknumber (queuenumber) of its Hasse diagram viewed as a directed acyclic graph. Upper bounds on the queuenumber of a poset are derived in terms of its jumpnumber, its length, its width, and the queuenumber of its covering graph. A lower bound of  is shown for the queuenumber of the class of planar posets. The queuenumber of a planar poset is shown to be within a small constant factor of its width. The stacknumber of posets with planar covering graphs is shown to be . These results exhibit sharp differences between the stacknumber and queuenumber of posets as well as between the stacknumber (queuenumber) of a poset and the stacknumber (queuenumber) of its covering graph.",1992
Convergence of Trust Region Augmented Lagrangian Methods Using Variable Fidelity Approximation Data,"To date the primary focus of most constrained approximate optimization strategies is that application of the method should lead to improved designs.  Few researchers have focused on the development of constrained approximate optimization strategies that are assured of converging to a Karush-Kuhn-Tucker (KKT) point for the problem.  Recent work by the authors based on a trust region model management strategy has shown promise in managing the convergence of constrained approximate optimization in application to a suite of single level optimization test problems.  Using a trust-region model management strategy, coupled with an augmented Lagrangian approach for constrained approximate optimization, the authors have shown in application studies that the approximate optimization process converges to a KKT point for the problem.  The approximate optimization strategy sequentially builds a cumulative response surface approximation of the augmented Lagrangian which is then optimized subject to a trust region constraint.  In this research the authors develop a formal proof of convergence for the response surface approximation based optimization algorithm.  Previous application studies were conducted on single level optimization problems for which response surface approximations were developed using conventional statistical response sampling techniques such as central composite design to query a high fidelity model over the design space. In this research the authors extend the scope of application studies to include the class of multidisciplinary design optimization (MDO) test problems.  More importantly the authors show that response surface approximations constructed from variable fidelity data generated during concurrent subspace optimizations (CSSOs) can be effectively managed by the trust region model management strategy.  Results for two multidisciplinary test problems are presented in which convergence to a KKT point is observed.  The formal proof of convergence and the successfull MDO application of the algorithm using variable fidelity data generated by CSSO are original contributions to the growing body of research in MDO.",1997-08-01
Feature Reduction using a Singular Value Decomposition for the Iterative Guided Spectral Class Rejection Hybrid Classifier,"Feature reduction in a remote sensing dataset is often desirable to decrease the processing time required to perform a classification and improve overall classification accuracy. This work introduces a feature reduction method based on the singular value decomposition (SVD). This feature reduction technique was applied to training data from two multitemporal datasets of Landsat TM/ETM+ imagery acquired over a forested area in Virginia, USA and Rondonia, Brazil. Subsequent parallel iterative guided spectral class rejection (pIGSCR) forest/nonforest classifications were performed to determine the quality of the feature reduction. The classifications of the Virginia data were five times faster using SVDbased feature reduction without affecting the classification accuracy. Feature reduction using the SVD was also compared to feature reduction using principal components analysis (PCA). The highest average accuracies for the Virginia dataset (88.34%) and for the Rondonia dataset (93.31%) were achieved using the SVD. The results presented here indicate that SVDbased feature reduction can produce statistically significantly better classifications than PCA.",2007
On Utilization of Contributory Storage in Desktop Grids,"The availability of desktop grids and shared computing platforms has popularized the use of contributory resources, such as desktops, as computing substrates for a variety of applications. However, addressing the exponentially growing storage demands of applications, especially in a contributory environment, remains a challenging research problem. In this report, we propose a transparent distributed storage system that harnesses the storage contributed by grid participants arranged in a peer-to-peer network to yield a scalable, robust, and self-organizing system. The novelty of our work lies in (i) design simplicity to facilitate actual use; (ii) support for easy integration with grid platforms; (iii) ingenious use of striping and error coding techniques to support very large data files; and (iv) the use of multicast techniques for data replication. Experimental results through simulations and an actual implementation show that our system can provide reliable and efficient storage with large file support for desktop grid applications.",2007
Caching Proxies: Limitations and Potentials,"As the number of World-Wide Web users grow, so does the number of connections made to servers.  This increases both network load and server load.  Caching can reduce both loads by migrating copies of server files closer to the clients that use those files.  Caching can either be done at a client or in the network (by a proxy server or gateway).  We assess the potential of proxy servers to cache documents retrieved with the HTTP protocol.  We monitored traffic corresponding to three types of educational workloads over a one semester period, and used this as input to a cache simulation.  Our main findings are (1) that with our workloads a proxy has a 30-50% maximum possible hit rate no matter how it is designed; (2) that when the cache is full and a document is replaced, least recently used (LRU) is a poor policy, but simple variations can dramatically improve hit rate and reduce cache size; (3) that a proxy server really functions as a second level cache, and its hit rate may tend to decline with time after initial loading given a more or less constant set of users; and (4) that certain tuning configuration parameters for a cache may have little benefit.",1995-07-01
The Pagenumber of k-Trees is 0(k),"A k-tree is a graph defined inductively in the following way: the complete graph K(sub-k) is a K-tree, and if G is a k-tree, then the graph resulting from adding a new vertex to k vertices inducing a K(sub-k) in G is also a k-tree.  This paper examines the book embedding problem for k-trees.  A book embedding of a graph maps the vertices onto a line along the spine of the book and assigns the edges to pages of the book such that no two edges on the same page cross.  The pagenumber of a graph is the minimum number of pages in a valid book embedding.  In this paper, it is proven that the pagenumber of a k-tree is at most k + 1. Furthermore, it is shown that there exist k-trees that require k pages. The upper bound leads to bounds on the pagenumber of a variety of classes of graphs for which no bounds were previously known.",1995-10-01
Alternatives in Implementing Noncommutative Grobner Basis Systems,"Alternatives in implementing systems for computing Grobner bases in path algebras (noncommutative polynomial rings) are considered  and compared.  Adapted forms of the standard variations to the Buchberger's algorithm (for commutative polynomial rings) are discussed, as is a pattern matching approach that finds the divisors and common multiples among the leading terms of a set of polynomials. Results from preliminary experimentation with a prototype system are used to compare the different configurations of two variations (triple elimination and basis reduction).  Eight problem instances split between two classes of problems (one over free algebras, the other over mesh algebras) are used to compare the configurations.  An informal analysis suggests that order plays a larger role in determining the execution time for a problem instance that the algorithm.  However, by comparing configurations for each of the admissible orders, some observations can be made about the algorithms.",1996-05-01
"A Workload-Aware, Eco-Friendly Daemon for Cluster Computing","This paper presents an eco-friendly daemon that reduces power consumption while better maintaining high performance via a novel behavioral quantification of workload.  Specifically, our behavioral quantification achieves a more accurate workload characterization than previous approaches by inferring ""processor stall cycles due to off-chip activities.""  This quantification, in turn, provides a foundation upon which we construct an interval-based, power-aware, run-time algorithm that is implemented within a system-wide daemon.  We then evaluate our power-aware daemon in a cluster-computing environment with the NAS Parallel Benchmarks.  The results indicate that our novel behavioral quantification of workload allows our power-aware daemon to more tightly control performance while delivering substantial energy savings.",2008
The Synergy Between Object-Oriented Programming and Open System Interconnection,"The software engineering practice of building distributed object-oriented applications can be improved dramatically by exploiting the powerful synergism between object-oriented programming (OOP) and Open System Interconnection (OSI). The synergy arises because there are corresponding and complementary elements in both OOP and OSI; these elements are detailed and the synergism resulting from their integration is explained. The architecture of a prototype implementation, the goal of Project Synergy,  is described. The environment created by Project Synergy supports application development using classes which are defined in an implementation-independent manner, implemented in possibly different programming languages, and executed in a distributed system on possibly heterogeneous processor architectures.",1991
Tracking Text in Mixed Mode Documents,"This paper describes a method for extracting arbitrarily oriented text in documents containing both text and graphics. The technique presented is inspired by the tracking algorithms frequently found in raster to vector conversion systems. By identifying text components in the document, reducing the resolution of the image by the size of the characters, and then tracking the centers of the character components, all text strings can be removed and subsequently reoriented to the horizontal. They can then be presented for automated character recognition. A by-product of the method is that characters are automatically grouped together to form words and/or phrases. We give a detailed description of the algorithm, discuss its strengths and weaknesses, and present some sample results obtained from a typical city street map.",1988
Decomposing Rectilinear Figures into Rectangles,"We discuss the problem of decomposing rectilinear regions, with or without holes, into a minimum number of rectangles. There are two different problems considered here: decomposing a figure into non-overlapping parts, called partitioning, and decomposing a figure into possibly overlapping parts, called covering.  A method is outlined and proved for solving the above two problems, and algorithms for the solutions of these problems are presented.  The partitioning problem can be solved in time O(n-to the 5/2), where n is the number of vertices of the figure, whereas the covering problem is exponential in its time complexity.",1988
Management Tools Associated with the Development of Computer Software Products,No abstract available.,1983
An Artificial Intelligence Approach to the Symbolic Factorization of Multivariable Polynomials,"A new heuristic factorization scheme that uses learning to improve the efficiency of determining the symbolic factorization of multivariable polynomials with integer coefficients and an arbitrary number of variables and terms is described. The factorization scheme makes extensive use of Artificial Intelligence techniques, e.g. model-building, learning, and automatic classification in an attempt to reduce the amount of searching for the irreducible factors of a polynomial. The approach taken to polynomial factorization is quite different from previous attempts because: (1) it is distinct from numerial techniques, (2) possibilities for terms in a factor are generated from the terms in the polynomial, and (3) a reclassification technique is used to allow the application of different sets of heuristics to a polynomial during factorization attempts on it. Tables are presented that demonstrate the importance of learning to the efficiency of operation of the scheme. Factorizat5.on times of polynomials factored by both the scheme described in this paper and Wang's implementation of Berlekamp's algorithm are given and compared and an analysis of variance experiment provides an indication of the significant sources of variation influencing the factorization time.",1974
Web Response Time and Proxy Caching,"It is critical to understand WWW latency in order to design better HTTP protocols. In this paper we characterize Web response time and examine effects of proxy caching on response time. We show that at least a quarter of the total elapsed time is spent in setting up TCP connections. We also characterize the effect of a user's network bandwidth on response time. Average connection time from a client via a 33.6 K modem is two times longer than that from a client via switched Ethernet. Contrary to the typical thought about Web proxy caching, this study finds that a single stand alone proxy cache does not always reduce response time. Implications of these results to the HTTP-NG protocol and Web application design also are discussed in the paper.",1998-03-01
Stability and Postbuckling of a Platform with Flexible Legs Resting on a Slippery Surface,A rigid platform is supported by thin elastic legs.  The legs are able to slide on the ground as they deform.  The governing equations for large deformations are formulated and solved numerically by homotopy and quasi-Newton methods.  Nonlinear phenomena such as nonuniqueness are found.  A global critical load for nonlinear stability is presented.,1997-12-01
Rapid Prototyping in Human-Computer Interface Development,"Some conventional approaches to interactive system development tend to force commitment to design detail without a means for visualizing the result until it is too late to make significant changes.  Rapid prototyping and interactive system refinement, especially for the human interface, allow early observation of system behavior and opportunities for refinement in response to user feedback.  The role of rapid prototyping for evaluation of interface designs is set in the system development life cycle.  Advantages and pitfalls are weighed, and detailed examples are used to show the application of rapid prototyping in a real development project.  Kinds of prototypes are classified according to how they can be used in the development process, and system development issues are presented.  The future of rapid prototyping depends on solutions to technical problems that presently limit effectiveness of the techniques in the context of present day software development environments.",1989
The Poset Cover Problem,"A partial order or poset P = (X,<) on a (finite) base set X determines the set L(P) of linear extensions of P. The problem of computing, for a poset P, the cardinality of L(P) is #P-complete. A set {P1, P2, . . . , Pk} of posets on X covers the set of linear orders that is the union of the L(Pi). Given linear orders L1,L2, . . . ,Lm on X, the Poset Cover problem is to determine the smallest number of posets that cover {L1,L2, . . . ,Lm}. Here, we show that the decision version of this problem is NP- complete. On the positive side, we explore the use of cover relations for finding posets that cover a set of linear orders and present a polynomial-time algorithm to find a partial poset cover.",2012
A Full Variational Calculation Based on a Tensor ProductDecomposition,"A new direct full variational approach exploits a tensor (Kronecker) product decomposition of the Hamiltonian.  Explicit assembly and storage of the Hamiltonian matrix is avoided by using the Kronecker product structure to form matrix-vector products directly from the molecular integrals.  Computation-intensive integral transformations and formula tapes are unnecessary.  The wavefunction is expanded in terms of spin-free primitive kets rather than Staler determinants of configuration state functions, and the expansion is equivalent to a full configuration interaction expansion.  The approach suggests compact storage schemes and algorithms which are naturally suited to parallel and pipelined machines.",1989
Performance of Some CFD Codes on the Alliant FX/8,Three CFD codes are ported to the Alliant FX/8.  The first solves the 3-D unsteady Euler equations using an explicit finite-volume Runge-Kutta time stepping method.  The second solves the same problem using the Beam and Worming implicit method.  The third is ARC2D from NASA  Ames which solves the unsteady 2-D Navier-Stockes equations using an implicit method.  Extensive observations and results on the performance of these codes on the FX/8 are presented.  Careful interaction with parallelizing compiler improves the performance some.  Better results are obtained by simple recoding of different segments in the programs.,1988
Geometric Performance Analysis of Periodic Behavior in Detail,"A fundamental problem in designing parallel programs that achieve a desired performance goal is the ability to exactly analyze program performance, given a specification of the process synchronization structure and the execution timings of all code segments.  This paper presents a novel performance analysis method based on geometry.  Given a definition of program state, an execution of a program can be represented by a timed execution sequence (TES). A TES is a sequence of states that the program passes through in an execution, along with the duration of time spent in each state.  In some parallel programs, all TESs that can arise in any execution contain a suffix that consists of the repetition of a finite sequence of states, excluding  deadlocks and nondeterministic behavior.  The repeated sequence is termed the limit cycle execution sequence.  This paper derives, for all possible process starting times, a representation of the set of all possible limit cycle execution sequences in which a process blocks.  The paper makes two contributions.  First, it employs a novel analysis method to derive TESs from a geometric program execution model, using timed progress graphs (TPGs).  TPGs represent the progress of each process by an axis in a Cartesian graph; process synchronization by line segments; and a TES by a direct, continuous path that does not cross a segment.  Second, it solves for TESs in TPGs not by a computational geometric algorithm, as employed by most solutions in the literature to (untimed) progress graphs, but by an analytic solution.",1995-09-01
A Framework for the Study of Query Decomposition for Heterogeneous Distributed Database Management Systems,"This paper presents a framework for the study of the query decomposition translation for heterogeneous record -oriented database management systems. This framework is based on the applied database logic representation of relational, hierarchical and network databases. The input to the query decomposition translation is the query graph which is derived from the complex to basic, external to conceptual and logical optimization translations. Once the query graph is obtained the objective of the query decomposition translation is to break up a query expressed in terms of the actual or conceptual databases into its component parts or subqueries and find a strategy indicating the sequence of primitive or fundamental operations and their corresponding processing sites in the network necessary to answer the query. The query processing strategy is usually chosen so as to satisfy some performance criterion such as response time reduction.  Contingent on after each primitive operation. The prequery decomposition translation, the query decomposition translation and the size estimation issues are presented through an example based on the current implementation of the Distributed Access View Integration Database (DAVID) currently being built at NASA's Goddard Space Flight Center (GSFC).  The choice of a query processing strategy is the successful estimation of intermediate results",1987
Education and Design: Using Human-Computer Interaction Case Studies to Learn,"Computers are essentially an ever-present tool that can be used in almost any discipline to make work faster and easier. Creating these programs, however, such that they fulfill the needs of the customer is a challenging process given the uniqueness of the discipline and circumstance. Thus, the use of a programming design methodology can enable the computer program designer to create a better system that meets the needs of the customer. Teaching this process, or in essence how to design, is the focus of this work. In this paper we present how using case studies in Human-Computer Interaction, and more specifically displaying the evolution of a case study, increases a designer’s ability to learn and then apply this knowledge. We investigate how to use this design evolution within case studies and the effects it had on application, while also exploring how case studies can be used in educating computer scientists.",2006
Homotopy Methods for Solving the Optimal Projection Equations for the H2 Reduced Order Model Problem,"The optimal projection approach to solving the H2 reduced order model problem produces two coupled, highly nonlinear matrix equations with rank conditions as constraints. Due to the resemblance of these equations to standard matrix Lyapunov equations, they are called modified Lyapunov equations. The algorithms proposed herein utilize probability-one homotopy theory as the main tool. It is shown that there is a family of systems (the homotopy) that make a continuous transformation from some initial system to the final system. With a carefully chosen initial problem a theorem guarantees that all the systems along the homotopy path will be asymptotically stable, controllable and observable. One method, which solves the equations in their original form, requires a decomposition of the projection matrix using the Drazin inverse of a matrix. It is shown that the appropriate inverse is a differentiable function. An effective algorithm for computing the derivative of the projection matrix that involves solving a set of Sylvester equations is given. Another class of methods considers the equations in a modified form, using a decomposition of the pseudogramians based on a contragredient transformation. Some freedom is left in making an exact match between the number of equations and the number of unknowns, thus effectively generating a family of methods.",1991
A Process for Producing Tactually Perceptible Images of Line Drawings for Use by the Blind,A working prototype computer controlled process is described for automatically converting line drawings into raised line images perceptible to the touch. The  process uses an automatic digitizer for input and a milling machine for output.,1974
Improved Genetic Algorithm for the Design of Stiffened Composite Panels,"The design of composite structures against buckling presents two major challenges to the designer.  First, the problem of laminate stacking sequence design is discrete in nature, involving a small set of fiber orientations, which complicates the solution process.  Therefore, the design of the stacking sequence is a combinatorial optimization problem which is suitable for genetic algorithms.  Second, many local optima with comparable performance may be found.  Most optimization algorithms find only a single optimum, while often a designer would want to obtain all the local optima with performance close to the global optimum.  Genetic algorithms can easily find many near optimal solutions.  However, they usually require very large computational costs.  Previous work by the authors on the use of genetic algorithms for designing stiffened composite panels revealed both the above strength and weakness of the genetic algorithm.  The present paper suggests several changes to the basic genetic algorithm developed previously, and demonstrates reduced computational cost and increased reliability of the algorithm due to these changes.  Additionally, for a stiffened composite panel used in this study, designs lighter by about 4 percent compared to previous results were obtained.",1994-10-01
New Algorithms for Generating Conway Polynomials over Finite Fields,"Arithmetic in a finite field of prime characteristic p normally employs an irreducible polynomial in Z_p[X].  A particular class of irreducible polynomials, generally known as Conway polynomials, provides a means for representing several finite fields of characteristic p in a compatible manner.  Conway polynomials are used in computational algebra systems such as GAP and Magma to represent finite fields.  The generation of the Conway polynomials for a particular finite field has previously been done by an often expensive brute force search.  As a consequence, only a few Conway polynomials have been generated.  We present two algorithms for generating Conway polynomials that avoid the brute force search.  We have implemented one of these algorithms in Magma and present numerous new Conway polynomials that implementation generated.",1998-07-01
Positive Alternatives to Computer Misuse: A Report of the Proceedings of an ACM Panel on Hacking,"If one believes press reports, the misuse of computers is rampant.  The same the many other misuses have gone unreported as a result of the embarrassment and loss of image/integrity of those affected (as victims) and the belief that to expose misuses will lead to further problems and expense, as well as encourage copy-cat crimes. Much of the press coverage of computer misuse has centered on the younger members of computing community using inexpensive personal computer systems. The placement of powerful tools into the hands of adolescents without imposing the appropriate control would appear to be a contributing factor to their unfettered use of computers and communications systems. Such usage ranges from benign exploration of the powers of computational and communication systems, through the malicious use of those devices for personal aggrandisement, to the use of computers for distinctly criminal activities.  At present there are no technological or ethical barriers that separate the explorer from the criminal; possibly the prospect of being able to perform more and more complex projects can innocently lead the benign learner into improper activities. Conversely the blame for breaches of security and the infiltration of personal, private systems, is sometimes placed on the backs of the owners of those systems for maintaining an attractive nuisance.  Concern must also be expressed about the community-developing attractive nuisances of bulletin boards (BBS's), many of which are benign, but too many of which are used to extend the fringe area of illegality in exchanging pirated software, providing message systems for immoral or illegal purposes, and revealing cracks in governmental and commercial security.",1986-04-01
Attainable Resource Allocation Bounds,"Upper and lower bounds attained by the variables of a generalized resource allocation problem are distinguished from the a priori specified bounds defining the feasible set.  General theoretical criteria directly relating the attainable bounds to the specified bounds are presented, which are computationally superior to the traditional ""modified simplex method"".  An efficient algorithm is presented, and its computational complexity is analyzed.",1990
VizCraft: A Problem Solving Environment for Configuration Design of High Speed Civil Transport,"We describe a problem solving environment (PSE) named VizCraft that aids aircraft designers during the conceptual design stage. At this stage,an aircraft design is defined by a vector of 10-30 parameters. The goal is to find a vector that minimizes a performance-based objective function while meeting a series of constraints. VizCraft integrates the simulation code that evaluates a design with visualization for analyzing a design individually or in contrast to other designs. VizCraft allows the designer to easily switch between the view of a design in the form of a parameter set, and a visualization of the corresponding aircraft geometry. The user can easily see which, if any, constraints are violated. VizCraft also allows the user to view a database of designs using parallel coordinates.  Keywords: Problem solving environment,scientific data visualization, mutlidiimensional visualization, aircraft design, multidisciplinary design optimization.",1999-09-01
Management Issues and Software Reuse: An Empirical Study,"This paper describes the results of a controlled experiment designed to evaluate the impact of managerial influence and cognitive abilities on software reuse. The experiment concludes that (1) software reuse promotes higher productivity, (2) reuse resulting from both moderate and strong encouragement promote higher productivity than no reuse, (3) management's strong encouragement to reuse tends to promote improper reuse activities, (4) in general, reuse of a module is unproductive if 30 percent or less is used for the target system, though as much as 50 percent can be discarded for some modules and still be worth reusing, (5) of integrative ability, perception speed, and visualization, only the ability to visualize changes made to patterns was related to software reuse, and (6) of the subject's prior experience, only the amount of testing experience was related to software reuse.",1992
Magnetohydrodynamic Flow and Heat Transfer About a Rotating Disk with Suction and Injection at the Disk Surface,"This paper studies the effects of a partial magnetic field on the flow and heat transfer about a porous rotating disk. Using modem quasi-Newton and globally convergent homotopy methods, numerical solutions are obtained for a wide range of magnetic field strengths and injection and suction velocities. Results are presented graphically in terms of three nondimensional parameters.  There is excellent agreement with previous work and asymptotic formulas.",1986
